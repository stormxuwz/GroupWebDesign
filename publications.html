<!DOCTYPE html>
<head>

  <!-- Basic Page Needs
  ================================================== -->
  <meta charset="utf-8">
  <title>EISA - Publications</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  ================================================== -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Add jQuery library -->
  <script type="text/javascript" src="lib/jquery-1.9.0.min.js"></script>

  <!-- Add mousewheel plugin (this is optional) -->
  <script type="text/javascript" src="lib/jquery.mousewheel-3.0.6.pack.js"></script>

  <!-- Add fancyBox main JS and CSS files -->
  <script type="text/javascript" src="source/jquery.fancybox.js?v=2.1.4"></script>
  <link rel="stylesheet" type="text/css" href="source/jquery.fancybox.css?v=2.1.4" media="screen" />

  <!-- Add Button helper (this is optional) -->
  <link rel="stylesheet" type="text/css" href="source/helpers/jquery.fancybox-buttons.css?v=1.0.5" />
  <script type="text/javascript" src="source/helpers/jquery.fancybox-buttons.js?v=1.0.5"></script>

  <!-- Add Thumbnail helper (this is optional) -->
  <link rel="stylesheet" type="text/css" href="source/helpers/jquery.fancybox-thumbs.css?v=1.0.7" />
  <script type="text/javascript" src="source/helpers/jquery.fancybox-thumbs.js?v=1.0.7"></script>

  <!-- Add Media helper (this is optional) -->
  <!--<script type="text/javascript" src="source/helpers/jquery.fancybox-media.js?v=1.0.5"></script> -->

<script type='text/javascript'>
function toggle(id)
{
var el = document.getElementById(id);
if(el.style.display == 'block')
el.style.display = 'none';
else
el.style.display = 'block';
}
</script>

<script>

$(function() {
    $('li').filter(function(i) { return $('ul', this).length >= 1; }).each(function(i) {
        $(this).children("a").append(
        )
        .click(function(e) {
            var $ul = $(this).next("ul");
            if ($ul.is(":visible")) {
                $ul.find("ul").slideUp();
                $ul.slideUp();
            }
            else {
                $ul.slideDown();
            };
        })
    });
});

</script>


<script>

    $(window).resize(function(){

        w = $(this).width();

        $("div#wrap").css('width',(w < 920) ? w*0.95 : 570);

        $("body").css('overflow-x','hidden');

    });


/****************************************
 * Floating menu jQuery feature
 ****************************************/

</script>


  <style type="text/css">
    .fancybox-custom .fancybox-skin {
      box-shadow: 0 0 50px #222;
    }
	ul.a {list-style-type:disc;}
  </style>
  <!-- CSS
  ================================================== -->
	<link rel="stylesheet" href="stylesheets/base.css">
  	<link rel="stylesheet" href="stylesheets/skeleton.css">
  	<link rel="stylesheet" href="stylesheets/layout.css">
	<link rel="stylesheet" href="stylesheets/orbit-1.2.3.css">
	<link rel="stylesheet" href="stylesheets/doc.css">
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

  <!-- Favicons
  ================================================== -->
  <link rel="shortcut icon" href="images/favicon.ico">
  <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
  <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">

</head>
<body>
  <div class="container">
    <div class="three columns sidebar">
      <nav id="navi">
      <a href="http://www.ncsa.uiuc.edu">
        <img src="img/ncsa_horizontal.jpg" alt="[NCSA]" height="19" width="120">
      </a>
      
      <a href="http://www.illinois.edu">
        <img src="img/ilogo_horz_bold.gif" alt="[U of I]" height="23" width="120">
      </a>
      
      <hr>
        <h2 id="logo">EISA</h2>
        <ul>
          <li><a href="./index.html">Welcome</a></li>
          <li><a href="./people.html">People</a></li>
          <li><a href="./publications.html">Publications</a></li>
          	<ul>
          		<li><a href="#journal">Peer Reviewed Paper</a></li>
				<li><a href="#proceedings">Conference Proceedings</a></li>
          		<li><a href="#thesis">Thesis</a></li>
          		<li><a href="#book">Book</a></li>
          	</ul>
          <li><a href="./projects.html">Projects</a></li>
        </ul>
       <br /><br /> <br /><br /> <br /><br /> <br /><br />
       	<hr>
       	<p><b><font face="helvetica,arial,sans-serif" size="3" color="red">Open source codes</font></b></p>
        <a href="https://github.com/MinskerGroup/" class="newbuttom">GitHub</a>
        </nav>
        &nbsp;
     </div>
	
        	
   	 <div class="twelve columns offset-by-one content">
   	 	<div>
   	 	<header>
            <h1> Publications </h1>
            <hr class="large"/>
			<img src="img/WordCloudAbstracts2.png" alt="Word Cloud" class="ten columns">
        </header>
   	 	</div>
   	 	<hr>
   	 	<div class="doc-section clearfix" id="journal">
		<h3>Peer Reviewed Papers</h3>
<li>Zimmer, A., Schmidt, A., Ostfeld, A. & Minsker, B. (2013). <a href="http://ascelibrary.org/doi/abs/10.1061/%28ASCE%29HY.1943-7900.0000747">A New Method for the Offline Solution of Pressurized and Supercritical Flows</a>, <i>Journal of Hydraulic Engineering</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid1');">Abstract</a><p class="more" id="hid1" style="display: none;">An offline approximation of the Saint Venant equations is proposed for combined sewer overflow (CSO) prediction. Precalculated tabulations of the mass and momentum equations allow online interpolation, accelerating run-time hydraulic computations above those yielded by standard industry software, while preserving accuracy. Previous methods of backwater profile compilation account onlyfor subcritical, open-channel flows. This work further extends the backwater profiles to include pressurized flows, which are key for CSO conditions. Incorporation of the Darcy-Weisbach equation eliminates potential lookup table discontinuities between open-channel and pressurized conditions.Graphical depiction of supercritical flows causes iteration errors as nonunique flow rates appear for equivalent water surface elevations. Iteration errors can be eliminated by allowing the solution to proceed separately upstream and downstream from the governing critical water surface. This precalculated curve approach addresses discrepancies in supercritical flow and submerged weir calculations in the EPA SWMM model. The implicit computational approach surpasses SWMM run times by as much as 57% and proves an accurate tool for evaluation of sewer water levels for real-time optimization model incorporation. copy; 2013 American Society of Civil Engineers.</p>
<li>Ahalt, S.,Band, L., Minsker, B., Palmer, M., Tiemann, M., Idaszak, R., Lendhardt, C. & Whitton, M. (2013). <a href="http://waters2i2.org/documents/2013/05/water-science-software-institute-an-open-source-engagement-approach.pdf">Water Science Software Institute: An Open Source Engagement Process</a>, <i> Proceedings of the 2013 International Workshop on Software Engineering for Computational Science and Engineering</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid2');">Abstract</a><p class="more" id="hid2" style="display: none;">[No abstract available]</p>
<li>Wietsma, T. & B. Minsker (2012). <a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6404475">Adaptive sampling of streaming signals</a>, <i>2012 IEEE 8th International Conference on E-Science, Chicago, IL</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid3');">Abstract</a><p class="more" id="hid3" style="display: none;">Higher sensor throughput has increased the demand for cyberinfrastructure, requiring those unfamiliar with large database management to acquire new skills or outsource. Some have called this shift from sensor-limited data collection the "data deluge." As an alternative, we propose that the deluge is the result of sensor control software failing to keep pace with hardware capabilities. Rather than exploit the potential of powerful embedded operating systems and construct intelligent sensor networks that harvest higher quality data, the old paradigm (i.e. collect everything) is still dominant. To mitigate the deluge, we present an adaptive sampling algorithm based on the Nyquist-Shannon sampling theorem. We calibrate the algorithm for both data reduction and increased sampling over "hot moments," which we define as periods of elevated signal activity, deviating from previous works which have emphasized adaptive sampling for data compression via minimization of signal reconstruction error. Under the feature extraction concept, samples drawn from userdefined events carry greater importance and effective control requires the researcher to describe the context of events in the form of both an identification heuristic (for calibration) and a real-time sampling model. This event-driven approach is important when observation is focused on intermittent dynamics. In our case study application, we develop a heuristic to identify hot moments from historical data and use it to train and evaluate the adaptive model in an offline analysis using soil moisture data. Results indicate the adaptive model is superior to uniform sampling, capable of extracting 20% to 100% more samples during hot moments at equivalent levels of overall efficiency. ?2012 IEEE.</p>
<li>Gartial, M., Braunschweig, B., Chang, T., Moinzadeh, P., Minsker, B., Agha, G., Wieckowski, A., Keefer, L. & Liu, G. (2012). <a href="http://pubs.rsc.org/en/content/articlelanding/2012/em/c2em30380a#!divAbstract">Micro Electronic Wireless Nitrate Sensor Network for Environmental Water Monitoring</a>, <i>Environmental Science: Processes & Impacts (formerly J. Environ. Monit.)</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid4');">Abstract</a><p class="more" id="hid4" style="display: none;">Quantitative monitoring of water conditions in a field is a critical ability for environmental science studies. We report the design, fabrication and testing of a low cost, miniaturized and sensitive electrochemical based nitrate sensor for quantitative determination of nitrate concentrations in water samples. We have presented detailed analysis for the nitrate detection results using the miniaturized sensor. We have also demonstrated the integration of the sensor to a wireless network and carried out field water testing using the sensor. We envision that the field implementation of the wireless water sensor network will enable "smart farming" and "smart environmental monitoring"</p>
<li>Hill, D., Liu, Y., Marini, L., Kooper, R., Rodriguez, A., Minsker, B., Myers, J. & McLaren, T. (2011). <a href="http://eisa.ncsa.illinois.edu/Documents/Papers/Hill_etal_2011.pdf">Using A Virtual Sensor System to Create Real-Time Customized Environmental Data Products</a>, <i>Environmental Modelling and Software</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid5');">Abstract</a><p class="more" id="hid5" style="display: none;">With the advent of new instrumentation and sensors, more diverse types and increasing amounts of data are becoming available to environmental researchers and practitioners. However, accessing and integrating these data into forms usable for environmental analysis and modeling can be highly time-consuming and challenging, particularly in real time. For example, radar-rainfall data are a valuable resource for hydrologic modeling because of their high resolution and pervasive coverage. However, radar-rainfall data from the Next Generation Radar (NEXRAD) system continue to be underutilized outside of the operational environment because of limitations in access and availability of research-quality data products, especially in real time. This paper addresses these issues through the development of a prototype Web-based virtual sensor system at NCSA that creates real-time customized data streams from raw sensor data. These data streams are supported by meta-data, including provenance information. The system uses workflow composition and publishing tools to facilitate creation and publication (as Web services) of user-created virtual sensors. To demonstrate the system, two case studies are presented. In the first case study, a network of point-based virtual precipitation sensors is deployed to analyze the relationship between radar-rainfall measurements, and in the second case study, a network of polygon-based virtual precipitation sensors is deployed to be used as input to urban flooding models. These case studies illustrate how, with the addition of some application-specific information, this general-purpose system can be utilized to provide customized real-time access to significant data resources such as the NEXRAD system. Additionally, the creation of new types of virtual sensors is discussed, using the example of virtual temperature sensors. </p>
<li>Babbar-Sebens, M. & Minsker, B. (2011). <a href="http://www.sciencedirect.com/science/article/pii/S1568494611003371">Interactive Genetic Algorithm With  Mixed Initiative Interaction For Multi-Criteria Ground Water Monitoring Design</a>, <i>Applied Soft Computing</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid6');">Abstract</a><p class="more" id="hid6" style="display: none;">Design of optimal plans for environmental planning and management applications should ideally consider the multiple quantitative and qualitative criteria relevant to the problem. For example, in ground water monitoring design problems, qualitative criteria such as acceptable spatial extent and shape of the contaminant plume predicted from the monitored locations can be equally important as the typical quantitative criteria such as economic costs and contaminant prediction accuracy. Incorporation of qualitative criteria in the problem-solving process is typically done in one of two ways: (a) quantifying approximate representations of the qualitative criteria, which are then used as additional criteria during the optimization process, or (b) post-optimization analysis of designs by experts to evaluate the overall performance of the optimized designs with respect to the qualitative criteria. These approaches, however, may not adequately represent all of the relevant qualitative information that affect a human expert involved in design (e.g. engineers, stakeholders, regulators, etc.), and do not necessarily incorporate the effect of the expert's own learning process on the suitability of the final design. The Interactive Genetic Algorithm with Mixed Initiative Interaction (IGAMII) is a novel approach that addresses these limitations by using a collaborative humancomputer search strategy to assist users in designing optimized solutions to their applications, while also learning about their problem. The algorithm adaptively learns from the expert's feedback, and explores multiple designs that meet her/his criteria using both the human expert and a simulated model of the expert's responses in a collaborative fashion. The algorithm provides an introspection-based learning framework for the human expert and uses the human's subjective confidence measures to adjust the optimization search process to the transient learning process of the user. This paper presents the design and testing of this computational framework, and the benefits of using this approach for solving groundwater monitoring design problems.</p>
<li>Gopalakrishnan, G., Minsker, B. & Valocchi, A. (2011). <a href="http://pubs.acs.org/doi/abs/10.1021/es1042657?journalCode=esthag">Monitoring Network Design for Phytoremediation Systems Using Primary and Secondary Data Sources</a>, <i>Environmental Science and Technology</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid7');">Abstract</a><p class="more" id="hid7" style="display: none;">Phytoremediation, or contaminant removal using plants, has been deployed at many sites to remediate contaminated soil and groundwater. Research has shown that trees are low-cost, rapid, and relatively simple-to-use monitoring systems as well as inexpensive alternatives to traditional pump-and-treat systems. However, tree monitoring is also an indirect measure of subsurface contamination and inherently more uncertain than conventional techniques such as wells or soil borings that measure contaminant concentrations directly. This study explores the implications for monitoring network design at real-world sites where scarce primary data such as monitoring wells or soil borings are supplemented by extensive secondary data such as trees. In this study, we combined secondary and primary data into a composite data set using models to transform secondary data to primary, as primary data were too sparse to attempt cokriging. Optimal monitoring networks using both trees and conventional techniques were determined using genetic algorithms, and trade-off curves between cost and uncertainty are presented for a phytoremediation system at Argonne National Laboratory. Optimal solutions found at this site indicate that increasing the number of secondary data sampled resulted in a significant decrease in global uncertainty with a minimal increase in cost. The choice of the data transformation model had an impact on the optimal designs and uncertainty estimated at the site. Using a data transformation model with a higher error resulted in monitoring network designs where primary data were favored over colocated secondary data. The spatial configuration of the monitoring network design was similar with regard to the areas sampled, irrespective of the data transformation model used. Overall, this study shows that using a composite data set, with primary and secondary data, results in effective monitoring designs, even at sites where the only data transformation model available is one with significant error.</p>
<li>Yan, S., & Minsker, B. (2011). <a href="http://ascelibrary.org/doi/abs/10.1061/%28ASCE%29WR.1943-5452.0000106">Applying Dynamic Surrogate Models in Noisy Genetic Algorithms to Optimize Groundwater Remediation Design</a>, <i>Journal of Water Resources Planning and Management</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid8');">Abstract</a><p class="more" id="hid8" style="display: none;">Computational cost is a critical issue for large-scale water-resource optimization under uncertainty, since time-intensive MonteCarlo simulations are often required to evaluate over multiple parameter realizations. This paper presents an efficient approach for replacing most MonteCarlo simulations with surrogate models within a noisy genetic algorithm (GA). The surrogates are trained to predict the posterior expectations online on the basis of stochastic decision theory, using MonteCarlo simulation results created during the GA run. The surrogates, which in this application are neural networks, are adaptively updated to improve their prediction performance as the search progresses. A Latin hypercube sampling method is used to efficiently sample parameters for the MonteCarlo simulation, and the sampling results are archived so that the estimate of posterior expectation can be iteratively improved in an efficient manner. In addition, the GA is modified to incorporate hypothesis tests in its selection operator to account for sampling noise. The method is applied to a field-scale groundwater remediation design case study, whereas the primary source of uncertainty stems from hydraulic conductivity values in the aquifers. The results show that the method identified more reliable and cost-effective solutions with 8690% less computational effort than the purely physically based noisy GA approach.</p>
<li>Coopersmith, E., Minsker, B. & Montagna, P. (2011). <a href="http://www.iwaponline.com/jh/013/jh0130064.htm">Understanding and Forecasting Hypoxia Using Machine Learning Algorithms</a>, <i>Journal of Hydroinformatics</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid9');">Abstract</a><p class="more" id="hid9" style="display: none;">This study's primary objective lies in short-term forecasting of where and when hypoxia may transpire to enable observing its effects in real time, focusing on a case study in Corpus Christi Bay (Texas). Dissolved oxygen levels in this bay can be characterized by three temporal trends (daily, seasonal, and long-term). To predict hypoxic events, these three mathematical trends are isolated and extracted to obtain unbiased forecasts using a sequential normalization approach. Next, machine learning algorithms are constructed employing the continuous, normalized values from a variety of sensor locations. By including latitude and longitude coordinates as additional variables, a spatial depiction of hypoxic conditions can be illustrated effectively, allowing for more efficient summer data collection and more accurate, near-real-time projections. Using k-nearest neighbor and regression tree algorithms, approximate probabilities of observing hypoxia the following day were calculated, and estimates of dissolved oxygen levels were also computed. During periods in which hypoxia was observed, forecast probabilities of hypoxia exceeded 80%. Conversely, during periods in which no hypoxia was observed, the model's estimate remained below 20%. These results indicate that the modeling approach produces reasonable forecasts for this case study.</p>
<li>Gartia, M., Braunschweig, B., Chang, T.-W., Moinzadeh, P., Minsker, B., Agha, G., Wieckowski, A., Keefer, L. & Liu, G. (2012). <a href="http://www.ncbi.nlm.nih.gov/pubmed/23138753">The microelectronic wireless nitrate sensor network for environmental water monitoring</a>, <i>Journal of Environmental Monitoring</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid10');">Abstract</a><p class="more" id="hid10" style="display: none;">Quantitative monitoring of water conditions in a field is a critical ability for environmental science studies. We report the design, fabrication and testing of a low cost, miniaturized and sensitive electrochemical based nitrate sensor for quantitative determination of nitrate concentrations in water samples. We have presented detailed analysis for the nitrate detection results using the miniaturized sensor. We have also demonstrated the integration of the sensor to a wireless network and carried out field water testing using the sensor. We envision that the field implementation of the wireless water sensor network will enable "smart farming" and "smart environmental monitoring". ? The Royal Society of Chemistry 2012.</p>
<li>Wietsma, T. & Minsker, B. (2012). <a href="http://www.computer.org/csdl/proceedings/e-science/2012/4467/00/06404475-abs.html">Adaptive sampling of streaming signals</a>, <i>2012 IEEE 8th International Conference on E-Science, e-Science 2012</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid11');">Abstract</a><p class="more" id="hid11" style="display: none;">Higher sensor throughput has increased the demand for cyberinfrastructure, requiring those unfamiliar with large database management to acquire new skills or outsource. Some have called this shift from sensor-limited data collection the data deluge. As an alternative, we propose that the deluge is the result of sensor control software failing to keep pace with hardware capabilities. Rather than exploit the potential of powerful embedded operating systems and construct intelligent sensor networks that harvest higher quality data, the old paradigm (i.e. collect everything) is still dominant. To mitigate the deluge, we present an adaptive sampling algorithm based on the Nyquist-Shannon sampling theorem. We calibrate the algorithm for both data reduction and increased sampling over hot moments, which we define as periods of elevated signal activity, deviating from previous works which have emphasized adaptive sampling for data compression via minimization of signal reconstruction error. Under the feature extraction concept, samples drawn from user-defined events carry greater importance and effective control requires the researcher to describe the context of events in the form of both an identification heuristic (for calibration) and a real-time sampling model. This event-driven approach is important when observation is focused on intermittent dynamics. In our case study application, we develop a heuristic to identify hot moments from historical data and use it to train and evaluate the adaptive model in an offline analysis using soil moisture data. Results indicate the adaptive model is superior to uniform sampling, capable of extracting 20% to 100% more samples during hot moments at equivalent levels of overall efficiency.</p>
<li>Zimmer, A., Schmidt, A., Ostfeld, A. & Minsker, B. (2012). <a href="http://ascelibrary.org/doi/abs/10.1061/9780784412312.032">Computationally implicit hydraulics for real-time combined sewer overflow modeling and decision support</a>, <i>World Environmental and Water Resources Congress 2012: Crossing Boundaries, Proceedings of the 2012 Congress</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid12');">Abstract</a><p class="more" id="hid12" style="display: none;">Large-scale combined sewer systems necessitate accurate hydraulic models with a low computational requirements to depict combined sewer overflows (CSOs) in real-time. A hydraulic model is proposed that incorporates the mass and momentum equations into a series of look-up tables. Flow that enters the interceptors is routed downstream based on a hydraulic performance graph (HPG) which conserves momentum, and a volumetric performance graph (VPG) which conserves mass, established for each conduit. Weirs and sluice gates that control water distribution throughout the combined sewer system are also represented by look-up tables created off-line. During real-time computations, referencing the look-up tables is faster than computing the full equations. The model includes accurate sewer and deep tunnel components imported from Arc-GIS, and is shown to emulate EPA SWMM 5.0 dynamic wave results on a faster time scale. Calibration and timing results show that the model may be successfully applied to evaluate potential operating scenarios more quickly than SWMM. ? 2012 ASCE.</p>
<li>Babbar-Sebens ,M. & Minsker, B. (2012). <a href="http://www.sciencedirect.com/science/article/pii/S1568494611003371">Interactive Genetic Algorithm with Mixed Initiative Interaction for multi-criteria ground water monitoring design</a>, <i>Applied Soft Computing Journal</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid13');">Abstract</a><p class="more" id="hid13" style="display: none;">Design of optimal plans for environmental planning and management applications should ideally consider the multiple quantitative and qualitative criteria relevant to the problem. For example, in ground water monitoring design problems, qualitative criteria such as acceptable spatial extent and shape of the contaminant plume predicted from the monitored locations can be equally important as the typical quantitative criteria such as economic costs and contaminant prediction accuracy. Incorporation of qualitative criteria in the problem-solving process is typically done in one of two ways: (a) quantifying approximate representations of the qualitative criteria, which are then used as additional criteria during the optimization process, or (b) post-optimization analysis of designs by experts to evaluate the overall performance of the optimized designs with respect to the qualitative criteria. These approaches, however, may not adequately represent all of the relevant qualitative information that affect a human expert involved in design (e.g. engineers, stakeholders, regulators, etc.), and do not necessarily incorporate the effect of the expert's own learning process on the suitability of the final design. The Interactive Genetic Algorithm with Mixed Initiative Interaction (IGAMII) is a novel approach that addresses these limitations by using a collaborative human-computer search strategy to assist users in designing optimized solutions to their applications, while also learning about their problem. The algorithm adaptively learns from the expert's feedback, and explores multiple designs that meet her/his criteria using both the human expert and a simulated model of the expert's responses in a collaborative fashion. The algorithm provides an introspection-based learning framework for the human expert and uses the human's subjective confidence measures to adjust the optimization search process to the transient learning process of the user. This paper presents the design and testing of this computational framework, and the benefits of using this approach for solving groundwater monitoring design problems. ? 2011 Elsevier B.V. All rights reserved.</p>
<li>Hill, D., Liu, Y., Marini, L., Kooper, R., Rodriguez, A., Futrelle, J., Minsker, B., Myers, J. & McLaren, T. (2011). <a href="http://www.sciencedirect.com/science/article/pii/S1364815211001988">A virtual sensor system for user-generated, real-time environmental data products</a>, <i>Environmental Modelling and Software</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid14');">Abstract</a><p class="more" id="hid14" style="display: none;">With the advent of new instrumentation and sensors, more diverse types and increasing amounts of data are becoming available to environmental researchers and practitioners. However, accessing and integrating these data into forms usable for environmental analysis and modeling can be highly time-consuming and challenging, particularly in real time. For example, radar-rainfall data are a valuable resource for hydrologic modeling because of their high resolution and pervasive coverage. However, radar-rainfall data from the Next Generation Radar (NEXRAD) system continue to be underutilized outside of the operational environment because of limitations in access and availability of research-quality data products, especially in real time. This paper addresses these issues through the development of a prototype Web-based virtual sensor system at NCSA that creates real-time customized data streams from raw sensor data. These data streams are supported by metadata, including provenance information. The system uses workflow composition and publishing tools to facilitate creation and publication (as Web services) of user-created virtual sensors. To demonstrate the system, two case studies are presented. In the first case study, a network of point-based virtual precipitation sensors is deployed to analyze the relationship between radar-rainfall measurements, and in the second case study, a network of polygon-based virtual precipitation sensors is deployed to be used as input to urban flooding models. These case studies illustrate how, with the addition of some application-specific information, this general-purpose system can be utilized to provide customized real-time access to significant data resources such as the NEXRAD system. Additionally, the creation of new types of virtual sensors is discussed, using the example of virtual temperature sensors. ? 2011 Elsevier Ltd.</p>
<li>Zimmer, A., Minsker, B., Schmidt, A. & Ostfeld, A. (2011). <a href="http://eisa.ncsa.illinois.edu/Documents/Conference/Zimmer_etal_EWRI2011.pdf">Benefits of meta-model validation for real-time sewer system decision support</a>, <i>World Environmental and Water Resources Congress 2011: Bearing Knowledge for Sustainability - Proceedings of the 2011 World Environmental and Water Resources Congress</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid15');">Abstract</a><p class="more" id="hid15" style="display: none;">Large-scale combined sewer systems are susceptible to overflows (CSOs) during heavy storm events. Management strategies that partition water flow into the sewers or nearby waterways may be based on conservative operational rules designed to prevent possible flow instabilities. However, these operations may not effectively utilize system storage capacity for all types of storm events. Real-time adaptation of system operating rules can reduce overflows while continuing to avoid hydraulic conditions that lead to transients and geysers. In this study, realtime genetic algorithm (GA) optimization is evaluated for its success in minimizing CSOs for a test case modeled after a portion of the Chicago Tunnel and Reservoir Plan (TARP). ? 2011 ASCE.</p>
<li>Chinta I., Minsker B.S. (2011). <a href="http://ascelibrary.org/doi/abs/10.1061/41114%28371%2939">Forecasting hypoxia in Corpus Christi Bay, Texas by model fusion</a>, <i>World Environmental and Water Resources Congress 2011: Bearing Knowledge for Sustainability - Proceedings of the 2011 World Environmental and Water Resources Congress</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid16');">Abstract</a><p class="more" id="hid16" style="display: none;">This study aims to create more accurate and efficient near-real-time forecasts of hypoxia that will give researchers advance notice for manual sampling during hypoxic events. Hypoxic or dead zones, which occur when dissolved oxygen levels in water drop below 2 mg/L, are prevalent worldwide. An example of such an hypoxic zone forms intermittently in Corpus Christi Bay (CC Bay), Texas, a USEPA-recognized estuary of national significance. Hypoxia in CC Bay is caused by inflow of hypersaline waters that enter from adjacent bays and estuaries, natural fluctuations in oxygen levels due to the oxygen production-consumption cycle of the aquatic flora and fauna, seasonal fluctuations, and discharges from several wastewater treatment plants. The hypoxia forecasting method tested in this work involves a suite of data-driven model fusion techniques such as historical scenario modeling and boosting both a k-nearest neighbor (KNN) algorithm and the historical scenario model. Existing data-driven k-nearest neighbor and physics-based valve models are used as the basis for the model fusion. The historical scenario model combines the k-nearest neighbor algorithm with the valve model to predict the probability of hypoxia twenty-four hours ahead. Boosting involves training the model repeatedly on subsets of the training dataset. The results of the fused models are compared with those of the individual models to test the effectiveness of model fusion in predicting the estuarine conditions. The results showed that the valve model, which has been hitherto computing oxygen profiles, can be extended to forecast probabilities of hypoxia when combined with the k-nearest neighbor algorithm to form the historical scenario model. The findings also show that boosting significantly enhances the performance of the k-nearest neighbor algorithm and the historical scenario model, although further testing on more extensive continuous datasets is needed to verify the findings in other locations. The results show promise for model fusion to be effective for real-time forecasting in hypoxia-affected water bodies. ? 2011 ASCE.</p>
<li>Gopalakrishnan, G., Minsker, B. & Valocchi, A. (2011). <a href="http://www.ncbi.nlm.nih.gov/pubmed/21557573">Monitoring network design for phytoremediation systems using primary and secondary data sources</a>, <i>Environmental Science and Technology</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid17');">Abstract</a><p class="more" id="hid17" style="display: none;">Phytoremediation, or contaminant removal using plants, has been deployed at many sites to remediate contaminated soil and groundwater. Research has shown that trees are low-cost, rapid, and relatively simple-to-use monitoring systems as well as inexpensive alternatives to traditional pump-and-treat systems. However, tree monitoring is also an indirect measure of subsurface contamination and inherently more uncertain than conventional techniques such as wells or soil borings that measure contaminant concentrations directly. This study explores the implications for monitoring network design at real-world sites where scarce primary data such as monitoring wells or soil borings are supplemented by extensive secondary data such as trees. In this study, we combined secondary and primary data into a composite data set using models to transform secondary data to primary, as primary data were too sparse to attempt cokriging. Optimal monitoring networks using both trees and conventional techniques were determined using genetic algorithms, and trade-off curves between cost and uncertainty are presented for a phytoremediation system at Argonne National Laboratory. Optimal solutions found at this site indicate that increasing the number of secondary data sampled resulted in a significant decrease in global uncertainty with a minimal increase in cost. The choice of the data transformation model had an impact on the optimal designs and uncertainty estimated at the site. Using a data transformation model with a higher error resulted in monitoring network designs where primary data were favored over colocated secondary data. The spatial configuration of the monitoring network design was similar with regard to the areas sampled, irrespective of the data transformation model used. Overall, this study shows that using a composite data set, with primary and secondary data, results in effective monitoring designs, even at sites where the only data transformation model available is one with significant error. ? 2011 American Chemical Society.</p>
<li>Yan, S. & Minsker, B. (2011). <a href="http://ascelibrary.org/doi/abs/10.1061/%28ASCE%29WR.1943-5452.0000106">Applying dynamic surrogate models in noisy genetic algorithms to optimize groundwater remediation designs</a>, <i>Journal of Water Resources Planning and Management</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid18');">Abstract</a><p class="more" id="hid18" style="display: none;">Computational cost is a critical issue for large-scale water-resource optimization under uncertainty, since time-intensive Monte Carlo simulations are often required to evaluate over multiple parameter realizations. This paper presents an efficient approach for replacing most Monte Carlo simulations with surrogate models within a noisy genetic algorithm (GA). The surrogates are trained to predict the posterior expectations online on the basis of stochastic decision theory, using Monte Carlo simulation results created during the GA run. The surrogates, which in this application are neural networks, are adaptively updated to improve their prediction performance as the search progresses. A Latin hypercube sampling method is used to efficiently sample parameters for the Monte Carlo simulation, and the sampling results are archived so that the estimate of posterior expectation can be iteratively improved in an efficient manner. In addition, the GA is modified to incorporate hypothesis tests in its selection operator to account for sampling noise. The method is applied to a field-scale groundwater remediation design case study, whereas the primary source of uncertainty stems from hydraulic conductivity values in the aquifers. The results show that the method identified more reliable and cost-effective solutions with 86-90% less computational effort than the purely physically based noisy GA approach. ? 2011 American Society of Civil Engineers.</p>
<li>Coopersmith, E., Minsker, B. & Montagna, P. (2011). <a href="http://www.iwaponline.com/jh/013/jh0130064.htm">Understanding and forecasting hypoxia using machine learning algorithms</a>, <i>Journal of Hydroinformatics</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid19');">Abstract</a><p class="more" id="hid19" style="display: none;">This study's primary objective lies in short-term forecasting of where and when hypoxia may transpire to enable observing its effects in real time, focusing on a case study in Corpus Christi Bay (Texas). Dissolved oxygen levels in this bay can be characterized by three temporal trends (daily, seasonal, and long-term). To predict hypoxic events, these three mathematical trends are isolated and extracted to obtain unbiased forecasts using a sequential normalization approach. Next, machine learning algorithms are constructed employing the continuous, normalized values from a variety of sensor locations. By including latitude and longitude coordinates as additional variables, a spatial depiction of hypoxic conditions can be illustrated effectively, allowing for more efficient summer data collection and more accurate, near-real-time projections. Using k-nearest neighbor and regression tree algorithms, approximate probabilities of observing hypoxia the following day were calculated, and estimates of dissolved oxygen levels were also computed. During periods in which hypoxia was observed, forecast probabilities of hypoxia exceeded 80%. Conversely, during periods in which no hypoxia was observed, the model's estimate remained below 20%. These results indicate that the modeling approach produces reasonable forecasts for this case study. ? IWA Publishing 2010.</p>
<li>Hill, D., Minsker, B. & Amir, E. (2010). <a href="http://onlinelibrary.wiley.com/doi/10.1029/2008WR006956/abstract">Real-time Bayesian anomaly detection in streaming environmental data</a>, <i>Water Resources Research</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid20');">Abstract</a><p class="more" id="hid20" style="display: none;">With large volumes of data arriving in near real time from environmental sensors, there is a need for automated detection of anomalous data caused by sensor or transmission errors or by infrequent system behaviors. This study develops and evaluates three automated anomaly detection methods using dynamic Bayesian networks (DBNs), which perform fast, incremental evaluation of data as they become available, scale to large quantities of data, and require no a priori information regarding process variables or types of anomalies that may be encountered. This study investigates these methods' abilities to identify anomalies in eight meteorological data streams from Corpus Christi, Texas. The results indicate that DBN-based detectors, using either robust Kalman filtering or Rao-Blackwellized particle filtering, outperform a DBN-based detector using Kalman filtering, with the former having false positive/negative rates of less than 2%. These methods were successful at identifying data anomalies caused by two real events: a sensor failure and a large storm. Copyright 2009 by the American Geophysical Union.</p>
<li>Babbar-Sebens, M. & Minsker, B. (2010). <a href="http://www.sciencedirect.com/science/article/pii/S1364815210000861">A Case-Based Micro Interactive Genetic Algorithm (CBMIGA) for interactive learning and search: Methodology and application to groundwater monitoring design</a>, <i>Environmental Modelling and Software</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid21');">Abstract</a><p class="more" id="hid21" style="display: none;">Interactive optimization algorithms use real-time interaction to include decision maker preferences based on the subjective quality of evolving solutions. In water resources management problems where numerous qualitative criteria exist, use of such interactive optimization methods can facilitate in the search for comprehensive and meaningful solutions for the decision maker. The decision makers using such a system are, however, likely to go through their own learning process as they view new solutions and gain knowledge about the design space. This leads to temporal changes (nonstationarity) in their preferences that can impair the performance of interactive optimization algorithms. This paper proposes a new interactive optimization algorithm - Case-Based Micro Interactive Genetic Algorithm - that uses a case-based memory and case-based reasoning to manage the effects of nonstationarity in decision maker's preferences within the search process without impairing the performance of the search algorithm. This paper focuses on exploring the advantages of such an approach within the domain of groundwater monitoring design, though it is applicable to many other problems. The methodology is tested under non-stationary preference conditions using simulated and real human decision makers, and it is also compared with a non-interactive genetic algorithm and a previous version of the interactive genetic algorithm. ? 2010 Elsevier Ltd.</p>
<li>Hill, D. & Minsker, B. (2010). <a href="http://www.sciencedirect.com/science/article/pii/S1364815209002321">Anomaly detection in streaming environmental sensor data: A data-driven modeling approach</a>, <i>Environmental Modelling and Software</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid22');">Abstract</a><p class="more" id="hid22" style="display: none;">The deployment of environmental sensors has generated an interest in real-time applications of the data they collect. This research develops a real-time anomaly detection method for environmental data streams that can be used to identify data that deviate from historical patterns. The method is based on an autoregressive data-driven model of the data stream and its corresponding prediction interval. It performs fast, incremental evaluation of data as it becomes available, scales to large quantities of data, and requires no pre-classification of anomalies. Furthermore, this method can be easily deployed on a large heterogeneous sensor network. Sixteen instantiations of this method are compared based on their ability to identify measurement errors in a windspeed data stream from Corpus Christi, Texas. The results indicate that a multilayer perceptron model of the data stream, coupled with replacement of anomalous data points, performs well at identifying erroneous data in this data stream. ? 2009.</p>
<li>Liu, Y., Hill, D., Myers, J. & Minsker, B. (2010). <a href="http://ascelibrary.org/doi/abs/10.1061/41114%28371%2938">Integrated real time geospatial sensor web and visual analytics for environmental decision support</a>, <i>World Environmental and Water Resources Congress 2010: Challenges of Change - Proceedings of the World Environmental and Water Resources Congress 2010</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid23');">Abstract</a><p class="more" id="hid23" style="display: none;">In this paper, we present an integrated Real-Time Geospatial Sensor Web and Visual Analytics systEm (RT-GeoSWAVE), which allows user-driven creation and visualization of derived "virtual sensors" that transform and repurpose existing sensors for real-time situational awareness. We illustrate this cyber-sensing system through a near-real-time urban flooding and sewer overflow scenario in Chicago, where NEXRAD Level II data are converted through a series of spatial, temporal and thematic transformations to a group of persistent polygon-based virtual rainfall sensors at the urban hydrological unit scale (i.e., sewersheds), which in turn can be visualized on demand as a real-time animated movie in space and time on the integrated Web-based Google Earth to assess rainfall patterns in an urban sewershed. Such a system is invaluable for real-time decision support as the spatial distribution of intense rainfall significantly impacts the triggering and behavior of urban flooding and sewer overflows. Furthermore, the system also demonstrates a critical capability for environmental observatories that will enable their users to dynamically evolve a suite of derived data products to meet changing research and management needs. ? 2010 ASCE.</p>
<li>Chinta, I. & Minsker, B. (2010). <a href="http://ascelibrary.org/doi/abs/10.1061/41114%28371%2939">Forecasting hypoxia in Corpus Christi Bay, Texas by model fusion</a>, <i>World Environmental and Water Resources Congress 2010: Challenges of Change - Proceedings of the World Environmental and Water Resources Congress 2010</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid24');">Abstract</a><p class="more" id="hid24" style="display: none;">Hypoxic or dead zones, which occur when dissolved oxygen levels in water drop below 2 mg/L, are prevalent worldwide. An example of such an hypoxic zone forms intermittently in Corpus Christi Bay (CCBay), Texas, a USEPA-recognized estuary of national significance. Hypoxia in CCBay occurs primarily due to inflow of hypersaline waters that enter from adjacent bays and estuaries, fluctuations in oxygen levels due to the oxygen production-consumption cycle of the aquatic flora and fauna, seasonal fluctuations, and lastly, discharges from several wastewater treatment plants. The aim of this paper is to create more accurate and efficient near-real-time forecasts of hypoxia that will give researchers advance notice for manual sampling during hypoxic events. The approach involves developing and testing a suite of data-driven model fusion approaches with the help of cyberinfrastructure. The models that will be used as a part of this study are: a data-driven k-nearest neighbor model (Coopersmith, 2008), and a physics-based valve model (Sin Chit To, 2009). The k-nearest neighbor predicts the probability of occurrence of hypoxia 24 hours later, and the levels of dissolved oxygen. On the other hand, the valve model determines the distance traveled by a gravity-current upon entering Corpus Christi Bay from adjacent estuaries and bays and the period of time it persists. A comparison will be made between the results of the fused model and those of the individual models to test the effectiveness of model fusion in predicting the estuarine condition in the model. ? 2010 ASCE.</p>
<li>Zimmer, A., Minsker, B., Schmidt, A. & Ostfeld, A. (2010). <a href="http://ascelibrary.org/doi/abs/10.1061/41114%28371%29232">Evolutionary algorithm memory enhancement for real-time CSO control</a>, <i>World Environmental and Water Resources Congress 2010: Challenges of Change - Proceedings of the World Environmental and Water Resources Congress 2010</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid25');">Abstract</a><p class="more" id="hid25" style="display: none;">Control of combined sewer overflows (CSOs) may be enhanced through real-time decision support. An optimization algorithm adapted for changing rainfall is used to dynamically control complex sewer hydraulics to minimize CSO volume. Different methods of enhancing the optimization for real-time processing consist of: separating the hydraulic model for multi-objective optimization or to optimize only critical portions of the sewer system, using an efficient optimization technique, and incorporating memory into the optimization to speed convergence to a solution for each forecasted rainfall change. Potential optimal management solutions and the associated environmental characteristics can be stored and used to re-initialize the optimization at each environmental change. The memory may also be altered to indicate what precision of hydraulic model should be used for different rainfall conditions. ? 2010 ASCE.</p>
<li>Nicklow, J., Reed, P., Savic, D., Dessalegne, T., Harrell, L., Chan-Hilton, A., Karamouz, M., Minsker, B., Ostfeld, A., Singh, A. & Zechman, E. (2010). <a href="http://ascelibrary.org/doi/abs/10.1061/%28ASCE%29WR.1943-5452.0000053">State of the art for genetic algorithms and beyond in water resources planning and management</a>, <i>Journal of Water Resources Planning and Management</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid26');">Abstract</a><p class="more" id="hid26" style="display: none;">During the last two decades, the water resources planning and management profession has seen a dramatic increase in the development and application of various types of evolutionary algorithms (EAs). This observation is especially true for application of genetic algorithms, arguably the most popular of the several types of EAs. Generally speaking, EAs repeatedly prove to be flexible and powerful tools in solving an array of complex water resources problems. This paper provides a comprehensive review of state-of-the-art methods and their applications in the field of water resources planning and management. A primary goal in this ASCE Task Committee effort is to identify in an organized fashion some of the seminal contributions of EAs in the areas of water distribution systems, urban drainage and sewer systems, water supply and wastewater treatment, hydrologic and fluvial modeling, groundwater systems, and parameter identification. The paper also identifies major challenges and opportunities for the future, including a call to address larger-scale problems that are wrought with uncertainty and an expanded need for cross fertilization and collaboration among our field's subdisciplines. Evolutionary computation will continue to evolve in the future as we encounter increased problem complexities and uncertainty and as the societal pressure for more innovative and efficient solutions rises. ? 2010 ASCE.</p>
<li>Singh, A., Walker, D., Minsker, B. & Valocchi, A. (2010). <a href="http://link.springer.com/article/10.1007%2Fs00477-010-0384-1">Incorporating subjective and stochastic uncertainty in an interactive multi-objective groundwater calibration framework</a>, <i>Stochastic Environmental Research and Risk Assessment</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid27');">Abstract</a><p class="more" id="hid27" style="display: none;">The interactive multi-objective genetic algorithm (IMOGA) combines traditional optimization with an interactive framework that considers the subjective knowledge of hydro-geological experts in addition to quantitative calibration measures such as calibration errors and regularization to solve the groundwater inverse problem. The IMOGA is inherently a deterministic framework and identifies multiple large-scale parameter fields (typically head and transmissivity data are used to identify transmissivity fields). These large-scale parameter fields represent the optimal trade-offs between the different criteria (quantitative and qualitative) used in the IMOGA. This paper further extends the IMOGA to incorporate uncertainty both in the large-scale trends as well as the small-scale variability (which can not be resolved using the field data) in the parameter fields. The different parameter fields identified by the IMOGA represent the uncertainty in large-scale trends, and this uncertainty is modeled using a Bayesian approach where calibration error, regularization, and the expert's subjective preference are combined to compute a likelihood metric for each parameter field. Small-scale (stochastic) variability is modeled using a geostatistical approach and added onto the large-scale trends identified by the IMOGA. This approach is applied to the Waste Isolation Pilot Plant (WIPP) case-study. Results, with and without expert interaction, are analyzed and the impact that expert judgment has on predictive uncertainty at the WIPP site is discussed. It is shown that for this case, expert interaction leads to more conservative solutions as the expert compensates for some of the lack of data and modeling approximations introduced in the formulation of the problem. ? 2010 Springer-Verlag.</p>
<li>Singh, A., Minsker, B. & Bajcsy, P. (2010). <a href="http://isda.ncsa.illinois.edu/drupal/publication/image-based-machine-learning-reduction-user-fatigue-interactive-model-calibration-system">Image-based machine learning for reduction of user fatigue in an interactive model calibration system</a>, <i>Journal of Computing in Civil Engineering</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid28');">Abstract</a><p class="more" id="hid28" style="display: none;">The interactive multiobjective genetic algorithm (IMOGA) is a promising new approach to calibrate models. The IMOGA combines traditional optimization with an interactive framework, thus allowing both quantitative calibration criteria as well as the subjective knowledge of experts to drive the search for model parameters. One of the major challenges in using such interactive systems is the burden they impose on the experts that interact with the system. This paper proposes the use of a novel image-based machine-learning (IBML) approach to reduce the number of user interactions required to identify promising calibration solutions involving spatially distributed parameter fields (e.g., hydraulic conductivity parameters in a groundwater model). The first step in the IBML approach involves selecting a few highly representative solutions for expert ranking. The selection is performed using unsupervised clustering approaches from the field of image processing, which group potential parameter fields based on their spatial similarities. The expert then ranks these representative solutions, after which a machine-learning model (augmented with the spatial information of the selected fields) is trained to learn user preferences and predict rankings for solutions not ranked by the expert. To better mimic the "visual" information processing of human experts, algorithms from the field of image processing are used to mine information about the spatial characteristics of parameter fields, thus improving the performance of the clustering and machine-learning algorithms. The IBML approach is tested and demonstrated on a groundwater calibration problem and is shown to lead to significant improvements, reducing the amount of user interaction by as much as half without compromising the solution quality of the IMOGA. ? 2010 ASCE.</p>
<br><br><a href="#P2010"><h4>Publications previous to 2010</h4></a>
<ul class="a"><li class="am_dropdown">Hill, D., Minsker, B. & Schmidt, A. (2009). <a href="http://ascelibrary.org/doi/abs/10.1061/41036%28342%29220">Predicting CSOs for real time decision support</a>, <i>Proceedings of World Environmental and Water Resources Congress 2009 - World Environmental and Water Resources Congress 2009: Great Rivers</i>.
<br><a href="javascript:void(0)" onclick="toggle('hid29');">Abstract</a><p class="more" id="hid29" style="display: none;">This paper presents a novel data-driven method for modeling combined sewer overflows (CSOs) in real-time. This method treats CSO event generation as a threshold process that is triggered by increasingly intense rainfall events, and predicts the likelihood of a CSO given input conditions using a Bayesian network. The fusion of relevant data from multiple agencies into a unified data stream in real time is described, and a hierarchical modeling strategy is proposed that will facilitate the exploration of the causes of CSOs and direct research into the adaptive management of combined sewer systems using the Chicago wastewater system as a case study. ? 2009 ASCE.</p>
<li>Zimmer, A., Hill, D., Minsker, B., Ostfeld, A. & Schmidt, A. (2009). <a href="http://ascelibrary.org/doi/abs/10.1061/41036%28342%29110">Evolutionary optimization of combined sewer overflow control</a>, <i>Proceedings of World Environmental and Water Resources Congress 2009 - World Environmental and Water Resources Congress 2009: Great Rivers</i>
<br><a href="javascript:void(0)" onclick="toggle('hid30');">Abstract</a><p class="more" id="hid30" style="display: none;">Model predictive control (MPC) is coupled with a real-coded Genetic Algorithm to predict a decision sequence that minimizes combined sewer overflow (CSO) volume for a 3-hour rainfall event over a hypothetical sewer system. Rainfall is transformed to overland runoff through the cell model which depicts each sewershed (draining to an overflow dropshaft) by two linear reservoirs in series, and water entering the interceptor is routed downstream to establish water levels at the dropshaft connections. A pumping rate at the most downstream end of the interceptor plus one sluice gate position for each dropshaft connection will be altered to produce the best control strategy. Resulting management scenarios disperse overflows differently throughout the sewer, but may yield similar overflow volumes. This paper describes the simulation approach taken and displays the overflow distribution for favorable control sequences. ? 2009 ASCE.</p></li>
<li>Dawsey, W., Minsker, B. & Ostfeld, A. (2009). <a href="http://ascelibrary.org/doi/abs/10.1061/41036%28342%2964">Analysis of model sensitivity and uncertainty for chlorine transport and decay in a water distribution system</a>, <i>Proceedings of World Environmental and Water Resources Congress 2009 - World Environmental and Water Resources Congress 2009: Great Rivers</i>
<br><a href="javascript:void(0)" onclick="toggle('hid31');">Abstract</a><p class="more" id="hid31" style="display: none;">There are a number of sources of uncertainty in drinking water distribution system modeling. Uncertain parameters include pipe diameters, consumer demands, hydraulic energy loss coefficients, reaction coefficients and others. Understanding the relative importance of these sources of uncertainty can improve the allocation of resources for model refinement and calibration, as well as, aid knowledge inference from monitoring data. This paper presents an analysis of uncertainty and model sensitivity for chlorine transport and decay in a water distribution system. A clustering and global variance-based sensitivity methodology is proposed to account for spatial inconsistencies found in the results of previous studies of this problem. Results are presented from small and large scalecase studies. This methodology is then used to explore the occurrence of intrusion events in a water distribution system, and the potential to detect such events through online monitoring of chlorine residual concentrations. Noise present in the chlorine monitoring signal has the potential to overwhelm the detection of an upstream intrusion and its associated chlorine demand. Results are presented from simulated intrusion events of varying magnitude and duration. ? 2009 ASCE.</p></li>
<li>Liu, Y., Hill, D., Rodriguez, A., Marini, L., Kooper, R., Myers, J., Wu, X. & Minsker, B. (2009). <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5067462&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5067462">A new framework for on-demand virtualization, repurposing and fusion of heterogeneous sensors</a>, <i>2009 International Symposium on Collaborative Technologies and Systems, CTS 2009</i>
<br><a href="javascript:void(0)" onclick="toggle('hid32');">Abstract</a><p class="more" id="hid32" style="display: none;">This paper describes our first step towards the realization of complex and large scale cross-organization virtual observatories by presenting a new semantically-enhanced "Sensor Network as a Service" (SNaaS) framework, which can repurpose existing sensor networks as needed and aggregate and fuse heterogeneous sensors into new virtual sensors in near-real-time. The architecture of this system allows users to create virtual sensors in a Web 2.0 collaborative map interface. Components of the system are highlighted in the paper including a semantically enhanced streaming data toolkit, virtual sensor ontologies and management middleware. Case studies are presented which can allow users to create new virtual rain gages based on the NEXRAD (Next Generation Weather Radar) data stream with or without in-situ rain gages on demand in a Chicago urban watershed testbed. The resulting virtual sensor data streams then can be published in multiple formats including a SWE-compliant one so that external SWE-compliant users and applications can seamlessly query and integrate them. ?2009 IEEE.</p></li>
<li>Demissie, Y., Valocchi, A., Minsker, B. & Bailey, B. (2009). <a href="http://www.sciencedirect.com/science/article/pii/S002216940800543X">Integrating a calibrated groundwater flow model with error-correcting data-driven models to improve predictions</a>, <i>Journal of Hydrology</i>
<br><a href="javascript:void(0)" onclick="toggle('hid33');">Abstract</a><p class="more" id="hid33" style="display: none;">Physically-based groundwater models (PBMs), such as MODFLOW, contain numerous parameters which are usually estimated using statistically-based methods, which assume that the underlying error is white noise. However, because of the practical difficulties of representing all the natural subsurface complexity, numerical simulations are often prone to large uncertainties that can result in both random and systematic model error. The systematic errors can be attributed to conceptual, parameter, and measurement uncertainty, and most often it can be difficult to determine their physical cause. In this paper, we have developed a framework to handle systematic error in physically-based groundwater flow model applications that uses error-correcting data-driven models (DDMs) in a complementary fashion. The data-driven models are separately developed to predict the MODFLOW head prediction errors, which were subsequently used to update the head predictions at existing and proposed observation wells. The framework is evaluated using a hypothetical case study developed based on a phytoremediation site at the Argonne National Laboratory. This case study includes structural, parameter, and measurement uncertainties. In terms of bias and prediction uncertainty range, the complementary modeling framework has shown substantial improvements (up to 64% reduction in RMSE and prediction error ranges) over the original MODFLOW model, in both the calibration and the verification periods. Moreover, the spatial and temporal correlations of the prediction errors are significantly reduced, thus resulting in reduced local biases and structures in the model prediction errors.</p></li>
<li>Singh, A., Walker, D., Minsker, B. & Valocchi, A. (2008). <a href="http://ascelibrary.org/doi/abs/10.1061/40976%28316%29567">Interactive multi-objective inverse groundwater modelling - Incorporating subjective knowledge and conceptual uncertainty</a>, <i>World Environmental and Water Resources Congress 2008: Ahupua'a - Proceedings of the World Environmental and Water Resources Congress 2008</i>
<br><a href="javascript:void(0)" onclick="toggle('hid34');">Abstract</a><p class="more" id="hid34" style="display: none;">This paper addresses the important question of uncertainty assessment for predictions obtained from an interactive multi-objective groundwater inverse framework (proposed by the authors). This framework is based on an interactive multi-objective genetic algorithm (IMOGA) and considers subjective user preferences in addition to quantitative calibration measures such as calibration errors and regularization to solve the groundwater inverse problem. Given these criteria the IMOGA converges to a set of Pareto optimal parameter fields (transmissivity, in this case) that represent the best trade-off among all (qualitative as well as quantitative) objectives. Predictive uncertainty analysis for the IMOGA consists of assessing the uncertainty in the transmissivity fields found by the IMOGA, and the impact this uncertainty has on model predictions. To do this, we propose a multi-level sampling approach, incorporating uncertainty in both large-scale trends and the small-scale stochastic variability in the transmissivity fields found by the IMOGA. The multiple solutions found by the IMOGA are considered alternative models of the large-scale structure of the transmissivity field. Small-scale uncertainty is considered to be conditioned on the large-scale trend and correlated with a specified covariance structure. The prediction model is run using all simulated fields to obtain the distribution of predictions, which are then combined using model averaging approaches such as GLUE (generalized likelihood uncertainty estimation) and MLBMA (maximum likelihood Bayesian model averaging). The methodology has been applied to a field-scale case study based on the Waste Isolation Pilot Plant (WIPP) situated in Carlsbad, New Mexico. Results, with and without expert interaction, are analyzed and the impact expert judgment has on predictive uncertainty at the WIPP site are also discussed. It is shown that for this case expert interaction leads to more conservative solutions as the expert compensates for some of the lack of data and modeling approximations introduced in the formulation of the problem. ? 2008 ASCE.</p></li>
<li>Hill, D., Liu, Y., Myers, J. & Minsker, B. (2008). <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4736788&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4736788">End-to-end cyberinfrastructure for real-time enviornmental decision support</a>, <i>Proceedings - 4th IEEE International Conference on eScience, eScience 2008</i>
<br><a href="javascript:void(0)" onclick="toggle('hid35');">Abstract</a><p class="more" id="hid35" style="display: none;">Recent advances in sensor technology are facilitating the deployment of sensor networks into the environment that can produce measurements at high spatial and/or temporal resolutions. Through telemetry, these measurements can be delivered in near real time as data streams for use in real-time applications. These data streams can be instrumental in furthering our understanding of the environment, in monitoring and modeling the quality of the environments in which the sensors are deployed, and in providing real-time decision support for managing environmental systems. However, in order to realize these benefits, several challenges must be addressed, such as accessing heterogeneous data streams from multiple agencies; validating the data and fusing or transforming them for a desired application; providing high-dimensional models, visualizations, and decision support systems to explore data and forecast future conditions; and sharing the resulting knowledge. This poster will present an end-to-end cyberinfrastructure that can make these tasks possible in near real time, using a case study on real-time decision support for the combined sewer system in the Chicago Metropolitan Area. ? 2008 IEEE.</p></li>
<li>Liu, Y., Hill, D., Rodriguez, A., Marini, L., Kooper, R., Futrelle, J., Minsker, B.  & Myers, J. (2008). <a href="http://isda.ncsa.illinois.edu/drupal/publication/near-real-time-precipitation-virtual-sensor-using-nexrad-data">Near-real-time precipitation virtual sensor using NEXRAD data</a>, <i>GIS: Proceedings of the ACM International Symposium on Advances in Geographic Information Systems</i>
<br><a href="javascript:void(0)" onclick="toggle('hid36');">Abstract</a><p class="more" id="hid36" style="display: none;">In this demonstration paper, we describe the technologies and implementations that allow near real-time creation of new virtual precipitation sensors using NEXRAD Level II streaming data at user-specified point locations and time intervals in an integrated digital watershed with a Google Map-based web interface. The spatiotemporal and thematic transformation steps to produce such new time series data stream are implemented as a set of scientific workflows. A streaming data ontology is developed to handle temporal proximity concepts such as "previous" and "next" for irregular temporal data streams. Data and metadata management is provided by a semantic content management middleware. The new point-based virtual sensor can lower the barriers of using NEXRAD data for many hydrological applications. ? 2008 ACM.</p></li>
<li>Liu, Y., Marini, L., Kooper, R., Rodriguez, A., Hill, D., Myers, J. & Minsker, B. (2008). <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4736809&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4736809">Virtual sensors in a Web 2.0 virtual watershed</a>, <i>Proceedings - 4th IEEE International Conference on eScience, eScience 2008</i>
<br><a href="javascript:void(0)" onclick="toggle('hid37');">Abstract</a><p class="more" id="hid37" style="display: none;">This paper presents a Web 2.0 virtual observatory framework applied in an environmental watershed research context, where users not only can access existing sensor data such as the USGS (United States Geological Survey) rain gage data in near real-time, but also can create and share virtual sensors and trigger their associated workflows on-the-fly. Categories of virtual sensors are discussed and community participation and collaboration on creating virtual sensors can be promoted An eScience use case which allows users to create virtual rain gages on a Google map front end using NEXRAD (Next Generation Weather Radar) data is presented ? 2008 IEEE.</p></li>
<li>Demissie, Y., Valocchi, A., Minsker, B. & Bailey, B. (2008). <a href="http://eisa.ncsa.illinois.edu/Documents/Conference/Demissi_etal_ModelCare2008.pdf">Bias-corrected groundwater model prediction uncertainty analysis</a>, <i>IAHS-AISH Publication</i>
<br><a href="javascript:void(0)" onclick="toggle('hid38');">Abstract</a><p class="more" id="hid38" style="display: none;">The incomplete description of the subsurface processes by physically-based groundwater models often results in biased and correlated prediction errors, thus suggesting the need for systematic correction of errors before conducting prediction uncertainty analysis. In this work, error-mapping artificial neural networks (ANN) are used to correct the physically-based groundwater model (MODFLOW) prediction errors. The resulting prediction uncertainty of the coupled MODFLOW-ANN model is then assessed using three alternative methods. The first method establishes approximate confidence and prediction intervals using first-order least-squares regression approximation (also called first-order error analysis). The second method employs bootstrap approaches that involve resampling of the uncertain data with replacement and repeated model runs for constructing the confidence and prediction intervals. The third method relies on a Bayesian approach that uses analytical or Monte Carlo methods to derive the posterior distribution. The performance of these approaches is evaluated using a hypothetical case study developed based on a phytoremediation site at the Argonne National Laboratory, USA. The results indicate that the three approaches yield comparable confidence and prediction intervals, thus making the computationally efficient first-order error analysis approach attractive for estimating the coupled model uncertainty. The results also demonstrate that the error-mapping ANN not only captures some of the local biases in the MODFLOW prediction, but also systematically reduces the prediction variance. Copyright ? 2008 IAHS Press.</p></li>
<li>Babbar-Sebens, M. & Minsker, B. (2008). <a href="http://ascelibrary.org/action/showAbstract?page=538&volume=134&issue=6&journalCode=jwrmd5">Standard interactive genetic algorithm - Comprehensive optimization framework for groundwater monitoring design</a>, <i>Journal of Water Resources Planning and Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid39');">Abstract</a><p class="more" id="hid39" style="display: none;">Optimization for water resources management typically requires many simplifying assumptions about the definition and characteristics of the policy or design application in order to express decision makers' criteria as mathematical objectives and constraints. However, real-world applications often involve important subjective information that cannot be reflected in mathematical expressions accurately or completely. This can result in mathematically optimized solutions that are less meaningful or desirable to decision makers. To address this issue, this paper presents the standard interactive genetic algorithm (SIGA) methodology that enables human decision makers to effectively analyze subjective information that is not easily quantifiable and make decisions about the quality of a design based on their preferences. These decisions are used as continuous run-time subjective feedback, along with the mathematically defined objectives and constraints, to search for optimal designs that reflect both quantitative and qualitative objectives. Although this interactive optimization methodology is applicable for any water resources planning and management problems, this paper focuses on exploring the benefits of such an approach within the domain of groundwater monitoring design. Systematic procedures and guidelines for designing a SIGA are presented, along with proposed strategies for improving the performance of SIGA. The SIGA approach is also compared with a noninteractive genetic algorithm strategy for a real-world application, and the advantages and limitations of the interactive strategy are examined. ? 2008 ASCE.</p></li>
<li>Singh, A., Minsker, B. & Valocchi, A. (2008). <a href="http://www.sciencedirect.com/science/article/pii/S0309170808000845">An interactive multi-objective optimization framework for groundwater inverse modeling</a>, <i>Advances in Water Resources</i>
<br><a href="javascript:void(0)" onclick="toggle('hid40');">Abstract</a><p class="more" id="hid40" style="display: none;">The groundwater inverse problem of estimating heterogeneous groundwater model parameters (hydraulic conductivity in this case) given measurements of aquifer response (such as hydraulic heads) is known to be an ill-posed problem, with multiple parameter values giving similar fits to the aquifer response measurements. This problem is further exacerbated due to the lack of extensive data, typical of most real-world problems. In such cases, it is desirable to incorporate expert knowledge in the estimation process to generate more reasonable estimates. This work presents a novel interactive framework, called the 'Interactive Multi-Objective Genetic Algorithm' (IMOGA), to solve the groundwater inverse problem considering different sources of quantitative data as well as qualitative expert knowledge about the site. The IMOGA is unique in that it looks at groundwater model calibration as a multi-objective problem consisting of quantitative objectives - calibration error and regularization - and a 'qualitative' objective based on the preference of the geological expert for different spatial characteristics of the conductivity field. All these objectives are then included within a multi-objective genetic algorithm to find multiple solutions that represent the best combination of all quantitative and qualitative objectives. A hypothetical aquifer case-study (based on the test case presented by Freyberg [Freyberg DL. An exercise in ground-water model calibration and prediction. Ground Water 1988;26(3)], for which the 'true' parameter values are known, is used as a test case to demonstrate the applicability of this method. It is shown that using automated calibration techniques without using expert interaction leads to parameter values that are not consistent with site-knowledge. Adding expert interaction is shown to not only improve the plausibility of the estimated conductivity fields but also the predictive accuracy of the calibrated model. ? 2008 Elsevier Ltd. All rights reserved.</p></li>
<li>Singh, A. & Minsker, B. (2008). <a href="http://onlinelibrary.wiley.com/doi/10.1029/2005WR004436/abstract">Uncertainty-based multiobjective optimization of groundwater remediation design</a>, <i>Water Resources Research</i>
<br><a href="javascript:void(0)" onclick="toggle('hid41');">Abstract</a><p class="more" id="hid41" style="display: none;">Management of groundwater contamination often involves conflicting objectives and substantial uncertainty. The primary objective of this paper is to present a new approach, called the probabilistic multiobjective genetic algorithm (PMOGA), which incorporates uncertainty with multiobjective Pareto optimization. This type of approach is needed for water resources problems with multiple uncertain objectives; previous uncertainty-based optimization approaches consider uncertainty only in the constraints (typically constraining reliability with respect to environmental standards) or in a single objective. The optimization algorithm uses a probabilistic ranking and crowding scheme that improves decision making within the genetic algorithm by identifying a set of solutions that are Pareto optimal despite the uncertainty in the objective values. The algorithm is applied to two groundwater remediation test cases with uncertain objectives: a hypothetical case study and a field-scale pump-and-treat design problem at the Umatilla Chemical Depot situated at Hermiston, Oregon. For these case studies the primary source of uncertainty stems from uncertain aquifer hydraulic conductivity values, which affect both the remediation cost and efficiency. Results are analyzed, and the advantages of the PMOGA are discussed relative to an averaging-based multiobjective approach, a stochastic single-objective approach (similar to chance constrained optimization), and a deterministic multiobjective approach. The results demonstrate that using such an uncertainty-based multiobjective optimization scheme can give valuable information about remediation options, giving results that are not only cost effective but that also have lower uncertainty. Copyright 2008 by the American Geophysical Union.</p></li>
<li>Babbar, M. & Minsker, B. (2007). <a href="http://ascelibrary.org/doi/abs/10.1061/40856%28200%29116">A collaborative interactive genetic algorithm framework for mixed-initiative interaction with human and simulated experts: A case study in long-term groundwater monitoring design</a>, <i>Examining the Confluence of Environmental and Water Concerns - Proceedings of the World Environmental and Water Resources Congress 2006</i>
<br><a href="javascript:void(0)" onclick="toggle('hid42');">Abstract</a><p class="more" id="hid42" style="display: none;">The Interactive Genetic Algorithm (IGA) allows water resources and environmental decision makers to become active online participants during the optimization process, and thus provides a method to include qualitative expert knowledge within the search criteria. However, various interfering human factors, especially human fatigue, can limit the extent of the decision maker's participation. In this paper, we propose a mixed-initiative interaction technique for the IGA in which a simulated expert (created by using a machine learning model) can share the workload of interaction with the human expert, while constantly learning her/his preferences. This collaborative framework also allows the system to observe the learning behaviors of both the human and simulated expert, while utilizing their knowledge for search purposes. Many machine learning models can be utilized for creating the simulated experts, in our work we use fuzzy logic modeling that implements a rule based decision making criteria for modeling the human expert's preferences. These methodologies are tested on a field scale groundwater monitoring application to analyze their benefits. ? 2006 ASCE.</p></li>
<li>Dawsey, W., Minsker, B. & Amir, E. (2007). <a href="http://ascelibrary.org/doi/abs/10.1061/40927%28243%29507">Real time assessment of drinking water systems using a dynamic Bayesian network</a>, <i>Restoring Our Natural Habitat - Proceedings of the 2007 World Environmental and Water Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid43');">Abstract</a><p class="more" id="hid43" style="display: none;">This paper presents a methodology for real-time estimation of water distribution system state parameters using a dynamic Bayesian network to combine current observations with knowledge of past system behavior. The dynamic Bayesian network presented here allows the flexibility to model both discrete and continuous variables and represent causal relationships that exist within the distribution system. The posterior belief state can be inferred using a compact approximation algorithm that has been shown to contain inference errors. Simulations over stochastic variables are proposed to define the transition and observation models for the dynamic Bayesian network. ? 2007 ASCE.</p></li>
<li>Singh, A., Minsker, B., Valocchi, A. & Walker, D. (2007). <a href="http://ascelibrary.org/doi/abs/10.1061/40927%28243%29161">Interactive multi-objective inverse groundwater modeling for the WIPP site</a>, <i>Restoring Our Natural Habitat - Proceedings of the 2007 World Environmental and Water Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid44');">Abstract</a><p class="more" id="hid44" style="display: none;">This paper presents ongoing research on building an interactive and multi-objective framework to solve the groundwater inverse problem. Our research has shown that the inherent instability and non-uniqueness of this problem can be improved by incorporating expert knowledge about the hydro-geology of the site. The interactive multi-objective genetic algorithm (IMOGA) considers user preference (for different transmissivity fields) as an additional objective along with quantitative calibration measures, converging to a set of Pareto optimal solutions representing the best trade-off among all (qualitative as well as quantitative) objectives. An important part of groundwater inversion is assessing parameter uncertainty and its effect upon model predictions. To assess the uncertainty in prediction we a multi-level sampling approach is used that incorporates uncertainty in both large-scale trends and the small-scale stochastic variability. The large-scale uncertainty is modeled using a Bayesian approach where both calibration error and a prior transmissivity field (as specified by the expert through the interactive rankings of the IMOGA) are considered. A geostatistical approach is adopted for the small-scale uncertainty, which is considered to be unconditional and auto-correlated with a specified covariance structure. The prediction model is run using all simulated fields to get the distribution of predictions. This methodology is being applied to a field-scale case study based on the Waste Isolation Pilot Plant (WIPP) situated in Carlsbad, New Mexico. This work is still in progress and results will be presented at the EWRI conference. ? 2007 ASCE.</p></li>
<li>Dawsey, W. & Minsker, B. (2007). <a href="http://ascelibrary.org/doi/abs/10.1061/40941%28247%29158">Data mining to inform total coliform monitoring plan design</a>, <i>8th Annual Water Distribution Systems Analysis Symposium 2006</i>
<br><a href="javascript:void(0)" onclick="toggle('hid45');">Abstract</a><p class="more" id="hid45" style="display: none;">Monitoring of drinking water distribution systems should undertake to capture the worst case water quality scenarios in order to provide the maximum protection for public health. Total coliform monitoring is expensive and time consuming relative to other chemical or physical measures of system integrity that may be monitored in real time. This paper proposes a methodology for mining these additional water quality parameters to inform coliform monitoring. Three machine learning algorithms are selected for application to a case study distribution system: i) gradient tree boosting, ii) decision trees, and in) distance-weighted nearest neighbor algorithm. In addition, the effect of expanding training data to include unlabeled data records will be explored. The performance of these data mining techniques should provide insight into the use of surrogate water quality parameters to indicate high coliform levels. Copyright ASCE 2006.</p></li>
<li>Singh, A. & Minsker, B. (2007). <a href="http://adsabs.harvard.edu/abs/2006AGUFM.H23D1542S">Interactive multi-objective inverse groundwater modeling -Formulation and addressing user fatigue</a>, <i>Examining the Confluence of Environmental and Water Concerns - Proceedings of the World Environmental and Water Resources Congress 2006</i>
<br><a href="javascript:void(0)" onclick="toggle('hid46');">Abstract</a><p class="more" id="hid46" style="display: none;">This paper builds on work done on using interactive multi-objective genetic algorithms (IMOGA) to solve the groundwater inverse problem (Singh & Minsker, 2005) by searching for optimal hydraulic conductivity fields conditioned on field measurements of hydraulic heads and conductivities. The biggest challenge faced when using such interactive systems is that of user fatigue because the user is expected to evaluate many solutions during the search process. This paper discusses a two-step approach to reduce user fatigue. First the user is shown only a fraction of the total population in every generation. To ensure minimum redundancy during evaluation, the solutions are clustered using unsupervised clustering and the expert is shown unique samples from distinct clusters. Next the unranked solutions are ranked using a surrogate model that 'learns' from the user preferences. This is implemented using a supervised classification algorithm to cluster the solutions based on the 2-D images of hydraulic conductivity. We test 'content-based' and 'spectral' algorithms for the clustering and classification as these have been shown to be similar to how humans process images. The work on applying and testing these algorithms is on-going and this paper discusses some preliminary results. Complete results will be shown at the EWRI conference. ? 2007 ASCE.</p></li>
<li>Coopersmith, E., Minsker, B., Maidment, D., Ben, H., Bonner, J., Ojo, T. & Montagna, P. (2007). <a href="http://ascelibrary.org/doi/abs/10.1061/40927%28243%29289">An environmental information system for hypoxia in Corpus Christi Bay: A WATERS network testbed</a>, <i>Restoring Our Natural Habitat - Proceedings of the 2007 World Environmental and Water Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid47');">Abstract</a><p class="more" id="hid47" style="display: none;">This project is creating and demonstrating a prototype Environmental Information System (EIS) that couples sensor measurements with end-to-end cyberinfrastructure to improve understanding of hypoxia in Corpus Christi Bay (CC Bay), Texas. Hypoxia is a common estuarine phenomenon that occurs when dissolved oxygen concentrations fall below 2 mg/L, and has resulted in about a ten-fold reduction in benthic standing stock and diversity in CC Bay. The hypoxia in CC Bay is correlated with salinity-induced stratification of the bay, but the stratification forcing and the spatial and temporal patterns of the hypoxia remain uncertain. In this project, an interdisciplinary team of hydrologists, environmental engineers, biologists, and computer scientists are collaborating to improve understanding of hypoxia by: (1) creating an Environmental Data Access System for CC Bay data archives, leveraging CUAHSI Hydrologic Information System (HIS) Web service developments to create data services that automatically ingest observed data in both national and local remote data archives; (2) developing an Environmental Modeling System for CC Bay hypoxia, leveraging NCSA Environmental Cyberinfrastructure Demonstrator (ECID) CyberIntegrator technology to combine numerical hydrodynamic, dissolved oxygen, and oxygen demand models with data mining using hierarchical machine learning algorithms; and (3) demonstrating the effectiveness of the EIS for supporting adaptive hypoxia sampling and collaborative research using ECID's CyberCollaboratory. This paper will give initial results and future plans for the project. ? 2007 ASCE.</p></li>
<li>Gopalakrishnan, G., Negri, M., Minsker, B. & Werth, C. (2007). <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1745-6592.2006.00124.x/abstract">Monitoring subsurface contamination using tree branches</a>, <i>Ground Water Monitoring and Remediation</i>
<br><a href="javascript:void(0)" onclick="toggle('hid48');">Abstract</a><p class="more" id="hid48" style="display: none;">This paper proposes a method of assessing the distribution of chlorinated solvents in soil and ground water using tree branches. Sampling branches is a potentially more cost-effective and easier method than sampling tree cores, with less risk of damage to the tree. This approach was tested at Argonne National Laboratory, where phytoremediation is being used to remove tetrachloroethene (PCE), trichloroethene (TCE), and carbon tetrachloride (CCl4) from soil and ground water. The phytoremediation system consists of shallow-rooted willows planted in an area with contaminated soil and deep-rooted poplars planted in an area with clean soil and contaminated ground water. Branch samples were collected from 126 willows and 120 poplars. Contaminant concentrations from 31 soil borings and six monitoring wells were compared to those from branches of adjacent trees. Regression equations with correlation coefficients of at least 0.89 were obtained, which were found to be chemical specific. Kriged profiles of TCE concentration based on soil and willow branch data were developed and showed good agreement. Profiles based on ground water data could not be developed due to lack of sufficient monitoring wells for a meaningful statistical analysis. An analytical model was used to simulate TCE concentrations in tree branches from soil concentrations; the diffusion coefficient for TCE in the tree was used as the fitting parameter and the best-fit value was two orders of magnitude greater than literature values. This work indicates that tree branch sampling is a useful approach to assess contaminant distribution and potentially to determine where to locate monitoring wells or perform detailed soil analysis. Further research is necessary prior to using this method as a quantitative monitoring tool for soil and ground water. ? 2007 National Ground Water Association.</p></li>
<li>Yan, S. & Minsker, B. (2007). <a href="http://ascelibrary.org/doi/abs/10.1061/40856%28200%29137">Optimizing groundwater remediation designs under uncertainty using dynamic surrogate models</a>, <i>Examining the Confluence of Environmental and Water Concerns - Proceedings of the World Environmental and Water Resources Congress 2006</i>
<br><a href="javascript:void(0)" onclick="toggle('hid49');">Abstract</a><p class="more" id="hid49" style="display: none;">Computational cost is a critical issue for large-scale water resource optimization problems that often involve time-consuming simulation models. This issue is compounded when optimizing under uncertainty, since Monte Carlo simulations are often required to evaluate objective function values over multiple parameter realizations. In order to improve computational efficiency, we propose a dynamic surrogate modeling approach to approximate and replace the time-consuming numerical models within a noisy genetic algorithm (GA) optimization framework. The surrogates are trained to predict the distribution of the objectives online, using Monte Carlo simulation results created during the GA run. The surrogates are then adaptively updated to improve their prediction performance and correct the GA's convergence as the search progresses. Latin Hypercube sampling method is used to efficiently sample parameters for the Monte Carlo simulation and the sampling results are archived so that the estimation of the objective function distributions is progressively improved. The GA is modified to incorporate hypothesis tests to produce reliable solutions. The method is applied to a hypothetical groundwater remediation design case study, where the primary source of uncertainty stems from hydraulic conductivity values in the aquifers. Our preliminary results show that the technique can lead to reliable and cost-effective solutions with significantly less computational effort. ? 2006 ASCE.</p></li>
<li>Montgomery, J., Harmon, T., Kaiser, W., Sanderson, A., Haas, C., Hooper, R., Minsker, B., Schnoor, J., Clesceri, N., Graham, W. & Brezonik, P. (2007). <a href="https://eng.ucmerced.edu/harmongroup/harmon/Group%20publications/WATERS_es072618f.pdf/view">The waters network: An integrated environmental observatory network for water research</a>, <i>Environmental Science and Technology</i>
<br><a href="javascript:void(0)" onclick="toggle('hid50');">Abstract</a><p class="more" id="hid50" style="display: none;">An integrated real-time distributed observing system could transform the understanding of the earth's water and related biogeochemical cycles across multiple spatial and temporal scales to enable improved forecasting and management of critical water processes affected by human activities. ? 2007 American Chemical Society.</p></li>
<li>Hill, D., Minsker, B., Valocchi, A., Babovic, V. & Keijzer, M. (2007). <a href="http://www.iwaponline.com/jh/009/jh0090251.htm">Upscaling models of solute transport in porous media through genetic programming</a>, <i>Journal of Hydroinformatics</i>
<br><a href="javascript:void(0)" onclick="toggle('hid51');">Abstract</a><p class="more" id="hid51" style="display: none;">Due to the considerable computational demands of modeling solute transport in heterogeneous porous media, there is a need for upscaled models that do not require explicit resolution of the small-scale heterogeneity. This study investigates the development of upscaled solute transport models using genetic programming (GP), a domain-independent modeling tool that searches the space of mathematical equations for one or more equations that describe a set of training data. An upscaling methodology is developed that facilitates both the GP search and the implementation of the resulting models. A case study is performed that demonstrates this methodology by developing vertically averaged equations of solute transport in perfectly stratified aquifers. The solute flux models developed for the case study were analyzed for parsimony and physical meaning, resulting in an upscaled model of the enhanced spreading of the solute plume, due to aquifer heterogeneity, as a process that changes from predominantly advective to Fickian. This case study not only demonstrates the use and efficacy of GP as a tool for developing upscaled solute transport models, but it also provides insight into how to approach more realistic multi-dimensional problems with this methodology. ? IWA Publishing 2007.</p></li>
<li>Sinha, E. & Minsker, B. (2007). <a href="http://www.sciencedirect.com/science/article/pii/S0309170807000425">Multiscale island injection genetic algorithms for groundwater remediation</a>, <i>Advances in Water Resources</i>
<br><a href="javascript:void(0)" onclick="toggle('hid52');">Abstract</a><p class="more" id="hid52" style="display: none;">Genetic algorithms have been shown to be powerful tools for solving a wide variety of water resources optimization problems. Applying these approaches to complex, large-scale water resources applications can be difficult due to computational limitations, especially when a numerical model is needed to evaluate different solutions. This problem is particularly acute for solving field-scale groundwater remediation design problems, where fine spatial grids are often needed for accuracy. Finer grids usually improve the accuracy of the solutions, but they are also computationally expensive. In this paper we present multiscale island injection genetic algorithms (IIGAs), in which the optimization algorithms have different multiscale populations working on different islands (groups of processors) and periodically exchanging information. This new approach is tested using a field-scale pump-and-treat design problem at the Umatilla Army Depot in Oregon, USA. The performance of several variations of this approach is compared with the results of a simple genetic algorithm. The new approach found the same solution as much as 81% faster than the simple genetic algorithm and 9-53% faster than other previously formulated multiscale strategies. These findings indicate substantial promise for multiscale IIGA approaches to improve solution of complex water resources applications at the field scale.</p></li>
<li>Montgomery, J., Haas, C., Minsker, B. & Schnoor, J. (2007). <a href="http://www.boliven.com/publication/17489267?q=%28water%29">The WATERS Network: Transforming our scientific understanding of the nation's waters</a>, <i>Water Environment Research</i>
<br><a href="javascript:void(0)" onclick="toggle('hid53');">Abstract</a><p class="more" id="hid53" style="display: none;">[No abstract available]</p></li>
<li>Farrell, D., Minsker, B., Tcheng, D., Searsmith, D., Bohn, J. & Beckman, D. (2007). <a href="http://www.iwaponline.com/jh/009/jh0090107.htm">Data mining to improve management and reduce costs of environmental remediation</a>, <i>Journal of Hydroinformatics</i>
<br><a href="javascript:void(0)" onclick="toggle('hid54');">Abstract</a><p class="more" id="hid54" style="display: none;">In this paper, data from 105 soil and groundwater remediation projects at BP gasoline service stations located in the state of Illinois were mined for lessons to reduce cost and improve management of remediation sites. Data mining software called D2K was used to train decision tree, stepwise linear regression and instance-based weighting models that relate hydrogeologic, sociopolitical, temporal and remedial factors in the site closure reports to remediation cost. The most important factors influencing cost were found to be the amount of soil excavated and the number of groundwater monitoring wells installed, suggesting that better management of excavation and well placement could result in significant cost savings. The best model for predicting cost classes (low, medium and high cost) was the decision tree, which had a prediction accuracy of approximately 73%. The misclassification of approximately 27% of the sites by even the best model suggests that remediation costs at service stations are influenced by other site-specific factors that may be difficult to accurately predict in advance. ? IWA Publishing 2007.</p></li>
<li>Estornell, P., Haas, C., Minsker, B., Schnoor, J. & Montgomery, J. (2007). <a href="http://www.iwaponline.com/w21/00902/w21009020042.htm">Waters network - Transforming the way the US assesses water quality and manages this valuable and threatened resource</a>, <i>Water 21</i>
<br><a href="javascript:void(0)" onclick="toggle('hid55');">Abstract</a><p class="more" id="hid55" style="display: none;">The US has established the WATERS network, a system of national observatories with sensors to gather environmental and socio-economic data, and an infrastructure for sharing, storing, analyzing and drawing information from the data. The system aims to understand and quantify the impacts to water resources to enable better management and protection of this valuable asset. The system will transform scientific understanding of complex environment and social systems and improve the assessment and management of water resources. The network incorporates existing and new environmental and socio-economic data at various spatial and temporal scales. Data includes physical, chemical and biological information to characterize air, water, land and socio-economic and behavioral information. Real-time data resources, analytical tools and models, networking tools and education and outreach devices of network ensure the efficiency of network to maintain water quality.</p></li>
<li>Characklis, G., Reed, P. & Minsker, B. (2007). <a href="http://ascelibrary.org/doi/abs/10.1061/%28ASCE%290733-9496%282007%29133%3A1%281%29">The role of the systems community in the National Science Foundation's environmental observatories</a>, <i>Journal of Water Resources Planning and Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid56');">Abstract</a><p class="more" id="hid56" style="display: none;">[No abstract available]</p></li>
<li>Minsker, B., Myers, J., Marikos, M., Wentling, T., Downey, S., Liu, Y., Bajcsy, P., Kooper, R., Marini, L., Contractor, N., Green, H. & Futrelle, J. (2006). <a href="http://isda.ncsa.uiuc.edu/ecid/ECID_publications.htm">NCSA environmental cyberinfrastructure demonstration project: Creating cyberenvironments for environmental engineering and hydrological science communities</a>, <i>Proceedings of the 2006 ACM/IEEE Conference on Supercomputing, SC'06</i>
<br><a href="javascript:void(0)" onclick="toggle('hid57');">Abstract</a><p class="more" id="hid57" style="display: none;">NCSA's Environmental Cyberinfrastructure Demonstration (ECID) project is developing a cyberenvironment to support environmental science and hydrology research. The ECID Cyberenvironment is based on a set of coordinating technologies that together provide unique capabilities for integrating local and remote work and for capturing and exploiting data and interaction provenance. The CyberCollaboratory portal provides collaboration tools to discover, share, analyze, and discuss data and information. The CyberIntegrator is a meta-workflow engine combining heterogeneous (local and remote) tools into workflows to support complex scientific analyses and simulations. Both of these tools record rich provenance into the RDF-based Tupelo 2 Toolkit where it can be graphically browsed or used through CI-KNOW, social network analysis software, to provide context-specific recommendations within the CyberCollaboratory and CyberIntegrator. These capabilities, and the ability to dynamically connect new tools into group spaces, workflows, and provenance trails, will be critical to the next-generation of community-scale, persistent cyberinfrastructure efforts. ? 2006 IEEE.</p></li>
<li>Haas, C., Montgomery, J., Minsker, B., Schnoor, J. & Reible, D. (2006). <a href="http://idea.library.drexel.edu/handle/1860/1506">Integrated hydrologic science and environmental engineering observatory: The cleaner vision for the waters network</a>, <i>AIChE Annual Meeting, Conference Proceedings</i>
<br><a href="javascript:void(0)" onclick="toggle('hid58');">Abstract</a><p class="more" id="hid58" style="display: none;">[No abstract available]</p></li>
<li>Becker, D., Minsker, B., Greenwald, R., Zhang, Y., Harre, K., Yager, K., Zheng, C. & Peralta, R. (2006). <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1745-6584.2006.00242.x/abstract">Reducing long-term remedial costs by transport modeling optimization</a>, <i>Ground Water</i>
<br><a href="javascript:void(0)" onclick="toggle('hid59');">Abstract</a><p class="more" id="hid59" style="display: none;">The Department of Defense (DoD) Environmental Security Technology Certification Program and the Environmental Protection Agency sponsored a project to evaluate the benefits and utility of contaminant transport simulation-optimization algorithms against traditional (trial and error) modeling approaches. Three pump-and-treat facilities operated by the DoD were selected for inclusion in the project. Three optimization formulations were developed for each facility and solved independently by three modeling teams (two using simulation-optimization algorithms and one applying trial-and-error methods). The results clearly indicate that simulation-optimization methods are able to search a wider range of well locations and flow rates and identify better solutions than current trial-and-error approaches. The solutions found were 5% to 50% better than those obtained using trial-and-error (measured using optimal objective function values), with an average improvement of ?20%. This translated into potential savings ranging from $600,000 to $10,000,000 for the three sites. In nearly all cases, the cost savings easily outweighed the costs of the optimization. To reduce computational requirements, in some cases the simulation-optimization groups applied multiple mathematical algorithms, solved a series of modified subproblems, and/or fit "meta-models" such as neural networks or regression models to replace time-consuming simulation models in the optimization algorithm. The optimal solutions did not account for the uncertainties inherent in the modeling process. This project illustrates that transport simulation-optimization techniques are practical for real problems. However, applying the techniques in an efficient manner requires expertise and should involve iterative modification to the formulations based on interim results. Copyright ? 2006 The Author(s).</p></li>
<li>Espinoza, F. & Minsker, B. (2006). <a href="http://ascelibrary.org/doi/abs/10.1061/%28ASCE%290887-3801%282006%2920%3A6%28420%29">Effects of local search algorithms on groundwater remediation optimization using a self-adaptive hybrid genetic algorithm</a>, <i>Journal of Computing in Civil Engineering</i>
<br><a href="javascript:void(0)" onclick="toggle('hid60');">Abstract</a><p class="more" id="hid60" style="display: none;">Genetic algorithms allow solution of more complex, nonlinear civil, and environmental engineering problems than traditional gradient-based approaches, but they are more computationally intensive. One way to improve algorithm performance is through inclusion of local search, creating a hybrid genetic algorithm (HGA). The inclusion of local search helps to speed up the solution process and to make the solution technique more robust. This paper focuses on the effects of different local search algorithms on the performance of two different HGAs developed in previous phases of this research, the self-adaptive hybrid genetic algorithm (SAHGA) and the enhanced SAHGA. The algorithms are tested on eight test functions from the genetic and evolutionary computation literature and a groundwater remediation design case study. The results show that the selection of the local search algorithm to be combined with the simple genetic algorithm is critical to algorithm performance. The best local search algorithm varies for different problems, but can be selected prior to solving the problem by examining the reduction in fitness standard deviation associated with each local search algorithm, and the time distribution associated to the local search algorithm. ? 2006 ASCE.</p></li>
<li>Babbar, M. & Minsker, B. (2006). <a href="http://ascelibrary.org/action/showAbstract?page=341&volume=132&issue=5&journalCode=jwrmd5">Groundwater remediation design using multiscale genetic algorithms</a>, <i>Journal of Water Resources Planning and Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid61');">Abstract</a><p class="more" id="hid61" style="display: none;">Water resources optimization models often use spatial numerical models to approximate the physics of natural systems. The discretization of the numerical grids can affect their search for optimal solutions, in terms of both solution reliability and computational costs. Computational costs are particularly significant for population-based optimization techniques such as genetic algorithms (GAs), which are being applied to water resources optimization. To overcome these bottlenecks, this paper proposes multiscale strategies for GAs that evaluate designs on different spatial grids at different stages of the algorithm. The strategies are initially tested on a hypothetical groundwater remediation problem, and then the best approach is used to solve a field-scale groundwater application at the Umatilla Chemical Depot in Oregon. For the Umatilla case, the multiscale GA was able to save as much as 80% of the computational costs (relative to the GA that used only the fine grid) with no loss of accuracy, thus exhibiting significant promise for improving performance of GA-based optimization methodologies for water resources applications. ? 2006 ASCE.</p></li>
<li>Espinoza, F. & Minsker, B. (2006). <a href="http://onlinelibrary.wiley.com/doi/10.1029/2005WR004221/abstract">Development of the enhanced self-adaptive hybrid genetic algorithm (e-SAHGA)</a>, <i>Water Resources Research</i>
<br><a href="javascript:void(0)" onclick="toggle('hid62');">Abstract</a><p class="more" id="hid62" style="display: none;">Genetic algorithms allow solution of more complex, nonlinear groundwater remediation design problems than traditional gradient-based approaches, but they are more computationally intensive. One way to improve performance is through inclusion of local search, creating a hybrid genetic algorithm (HGA). The inclusion of local search helps to speed up the solution process and to make the solution technique more robust. This technical note focuses on the development and application of a new HGA, the enhanced self-adaptive hybrid genetic algorithm (e-SAHGA), which is an enhancement of a previously developed HGA called SAHGA. The application of the e-SAHGA algorithm to a hypothetical groundwater remediation design problem showed 90% reliability in identifying the optimal solution faster than the SGA, with average savings of 64% across 100 random initial populations. These results are considerably improved over SAHGA, which attained only 80% reliability and 14% average savings on the same initial populations. Copyright 2006 by the American Geophysical Union.</p></li>
<li>Dawsey, W., Minsker, B. & VanBlaricum, V. (2006). <a href="http://ascelibrary.org/action/showAbstract?page=234&volume=132&issue=4&journalCode=jwrmd5">Bayesian belief networks to integrate monitoring evidence of water distribution system contamination</a>, <i>Journal of Water Resources Planning and Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid63');">Abstract</a><p class="more" id="hid63" style="display: none;">A Bayesian belief network (BBN) methodology is proposed for combining evidence to better characterize contamination events and reduce false positive sensor detections in drinking water distribution systems. A BBN is developed that integrates sensor data with other validating evidence of contamination scenarios. This network is used to graphically express the causal relationships between events such as operational changes or a true contaminant release and consequent observable evidence in an example distribution system. In the BBN methodology proposed here, multiple computer simulations of contaminant transport are used to estimate the prior probabilities of a positive sensor detection. These simulations are run over multiple combinations of possible source locations and initial mass injections for a conservative solute. This approach provides insight into the effect of uncertainties in source mass and location on the detection probability of the sensors. In addition, the simulations identify the upstream nodes that are more likely to result in positive detections. The BBN incorporates the probabilities that result from these simulations, and the network is updated to reflect three demonstration scenarios - a false positive and two true positive sensor detections. ? 2006 ASCE.</p></li>
<li>Yan, S. & Minsker, B. (2006). <a href="http://onlinelibrary.wiley.com/doi/10.1029/2005WR004303/abstract">Optimal groundwater remediation design using an Adaptive Neural Network Genetic Algorithm</a>, <i>Water Resources Research</i>
<br><a href="javascript:void(0)" onclick="toggle('hid64');">Abstract</a><p class="more" id="hid64" style="display: none;">Large-scale water resources optimization often involves using time-consuming simulation models to evaluate potential water resource designs or calibrate parameter values. Approximation models have been proposed for improving computational efficiency of the optimization. In most instances, multiple simulation runs have been done prior to the optimization, which are then used to fit an approximate model that is used during the optimization. This paper demonstrates that this approach can lead to suboptimal solutions and proposes a dynamic modeling approach, called Adaptive Neural Network Genetic Algorithm (ANGA), in which artificial neural networks are adaptively and automatically trained directly within a genetic algorithm (GA) to replace the time-consuming water resource simulation models. A dynamic learning approach is proposed to periodically sample new solutions both to update the ANNs and to correct the GA's convergence. Different configurations of ANGA were tested on a hypothetical groundwater remediation design case, and then the best configuration was applied to a field-scale case. In these applications, ANGA saved 85-90% percent of the simulation model calls with no loss in accuracy of the optimal solutions. These results show that the method has substantial promise for reducing computational effort associated with large-scale water resources optimization. Copyright 2006 by the American Geophysical Union.</p></li>
<li>Singh, A., Minsker, B. & Takagi, H. (2005). <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.2448">Interactive genetic algorithms for inverse groundwater modeling: Issues with human fatigue and prediction models</a>, <i>World Water Congress 2005: Impacts of Global Climate Change - Proceedings of the 2005 World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid65');">Abstract</a><p class="more" id="hid65" style="display: none;">This paper presents an interactive multi-objective evolutionary optimization based approach to solve the inverse problem of estimating heterogeneous aquifer parameters (in this case - hydraulic conductivity) for a groundwater flow model. A hypothetical aquifer, for which the 'true' parameter values are known, is used as a test case to demonstrate the usefulness of this method. It is shown that using automated calibration techniques without using expert interaction leads to parameter values that are not consistent with site knowledge. In such cases, it is desirable to incorporate expert knowledge in the estimation process to generate more reasonable estimates. An interactive approach is proposed within a multi-objective framework that allows the user to evaluate trade-offs between the expert knowledge and other measures of numerical errors. For the hypothetical aquifer, this type of expert interaction is shown to produce more plausible estimates. A major issue with interactive approaches is 'human fatigue'. One way of dealing with human fatigue is to use machine learning to model user preferences. This work presents some initial results that show that machine learning models can be used to augment user interaction, allowing the IGA to find good solutions with much less user effort. Copyright ASCE 2005.</p></li>
<li>Babbar, M., Minsker, B. & Takagi, H. (2005). <a href="http://ascelibrary.org/doi/abs/10.1061/40792%28173%29340">Expert knowledge in long-term groundwater monitoring optimization process: The interactive genetic algorithm perspective</a>, <i>World Water Congress 2005: Impacts of Global Climate Change - Proceedings of the 2005 World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid66');">Abstract</a><p class="more" id="hid66" style="display: none;">In most practical water resources optimization applications, a number of important subjective issues exist that cannot be represented in numerical optimization procedures. Considering these issues only in a post-optimization analysis of solutions by the expert (engineers, stakeholders, regulators, etc.) does not ensure that the final set of optimal designs address all qualitative issues important to the problem. The Interactive Genetic Algorithm (IGA) promises to overcome these hurdles by involving the expert directly in the online search process to steer the genetic algorithm to a solution or set of solutions that address both quantitative and qualitative criteria. This paper investigates the effect on the overall search process when a single user interacts with the IGA system. Some of the salient control parameters that affect performance of such a framework are algorithmic control parameters (i.e. the GA settings, visualization interfaces, etc.), human control parameters (i.e. the user's cognitive perception, user's degree of risk aversion, human fatigue, etc.), and external control parameters (i.e. environmental noise and uncertainty, etc.). This work begins a rigorous assessment of the effects of different control parameters on the IGA search process by simulating the human decision making process using fuzzy logic models of human preferences as 'pseudo humans'. Comparison of such a system with a conventional optimization framework (that lacks progressive user feedback) is made for a long-term groundwater monitoring optimization problem, and related ramifications are highlighted. Copyright ASCE 2005.</p></li>
<li>Hayes, M. & Minsker, B. (2005). <a href="http://ascelibrary.org/doi/abs/10.1061/40792%28173%29339">Evaluation of advanced genetic algorithms applied to groundwater remediation design</a>, <i>World Water Congress 2005: Impacts of Global Climate Change - Proceedings of the 2005 World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid67');">Abstract</a><p class="more" id="hid67" style="display: none;">Optimal design of a groundwater pump and treat system is a difficult task, especially given the computationally intensive nature of field-scale remediation design. Genetic algorithms (GAs) have been used extensively for remediation design because of their flexibility and global search capabilities, but computational intensity is a particularly difficult issue with GAs. This paper discusses a new competent GA, the hierarchical Bayesian Optimization Algorithm (hBOA), which is designed to reduce the computational effort. GAs operate by assembling highly fit segments of chromosomes (potential solutions), called building blocks. The hBOA enhances the efficiency of this process by using a Bayesian network to create models of the building blocks. The building blocks are nodes on the network, and the algorithm uses the network to generate new solutions, retaining the best building blocks of the parents. This work compares the performance of hBOA to a simple genetic algorithm (SGA) in solving a case study to determine if any benefit can be gained through the use of this approach. This work demonstrates that hBOA more reliably identifies the optimal solution to this groundwater remediation design problem. Copyright ASCE 2005.</p></li>
<li>Dawsey, W., Minsker, B. & VanBlaricum, V. (2005). <a href="http://ascelibrary.org/doi/abs/10.1061/40792%28173%2943">Reducing online contaminant monitoring uncertainty using a bayesian belief network</a>, <i>World Water Congress 2005: Impacts of Global Climate Change - Proceedings of the 2005 World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid68');">Abstract</a><p class="more" id="hid68" style="display: none;">There is a great deal of uncertainty in real time characterization of water distribution system contamination events. Much of this uncertainty is due to the lack of targeted sensors which makes it necessary to use surrogate water quality parameters to indirectly measure the presence of a contaminant. A positive sensor detection can often be validated by pieces of evidence observed in a distribution system. This paper illustrates how Bayesian belief networks can be used to represent distribution system contamination scenarios. A framework was developed that integrated sensor data with other validating evidence of a contamination event. This framework was used to express causality between the events and observed evidence that comprise contamination scenarios. Copyright ASCE 2005.</p></li>
<li>Yan, S. & Minsker, B. (2005). <a href="http://ascelibrary.org/doi/abs/10.1061/40792%28173%29348">Optimal groundwater remediation design using trust region based metamodels within a genetic algorithm</a>, <i>World Water Congress 2005: Impacts of Global Climate Change - Proceedings of the 2005 World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid69');">Abstract</a><p class="more" id="hid69" style="display: none;">Computational cost is a critical issue for large-scale water resource optimization problems that often involve time-consuming simulation models. Less accurate approximation ("meta") models can be used to improve computational efficiency. We propose a novel trust-region-based metamodel framework, in which hierarchically trained metamodels are embedded into a genetic algorithm (GA) optimization framework to replace time-consuming numerical models. Numerical solutions produced from early generations of the GA, along with solutions dynamically sampled from later generations, are used to retrain the metamodels and correct the GA's converging route. A bootstrap sampling technique is used to cluster the collected numerical solutions into hierarchical training regions and then multiple metamodels are trained based on these clustered regions. The hierarchically trained metamodels are then used to approximate the numerical models. A trust region testing strategy selects the most appropriate metamodels for prediction. This allows the local regions (particularly those near the optimal solution) to be approximated by smoother and smaller metamodels with higher accuracy. This can speed up GA's convergence when the population moves into local regions. The technique was tested with artificial neural networks (ANNs) and support vector machines (SVMs) on a field-scale groundwater remediation case in a distributed network computation environment. Our preliminary results show that the adaptive meta-model GA (AMGA) with the trust region based training technique converges with higher accuracy with the same computation effort. Copyright ASCE 2005.</p></li>
<li>Minsker, B., Groves, P. & Beckmann, D. (2005). <a href="http://ascelibrary.org/doi/abs/10.1061/40792%28173%29377">Optimizing long term monitoring at a BP site using multi-objective optimization</a>, <i>World Water Congress 2005: Impacts of Global Climate Change - Proceedings of the 2005 World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid70');">Abstract</a><p class="more" id="hid70" style="display: none;">BP (formerly British Petroleum) incurs significant costs associated with monitoring subsurface remediation sites. The purpose of this project is to evaluate whether these costs could be reduced by identifying and eliminating both spatial and temporal redundancies in the monitoring data at a BP site without significantly increasing monitoring errors. The project also aims to demonstrate the potential for multi-objective optimization approaches to improve monitoring decision making at the many sites at BP and elsewhere with long-term monitoring records. The first step in the optimization process is to identify monitoring objectives and constraints, and express them in mathematical form. In this case, the initial objectives were to minimize the number of samples collected and to minimize relative BTEX interpolation error. The BTEX interpolation error for trial sets of sampling plans are calculated by comparing the concentrations interpolated using all sampling locations and times with those interpolated using only reduced sampling frequencies or locations. Historical data from the wells that are currently being sampled are used to develop a suite of interpolation models, which are then tested using a cross-validation approach. Adaptive Environmental Monitoring System (AEMS) software, developed at the University of Illinois and RiverGlass Inc., is then used to search through the billions of sampling plans to identify the optimal tradeoffs between the number of samples collected and the relative error. Copyright ASCE 2005.</p></li>
<li>Ren, X. & Minsker, B. (2005). <a href="http://ascelibrary.org/action/showAbstract?page=351&volume=131&issue=5&journalCode=jwrmd5">Which groundwater remediation objective is better: A realistic one or a simple one?</a>, <i>Journal of Water Resources Planning and Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid71');">Abstract</a><p class="more" id="hid71" style="display: none;">One of the first steps in developing an optimal water resources design model is creating appropriate objective functions that represent the primary goals of the design. In many cases, one major objective is minimizing cost. A more realistic cost function, with detailed cost terms, may yield more accurate results but will require more development effort. This research examines the benefits of developing a realistic cost function using two multiobjective groundwater remediation case studies. The results show that realistic cost functions find better solutions than the simplified cost functions, as well as identifying more optimal solutions on the Pareto frontier than the other functions. The realistic cost functions achieved up to 14% improvement in total cost, although the degree of loss in accuracy varies substantially for the two case studies considered in this work and for different parameter settings within each case study. Given the difficulties of predicting which case studies or parameter settings would have significant loss of performance from using simplified cost functions, investments in developing accurate site-specific cost functions appear to be worthwhile. ? ASCE.</p></li>
<li>Michael, W., Minsker, B., Tcheng, D., Valocchi, A. & Quinn, J. (2005). <a href="http://onlinelibrary.wiley.com/doi/10.1029/2003WR002802/abstract">Integrating data sources to improve hydraulic head predictions: A hierarchical machine learning approach</a>, <i>Water Resources Research</i>
<br><a href="javascript:void(0)" onclick="toggle('hid72');">Abstract</a><p class="more" id="hid72" style="display: none;">This study investigates how machine learning methods can be used to improve hydraulic head predictions by integrating different types of data, including data from numerical models, in a hierarchical approach. A suite of four machine learning methods (decision trees, instance-based weighting, inverse distance weighting, and neural networks) are tested in several hierarchical configurations with different types of data from the 317/319 area at Argonne National Laboratory-East. The best machine learning model had a mean predicted head error 50% smaller than an existing MODFLOW numerical flow model, and a standard deviation of predicted head error 67% lower than the MODFLOW model, computed across all sampled locations used for calibrating the MODFLOW model. These predictions were obtained using decision trees trained with all historical quarterly data; the hourly head measurements were not as useful for prediction, most likely because of their poor spatial coverage. The results show promise for using hierarchical machine learning approaches to improve predictions and to identify the most essential types of data to guide future sampling efforts. Decision trees were also combined with an existing MODFLOW model to test their capabilities for updating numerical models to improve predictions as new data are collected. The combined model had a mean error 50% lower than the MODFLOW model alone. These results demonstrate that hierarchical machine learning approaches can be used to improve predictive performance of existing numerical models in areas with good data coverage. Further research is needed to compare this approach with methods such as Kalman filtering. Copyright 2005 by the American Geophysical Union.</p></li>
<li>Espinoza, F., Minsker, B. & Goldberg, D. (2005). <a href="http://ascelibrary.org/doi/abs/10.1061/%28ASCE%290733-9496%282005%29131:1%2814%29">Adaptive hybrid genetic algorithm for groundwater remediation design</a>, <i>Journal of Water Resources Planning and Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid73');">Abstract</a><p class="more" id="hid73" style="display: none;">Optimal groundwater remediation design problems are often complex, nonlinear, and computationally intensive. Genetic algorithms allow solution of more complex nonlinear problems than traditional gradient-based approaches, but they are more computationally intensive. One way to improve performance is through inclusion of local search, creating a hybrid genetic algorithm (HGA). This paper presents a new self-adaptive HGA (SAHGA) and compares its performance to a nonadaptive hybrid genetic algorithm (NAHGA) and the simple genetic algorithm (SGA) on a groundwater remediation problem. Of the two hybrid algorithms, SAHGA is shown to be far more robust than NAHGA, providing fast convergence across a broad range of parameter settings. For the test problem, SAHGA needs 75% fewer function evaluations than SGA, even with an inefficient local search method. These findings demonstrate that SAHGA has substantial promise for enabling solution of larger-scale problems than was previously possible. ? ASCE.</p></li>
<li>Minsker B.S. (2004). <a href="http://www.tandfonline.com/doi/abs/10.1080/10889860490887437">Long-term groundwater monitoring optimization: Improving performance and reducing costs associated with natural attenuation and other in situ treatments</a>, <i>Bioremediation Journal</i>
<br><a href="javascript:void(0)" onclick="toggle('hid74');">Abstract</a><p class="more" id="hid74" style="display: none;">[No abstract available]</p></li>
<li>Reed, P., Minsker, B. & Valocchi, A. (2004). <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.77.3586">Why optimize long term groundwater monitoring design? A multiobjective case study of Hill Air Force Base</a>, <i>Bridging the Gap: Meeting the World's Water and Environmental Resources Challenges - Proceedings of the World Water and Environmental Resources Congress 2001</i>
<br><a href="javascript:void(0)" onclick="toggle('hid75');">Abstract</a><p class="more" id="hid75" style="display: none;">Mathematical tools from the field of optimization have significant potential for reducing long-term monitoring costs and aiding site managers in making informed decisions on sampling strategies for sites undergoing long-term monitoring. A case study is presented that demonstrates the use of a Nondominated Sorted Genetic Algorithm (NSGA) for monitoring design at Hill Air Force Base (AFB). The method combines fate-and-transport simulation (although it can also be used only with historical data), plume interpolation, and adaptive search to identify the tradeoff between monitoring costs and mass estimation error. The method efficiently provides decision makers a direct representation of the tradeoff between monitoring objectives such as cost and error. Additionally, the most and least significant monitoring wells in a preexisting monitoring network are identified. Copyright ASCE 2004.</p></li>
<li>Reed, P., Minsker, B. & Valocchi, A. (2004). <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.137.9326">A multiobjective approach to long-term groundwater monitoring design</a>, <i>Joint Conference on Water Resource Engineering and Water Resources Planning and Management 2000: Building Partnerships</i>
<br><a href="javascript:void(0)" onclick="toggle('hid76');">Abstract</a><p class="more" id="hid76" style="display: none;">The goal of this study is to develop a new methodology that enables decision makers to visualize and quantify the tradeoff between cost and uncertainty for sites undergoing long-term monitoring. Monte Carlo simulation is used to predict discrete cumulative probability distributions (cdf's) for the dissolved contaminant concentrations at every available monitoring location at a site. Indicator kriging is then used to evaluate the local uncertainty associated with sampling subsets of the available monitoring locations. A Non-dominated Sorted Genetic Algorithm (NSGA) searches for sampling plans that are non-dominated in terms of the two objectives: (1) minimizing sampling costs and (2) minimizing relative local uncertainty. The NSGA evolves the Pareto optimal frontier that represents the optimal tradeoff between sampling costs and relative local uncertainty. Each point on the Pareto front can be decomposed into a sampling design's cost and a spatial mapping of its local uncertainty. This methodology will be applied to the Williams Air Force Base in Arizona. Copyright ASCE 2004.</p></li>
<li>Espinoza, F., Minsker, B. & Goldberg, D. (2004). <a href="http://ascelibrary.org/doi/abs/10.1061/40569%282001%2995">Optimal settings for a hybrid genetic algorithm applied to a groundwater remediation problem</a>, <i>Bridging the Gap: Meeting the World's Water and Environmental Resources Challenges - Proceedings of the World Water and Environmental Resources Congress 2001</i>
<br><a href="javascript:void(0)" onclick="toggle('hid77');">Abstract</a><p class="more" id="hid77" style="display: none;">Water resources management problems can be computationally intensive and improved methods are needed to allow solution of more complex applications. In this paper, we present a numerical algorithm designed to efficiently solve optimization applications such as groundwater management problems. The algorithm is a combination of a simple genetic algorithm and a local search method and is called a hybrid genetic algorithm (HGA). As a first step in the development of an effective HGA, this paper presents a new self-adaptive HGA (SAHGA) that can be used to competently solve the problem without extensive trial-and-error experimentation. This paper presents the SAHGA approach and compares its performance with the SGA and a non-adaptive HGA (NAHGA) for several test functions. The results show considerable promise for the SAHGA, which required less than 25% of the number of function evaluations required for the SGA at a 99% reliability level. The SAHGA algorithm was also more robust than the NAHGA, performing optimally across a broad range of parameter values. The next step will be to apply the SAHGA to the groundwater remediation design problem. Copyright ASCE 2004.</p></li>
<li>Liu, Y., Minsker, B. & Saied, F. (2004). <a href="http://ascelibrary.org/doi/abs/10.1061/40517%282000%29380">Spatial multiscale techniques for groundwater management modeling</a>, <i>Joint Conference on Water Resource Engineering and Water Resources Planning and Management 2000: Building Partnerships</i>
<br><a href="javascript:void(0)" onclick="toggle('hid78');">Abstract</a><p class="more" id="hid78" style="display: none;">An innovative spatial multiscale method has been proposed to solve a groundwater management model governed by partial differential equations. The management model, which was developed in previous work, uses an optimal control algorithm called successive approximation linear quadratic regulator (SALQR) to identify optimal well locations and pumping rates for in-situ bioremediation design. Previous work has shown that the computational time of this model is proportional to the cubic of the total number of nodes in the finite element mesh, which prevents it from being applied to large-scale, complex field sites. To reduce the computational burden of this model, a multiscale method for evaluating numerical derivatives is being developed, which exploits the "local-effect" of the derivative information by using multiple spatial scales within different subdomains of the model. This paper presents the methodology. Copyright ASCE 2004.</p></li>
<li>Gopalakrishnan, G., Minsker, B. & Goldberg, D. (2004). <a href="http://www.iwaponline.com/jh/005/jh0050011.htm">Optimal sampling in a noisy genetic algorithm for risk-based remediation design</a>, <i>Bridging the Gap: Meeting the World's Water and Environmental Resources Challenges - Proceedings of the World Water and Environmental Resources Congress 2001</i>
<br><a href="javascript:void(0)" onclick="toggle('hid79');">Abstract</a><p class="more" id="hid79" style="display: none;">A management model has been developed that predicts human health risk and uses a noisy genetic algorithm to identify promising risk-based corrective action designs [Smalley et al, 2000]. Noisy genetic algorithms are ordinary genetic algorithms that operate in noisy environments. The "noise" can be defined as any factor that hinders the accurate evaluation of the fitness of a given trial design. The noisy genetic algorithm uses a type of noisy fitness function called the sampling fitness function, which utilizes sampling in order to reduce the amount of noise from fitness evaluations in noisy environments. This Monte-Carlo-type sampling provides a more realistic estimate of the fitness as the design is exposed to a wide variety of conditions. Unlike Monte Carlo simulation modeling, however, the noisy genetic algorithm is highly efficient and can identify robust designs with only a few samples per design. For complex water resources and environmental engineering design problems with complex fitness functions, however, it is important that the sampling be as efficient as possible. In this paper, methods for reducing the computational effort through improved sampling techniques are investigated. A number of different sampling approaches will be presented and their performance compared using a case study of a risk-based corrective action design. Copyright ASCE 2004.</p></li>
<li>Reed, P., Minsker, B. & Goldberg, D. (2004). <a href="http://cedb.asce.org/cgi/WWWdisplay.cgi?128370">The practitioner's role in competent search and optimization using genetic algorithms</a>, <i>Bridging the Gap: Meeting the World's Water and Environmental Resources Challenges - Proceedings of the World Water and Environmental Resources Congress 2001</i>
<br><a href="javascript:void(0)" onclick="toggle('hid80');">Abstract</a><p class="more" id="hid80" style="display: none;">In the past decade genetic algorithms (GAs) have been used in a wide array of applications within the water resources field. Although usage of GAs has become widespread, the theoretical work from the genetic and evolutionary computation (GEC) field has been largely ignored. Most practitioners have instead treated the GA as a black box, specifying the parameters that control how the algorithms navigate the spaces of each application using trial-and-error analysis. Trial-and-error analysis is a time-consuming, difficult process resulting in an arbitrary selection of parameters without any regard to the fundamental properties of the GA. The concept of "competent search and optimization" as discussed in this work addresses this difficulty by using the available theoretical work from the GEC field to set the population size, the selection pressure, account for potential disruptions from crossover and mutation, and prevent drift stall. This paper provides an overview of a three-step method for utilizing GEC theory to ensure competent search and avoid common pitfalls in GA applications. Copyright ASCE 2004.</p></li>
<li>Liu, Y. & Minsker, B. (2004). <a href="http://ascelibrary.org/doi/abs/10.1061/40569%282001%2969">A full multiscale computational approach for groundwater management modeling</a>, <i>Bridging the Gap: Meeting the World's Water and Environmental Resources Challenges - Proceedings of the World Water and Environmental Resources Congress 2001</i>
<br><a href="javascript:void(0)" onclick="toggle('hid81');">Abstract</a><p class="more" id="hid81" style="display: none;">This paper presents the computational framework of a full multiscale method for solving groundwater management modeling. The management model used in this paper, which was developed in previous work, uses an optimal control algorithm called successive approximation linear quadratic regulator (SALQR) to identify optimal well locations and pumping rates for in-situ bioremediation design. The multiscale method integrates a one-way spatial multiscale approach, a V-cycle multiscale derivative calculation and a local effect derivative calculation. Application of this method starts from a coarsest mesh and solves for the optimal solution at that level, then uses the obtained solution as the initial guess for the finer mesh. While at the finer mesh, the method switches back to the coarser mesh to solve for the derivatives and uses those derivatives to interpolate back to the finer mesh. Only the peak area of the derivatives is solved at the finer mesh, with the flat area of the derivatives obtained by the interpolation. Full results combining these methods will be given at the conference, but initial results presented in this paper indicate great potential for computational savings. The reduction of computing time is about 76% for a case with over 1600 state variables. Much more savings can be expected for larger size problems. Copyright ASCE 2004.</p></li>
<li>Sinha, E., Minsker, B. & Babbar, M. (2004). <a href="http://ascelibrary.org/doi/abs/10.1061/40737%282004%29247">Multiscale Island injection genetic algorithm for ground water remediation</a>, <i>Proceedings of the 2004 World Water and Environmetal Resources Congress: Critical Transitions in Water and Environmetal Resources Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid82');">Abstract</a><p class="more" id="hid82" style="display: none;">Genetic algorithms have been shown to be powerful tools for solving a wide variety of water resources optimization problems. Applying these approaches to complex, large-scale applications, which is usually where these methods are most needed, can be difficult due to computational limitations. Large grid sizes are often needed for solving field-scale groundwater remediation design problems. Fine grids usually improve the accuracy of the solutions, but they are also computationally expensive. Multiscale parallel genetic algorithms have been shown to improve the performance of engineering design problems that use spatial grids. In this paper we present multiscale island injection genetic algorithms (IIGAs), in which the optimization algorithm has different multiscale populations working on different islands (group of processors). Each island has a fraction of its population on the coarse grid and a fraction on the fine grid. Different islands exchange the best individuals, at the same scale, after a fixed number of generations and thus drive the GA towards better and more accurate solutions faster. The performance of this approach is compared to a single population multiscale approach developed previously, using a field-scale pump-and-treat design problem at the Umatilla Army Depot.</p></li>
<li>Singh, A. & Minsker, B. (2004). <a href="http://cedb.asce.org/cgi/WWWdisplay.cgi?0410399">Uncertainty based multi-objective optimization of groundwater remediation at the Umatilla Chemical Depot</a>, <i>Proceedings of the 2004 World Water and Environmetal Resources Congress: Critical Transitions in Water and Environmetal Resources Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid83');">Abstract</a><p class="more" id="hid83" style="display: none;">Management of groundwater contamination often involves conflicting objectives and substantial uncertainty. A critical source of this uncertainty in groundwater problems often stems from uncertainty in the hydraulic conductivity values for the aquifer. For a remediation solution to be reliable in practice it is important that it is robust over such potential errors. This paper presents the application of a robust multi-objective optimization method on a field-scale pump-and-treat design problem at the Umatilla Chemical Depot site at Hermiston, Oregon. A simple methodology is used to establish plausible realizations of hydraulic conductivity that are then efficiently sampled within the optimization framework using Latin Hypercube sampling. A noisy multi-objective genetic algorithm, developed and tested earlier on a hypothetical aquifer, is then applied to this field-scale case to come up with a set of robust and Pareto-dominant design solutions for the clean up of contaminants (RDX and TNT) in the groundwater. Interactions between the various trade-offs and the inherent uncertainty at the site are analyzed. Finally it is demonstrated that by using such robust multi-objective optimization schemes, it is possible to increase robustness of the optimal solutions without significant increases in costs.</p></li>
<li>Gopalakrishnan, G., Minsker, B. & Padera, B. (2004). <a href="http://ascelibrary.org/doi/abs/10.1061/40569%282001%29101">Risk based corrective action design using genetic algorithms</a>, <i>Bridging the Gap: Meeting the World's Water and Environmental Resources Challenges - Proceedings of the World Water and Environmental Resources Congress 2001</i>
<br><a href="javascript:void(0)" onclick="toggle('hid84');">Abstract</a><p class="more" id="hid84" style="display: none;">Considerable resources have been expended in attempting to restore sites with contaminated groundwater. In the past, the cleanup goals were often established without regard to risk, mandating remediation of groundwater to background or non-detection or maximum contaminant limits. These are often difficult or impossible to achieve and have made site restoration prohibitively expensive. In response to these concerns, risk-based corrective action (RBCA) is becoming a method of choice for remediating contaminated groundwater sites. Under RBCA, the risks to human health and the environment due to contamination are evaluated and measures taken only to minimize the risk to acceptable levels. A major difficulty in RBCA is negotiating an appropriate risk-based limit and a reasonable corrective action approach, particularly given all of the sources of uncertainty in predicting risk. To aid in this process, a new framework for negotiation is being developed that combines an optimization model with simulation models in order to develop risk-based remedial designs that are both cost effective and reliable. The model combines contaminant fate and transport simulation models and health risk assessment procedures with genetic algorithms to simultaneously predict risk and propose cost effective strategies for reducing the risk. To use the model, stakeholders first negotiate the objectives of the remediation, which may include minimizing risk, minimizing cost, and minimizing cleanup time. Then any constraints such as hydraulic head limits or social or economic constraints are considered. In this paper, the steps are demonstrated using a case study. Copyright ASCE 2004.</p></li>
<li>Yan, S. & Minsker, B. (2004). <a href="http://citeseer.uark.edu:8080/citeseerx/viewdoc/summary?doi=10.1.1.114.8883">A dynamic meta-model approach to genetic algorithm solution of a risk-based groundwater remediation design model</a>, <i>Proceedings of the 2004 World Water and Environmetal Resources Congress: Critical Transitions in Water and Environmetal Resources Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid85');">Abstract</a><p class="more" id="hid85" style="display: none;">Approximation ("meta") models have been used in coupled water resources optimization and simulation models to improve computational efficiency. In most instances, multiple simulation runs have been done before the optimization, which are then used to fit an approximate model that is used for the optimization. In this study, we propose a dynamic meta-modeling approach, in which artificial neural networks (ANN) is embedded into a genetic algorithm (GA) optimization framework to replace time-consuming flow and contaminant transport models. Data produced from early generations of the GA are sampled to train the ANN. We propose a dynamic learning approach that periodically re-samples new solutions both to update the ANN and correct the GA's converging route. This allows the meta model to adapt to the area in which the GA is searching and provide more accuracy. The results show that a proper sampling strategy can benefit both GA's searching and ANN's retraining. In our test case, more than 90 percent of the numerical model calls were saved with no loss in accuracy of the optimal solution.</p></li>
<li>Babbar, M., Minsker, B. & Takagi, H. (2004). <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.322.131&rep=rep1&type=pdf">Interactive genetic algorithm framework for long term groundwater monitoring design</a>, <i>Proceedings of the 2004 World Water and Environmetal Resources Congress: Critical Transitions in Water and Environmetal Resources Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid86');">Abstract</a><p class="more" id="hid86" style="display: none;">In standard optimization approaches for water resources management problems, the designer is responsible for correctly formulating mathematical equations to describe the system objectives and constraints. The search for optimal or near-optimal solutions is made under the assumption that these formulated objectives and constraints completely describe the system. However, in real systems that is often not true. Many qualitative criteria can be integral parts of the design analysis that numerically based algorithms cannot capture. For such problems, designer interaction with the search algorithm can help the search be more creative and inclusive. Genetic algorithms are ideally suited for incorporating such interaction in their usual search process, and can successfully evolve solutions that are optimal with respect to both qualitative and quantitative objectives. Under an interactive approach, the genetic algorithm performs the usual operations of selection, crossover, and mutation, but the user evaluates the suitability ('fitness') of candidate solutions, enabling objectives that cannot be quantified to be included in the search process. In multi-objective problems, where quantitative objectives can be as important as qualitative fitness of designs, analysis of designs is done based on tradeoff fronts made from both quantitative and qualitative information. In this paper, we demonstrate the use of interactive genetic algorithms for long term groundwater monitoring problems, which have multiple numerical and subjective objectives. We also analyze the effects on the optimal monitoring designs of using an interactive optimization approach instead of more traditional numerical optimization approaches.</p></li>
<li>Reed, P., Ellsworth, T. & Minsker, B. (2004). <a href="http://www.ncbi.nlm.nih.gov/pubmed/15035584">Spatial Interpolation Methods for Nonstationary Plume Data</a>, <i>Ground Water</i>
<br><a href="javascript:void(0)" onclick="toggle('hid87');">Abstract</a><p class="more" id="hid87" style="display: none;">Plume interpolation consists of estimating contaminant concentrations at unsampled locations using the available contaminant data surrounding those locations. The goal of ground water plume interpolation is to maximize the accuracy in estimating the spatial distribution of the contaminant plume given the data limitations associated with sparse monitoring networks with irregular geometries. Beyond data limitations, contaminant plume interpolation is a difficult task because contaminant concentration fields are highly heterogeneous, anisotropic, and nonstationary phenomena. This study provides a comprehensive performance analysis of six interpolation methods for scatter-point concentration data, ranging in complexity from intrinsic kriging based on intrinsic random function theory to a traditional implementation of inverse-distance weighting. High resolution simulation data of perchloroethylene (PCE) contamination in a highly heterogeneous alluvial aquifer were used to generate three test cases, which vary in the size and complexity of their contaminant plumes as well as the number of data available to support interpolation. Overall, the variability of PCE samples and preferential sampling controlled how well each of the interpolation schemes performed. Quantile kriging was the most robust of the interpolation methods, showing the least bias from both of these factors. This study provides guidance to practitioners balancing opposing theoretical perspectives, ease-of- implementation, and effectiveness when choosing a plume interpolation method.</p></li>
<li>Reed, P. & Minsker, B. (2004). <a href="http://ascelibrary.org/doi/abs/10.1061/%28ASCE%290733-9496%282004%29130:2%28140%29">Striking the balance: Long-term groundwater monitoring design for conflicting objectives</a>, <i>Journal of Water Resources Planning and Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid88');">Abstract</a><p class="more" id="hid88" style="display: none;">This study demonstrates the use of high-order Pareto optimization (i.e., optimizing a system for more than two objectives) on a long-term monitoring (LTM) application. The LTM application combines quantile kriging and the nondominated sorted genetic algorithm-II (NSGA-II) to successfully balance four objectives: (1) minimizing sampling costs, (2) maximizing the accuracy of interpolated plume maps, (3) maximizing the relative accuracy of contaminant mass estimates, and (4) minimizing estimation uncertainty. Optimizing the LTM application with respect to these objectives reduced the decision space of the problem from a total of 500 million designs to a set of 1,156 designs identified on the Pareto surface. Visualization of a total of eight designs aided in understanding and balancing the objectives of the application en route to a single compromise solution. This study shows that high-order Pareto optimization holds significant potential as a tool that can be used in the balanced design of water resources systems. ? ASCE / MARCH/APRIL 2004.</p></li>
<li>Liu, Y. & Minsker, B. (2004). <a href="http://ascelibrary.org/action/showAbstract?page=26&volume=130&issue=1&journalCode=jwrmd5">Full multiscale approach for optimal control of in situ bioremediation</a>, <i>Journal of Water Resources Planning and Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid89');">Abstract</a><p class="more" id="hid89" style="display: none;">Solving field-scale optimal groundwater remediation design problems is a challenge, especially when computationally intensive reactive transport models are needed. In this paper, a full multiscale approach to partial differential equation (PDE) constrained optimization is developed and is used to solve a successive approximation linear quadratic regulator model for optimal control of in situ bioremediation. The method starts the search for optimal designs from the coarsest mesh and solves for the optimal solution at that level, then uses the optimal solution obtained as the initial guess for the finer mesh. While at the finer mesh, the method switches back to the coarser mesh to solve for the derivatives and uses those derivatives to interpolate back to the finer mesh. This procedure continues until convergence is achieved at the finest level. This approach exploits important interactions between PDE discretization and optimization and achieves significant computational saving by using approximations early in the search when a broad search of the decision space is being performed. As the solution becomes more refined, more accurate estimates are needed to fine-tune the solution, and finer spatial discretizations are used. Application of the method to a bioremediation case study with about 6,500 state variables converges in about 8.8 days, compared to nearly 1 year using the previous model. This substantial improvement will enable much more realistic bioremediation design problems to be solved than was previously possible, particularly once the model is implemented in parallel.</p></li>
<li>Ren, X. & Minsker, B. (2003). <a href="http://cedb.asce.org/cgi/WWWdisplay.cgi?0527330">Which Groundwater Remediation Objective is Better, a Realistic One or a Simple One?</a>, <i>World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid90');">Abstract</a><p class="more" id="hid90" style="display: none;">One of the first steps in setting up an optimal groundwater remediation design problem is developing an appropriate objective function, which represents the primary goals of the design. Selecting appropriate objective functions can be challenging. A more realistic objective function, which is a cost function applied to a realistic site without simplification, may yield more accurate results but at the same time it will require more time and effort to develop the appropriate function for a particular application. On the other hand, a simple function will save setup time but may sacrifice the accuracy of the results. This research seeks to identify what situations encountered in remediation design would make the development of a realistic objective function necessary. It also examines tradeoffs among three objectives: total cost, risk, and total cleanup time. A pump-and-treat system is designed for a case study to explore these questions. The model used here is NSGA II (Non-dominated Sorting Genetic Algorithm-II) combined with two numerical models (Modflow and RT3D) and an exposure and risk assessment model. Four different cost functions are applied, ranging from simple to complex. The results show that the realistic cost function generally found better solutions than the simplified ones, especially for shorter-term cleanups. These findings are now being tested for a field-scale application at Umatilla Army Depot in Oregon.</p></li>
<li>Singh, A., Minsker, B. & Goldberg, D. (2003). <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.15.8593">Combining Reliability and Pareto Optimality - An Approach Using Stochastic Multi-Objective Genetic Algorithms</a>, <i>World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid91');">Abstract</a><p class="more" id="hid91" style="display: none;">Genetic Algorithms have been successfully applied to numerous water resources problems, including problems with multiple objectives or uncertainty (noise). GAs tackle multi-objective optimization by following three basic principles - advancing the non-dominated frontier; maintaining diversity in the population (through various techniques like sharing, niching, and crowding); and using an elitist. However finding Pareto-optimal solutions becomes complicated when we add uncertainty to the problem. It was found that the solutions obtained using existing multi-objective solvers, although Pareto optimal were not the most robust or reliable solutions. In single-objective problems noise has typically been dealt with using Monte-Carlo-type sampling and some form of aggregate statistics (e.g., the average of the sample fitness). With multiple objectives the noise can interfere in determining non-domination of individuals, diversity preservation, and elitism (the three basic steps in multi-objective optimization). This paper proposes and tests several approaches to tackling some of these problems. These approaches strike a balance between finding the most optimal and the most reliable solution to the problem, thus giving decision makers and designers a practical and robust optimization tool.</p></li>
<li>Singh, A. & Minsker, B. (2003). <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.8.3982&rep=rep1&type=pdf">Modeling and Characterization of Uncertainty for Optimization of Groundwater Remediation at the Umatilla Chemical Depot</a>, <i>World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid92');">Abstract</a><p class="more" id="hid92" style="display: none;">Management of groundwater contamination is a very cost-intensive proposition filled with conflicting objectives and substantial uncertainty. A critical source of this uncertainty in groundwater problems comes from the data for the conductivity values for the aquifer on which the flow and transport of the contaminant is dependent. For a remediation solution to be reliable in practice it is important that it is robust over the error in modeling data. This paper presents our efforts to model the uncertainty for the Umatilla Chemical Depot site at Oregon, a difficult task given that the scarcity of available data precludes use of stochastic hydraulic conductivity generation techniques. The installation's modeling team has divided the site into conductivity zones. We use the results from various pumping tests to establish plausible ranges for the hydraulic conductivity value in each zone. Realizations for each zone are then generated randomly from these ranges. The hydraulic head conditions resulting from each realization are then compared with measured head conditions. To incorporate spatial as well as quantitative differences in the comparison, the first moments of the hydraulic head scenarios are also compared. Unrealistic realizations are eliminated and the remaining realizations are ranked based on the moment values. The ranked realizations will be used for efficient sampling using Latin Hypercube Sampling within the framework of an advanced stochastic multi-objective genetic algorithm to obtain robust, reliable and optimal solutions.</p></li>
<li>Reed, P., Minsker, B. & Goldberg, D. (2003). <a href="http://onlinelibrary.wiley.com/doi/10.1029/2002WR001483/abstract">Simplifying multiobjective optimization using genetic algorithms</a>, <i>World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid93');">Abstract</a><p class="more" id="hid93" style="display: none;">Many water resources problems require careful balancing of fiscal, technical, and social objectives. Informed negotiation and balancing of objectives can be greatly aided through the use of evolutionary multiobjective optimization (EMO) algorithms, which can evolve entire tradeoff (or Pareto) surfaces within a single run. The primary difficulty in using these methods lies in the large number of parameters that must be specified to ensure that these algorithms effectively quantify design tradeoffs. This paper addresses this difficulty by introducing a multi-population design methodology that automates parameter specification for the Nondominated Sorted Genetic Algorithm-II (NSGA-II). The NSGA-II design methodology is successfully demonstrated on a multiobjective long-term groundwater monitoring application. The design methodology fully exploits the efficiency of the NSGA-II to enable the solution of a new class of high order multiobjective applications in which users can balance more than two performance objectives. Using this methodology, multiobjective optimization problems can now be solved automatically with only a few simple user inputs.</p></li>
<li>Espinoza, F., Minsker, B. & Goldberg, D. (2003). <a href="http://link.springer.com/chapter/10.1007%2F3-540-45105-6_104">Performance evaluation and population reduction for a Self Adaptive Hybrid Genetic Algorithm (SAHGA)</a>, <i>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</i>
<br><a href="javascript:void(0)" onclick="toggle('hid94');">Abstract</a><p class="more" id="hid94" style="display: none;">This paper examines the effects of local search on hybrid genetic algorithm performance and population sizing. It compares the performance of a self-adaptive hybrid genetic algorithm (SAHGA) to a non-adaptive hybrid genetic algorithm (NAHGA) and the simple genetic algorithm (SGA) on eight different test functions, including unimodal, multimodal and constrained optimization problems. The results show that the hybrid genetic algorithm substantially reduces required population sizes because of the reduction in population variance. The adaptive nature of the SAHGA algorithm together with the reduction in population size allow for faster solution of the test problems without sacrificing solution quality. ? Springer-Verlag Berlin Heidelberg 2003.</p></li>
<li>Yan, S. & Minsker, B. (2003). <a href="http://citeseer.uark.edu:8080/citeseerx/showciting;jsessionid=650EA9ACAB09A98C83D3C4B8EB104BC5?cid=266179">A Dynamic Meta-Model Approach to Genetic Algorithm Solution of a Risk-Based Groundwater Remediation Design Model</a>, <i>World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid95');">Abstract</a><p class="more" id="hid95" style="display: none;">Approximation ("meta") models have been used in coupled optimization and simulation models to improve computational efficiency. In most instances, multiple simulation runs have been done before the optimization, which are used to fit an approximate model that is then used for the optimization. In this study, we propose a dynamic meta-modeling approach, in which artificial neural networks (ANN) and support vector machines (SVM) are embedded into a genetic algorithm (GA) optimization framework to replace time-consuming flow and contaminant transport models. Data produced from early generations of the GA are sampled to train the ANN and SVM and the numerical models are periodically called to dynamically update the ANN and SVM. This allows the meta model to adapt to the area in which the GA is searching and provide more accuracy. Preliminary results show that a well trained ANN or SVM can achieve satisfactory accuracy. Different approaches to dynamic training will be presented at the conference.</p></li>
<li>Espinoza, F., Minsker, B. & Goldberg, D. (2003). <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.6.6143&rep=rep1&type=pdf">Local search issues for the aplication of a self-adaptive hybrid genetic algorithm in groundwater remediation design</a>, <i>World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid96');">Abstract</a><p class="more" id="hid96" style="display: none;">Water resources management problems can be computationally intensive and improved methods are needed to allow solution of more complex applications. In this paper, we study a numerical algorithm designed to efficiently solve water resources management applications such as groundwater management problems. The algorithm is a combination of a simple genetic algorithm and a local search method and is called a self-adaptive hybrid genetic algorithm (SAHGA). The paper presents new ways to improve performance of this algorithm together with an analysis of different alternative local search algorithms. The paper also includes an analysis of the reduction in population size that is possible when using SAHGA relative to a simple genetic algorithm (SGA). The results show that the improved algorithm is more reliable and effective in solving the proposed problem, with average savings of 68% with respect to the SGA.</p></li>
<li>Yeh, L., Becker, D., Harre, K., Yager, K., Greenwald, R., Minsker, B., Peralta, R., Zhang, Y. & Zheng, C. (2003). <a href="http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0CDUQFjAB&url=http%3A%2F%2Fwww.serdp.org%2Fcontent%2Fdownload%2F3940%2F61292%2Ffile%2FCU-0010-FR-03.pdf&ei=ijM5UpvYNOfA2QWkqoH4Cg&usg=AFQjCNGz3t6ADBNL0tEfWiKc0UAKqy0kbA&bvm=bv.52288139,d.b2I">Application of flow and transport optimization codes to groundwater pump and treat system optimization</a>, <i>World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid97');">Abstract</a><p class="more" id="hid97" style="display: none;">The benefits and utility of transport optimization algorithms against trial and error approaches for pump and treat system design were evaluated. The transport optimization was applied to problems associated with reduction of contaminant concentrations or contaminant mass. The costs for pump and treat systems were reduced by millions of dollars by the application of transport optimization. The results show that mathematical optimization is capable of identifying substantially improved solutions to actual field-scale problems.</p></li>
<li>Babbar, M. & Minsker, B. (2003). <a href="http://ascelibrary.org/doi/abs/10.1061/40685%282003%29122">Multiscale Strategies for Solving Water Resources Management Problems with Genetic Algorithms</a>, <i>World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid98');">Abstract</a><p class="more" id="hid98" style="display: none;">Genetic Algorithms (GAs) face major computational bottlenecks when numerical models are used for estimating the fitness of the objective function. Especially in large-scale water resources design problems, where the scale of the spatial grids is important in determining the numerical accuracy of the design, a tradeoff exists in the precision of the fitness function and the computational expenses endured. This paper discusses multiscale strategies that can be utilized for improving the performance of GAs when working with spatial grid dependant fitness functions. The strategy uses fine grid and coarse grid fitness functions strategically to maintain the accuracy and computational speed of the problem and drive the GA towards better and more accurate solutions faster. The algorithm's efficacy is tested using a groundwater remediation design case study.</p></li>
<li>Minsker, B., Davis, C., Dougherty, D. & Williams, G. (2003). <a href="http://ascelibrary.org/doi/abs/10.1061/40685%282003%2911">Future Needs for Long-Term Monitoring Design</a>, <i>World Water and Environmental Resources Congress</i>
<br><a href="javascript:void(0)" onclick="toggle('hid99');">Abstract</a><p class="more" id="hid99" style="display: none;">An EWRI Task Committee consisting of 30 members from industry, government, and academia is studying the state of the art in long-term groundwater monitoring (LTM) programs for subsurface waste sites. LTM is an on-going activity aimed at assessing remediation performance, containment integrity, and/or continued non-contamination of the subsurface and groundwater. LTM has different goals and needs than site characterization, so data collection, analysis, and modeling approaches must evolve to meet these new needs. This paper summarizes the committee's recommendations for future research and technology transfer to improve LTM design.</p></li>
<li>Reed, P., Minsker, B. & Goldberg, D. (2003). <a href="http://130.203.133.150/showciting;jsessionid=FFDEB04BAC02BA762CA19A3B5EB8BEE8?cid=74240">Simplifying multiobjective optimization: An automated design methodology for the nondominated sorted genetic algorithm-II</a>, <i>Water Resources Research</i>
<br><a href="javascript:void(0)" onclick="toggle('hid100');">Abstract</a><p class="more" id="hid100" style="display: none;">Many water resources problems require careful balancing of fiscal, technical, and social objectives. Informed negotiation and balancing of objectives can be greatly aided through the use of evolutionary multiobjective optimization (EMO) algorithms, which can evolve entire tradeoff (or Pareto) surfaces within a single run. The primary difficulty in using these methods lies in the large number of parameters that must be specified to ensure that these algorithms effectively quantify design tradeoffs. This technical note addresses this difficulty by introducing a multipopulation design methodology that automates parameter specification for the nondominated sorted genetic algorithm-II (NSGA-II). The NSGA-II design methodology is successfully demonstrated on a multiobjective long-term groundwater monitoring application. Using this methodology, multiobjective optimization problems can now be solved automatically with only a few simple user inputs.</p></li>
<li>Michael, W., Tcheng, D., Minsker, B., Valocchi, A., Quinn, J. & Williams, G. (2002). <a href="">Integrating data sources to optimize long-term monitoring, operation, and stewardship</a>, <i>Proceedings of the Third International Conference on Remediation of Chlorinated and Recalcitrant Compounds</i>
<br><a href="javascript:void(0)" onclick="toggle('hid101');">Abstract</a><p class="more" id="hid101" style="display: none;">Due to technical limitations and the high cost of hazardous waste site clean up, there has been a shift toward risk-based long-term management of sites, where contamination is left in place. This study demonstrates how integrating all available site data can improve long-term monitoring, operation, and stewardship (LTMOS) decision making and provide cost savings. A learning machine is used to integrate historic and current data from the 317/319 Area phytoremediation site at Argonne National Lab-East (ANL-E). The learning machine uses these data and daily weather data to build a model to forecast groundwater head levels. Development of the learning machine framework provides a method for integrating the diverse data sources available at the site and using that information to determine the importance of each data source in achieving monitoring objectives. Future work will determine how long the historical record will retain its accurate predictive capability and whether the value of the surrogate data (continuous samples and rainfall) increases over time. In this preliminary study, the entire historical quarterly dataset was shown to be the most important data source, which could be used to predict future water levels with far more accuracy than the most recent quarterly dataset alone.</p></li>
<li>Liu, Y. & Minsker, B. (2002). <a href="http://citeseer.uark.edu:8080/citeseerx/showciting;jsessionid=CA6CC7F668E9395F4DE712F30A222965?cid=1534359">Efficient multiscale methods for optimal in situ bioremediation design</a>, <i>Journal of Water Resources Planning and Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid102');">Abstract</a><p class="more" id="hid102" style="display: none;">This paper presents a multiscale derivative method for solving a successive approximation linear quadratic regulator model for optimal in situ bioremediation design. An efficient one-sided forward divided difference numerical derivatives calculation was implemented as the first stage of the method, which only required assembling the right-hand-side vector of the linear systems of equations of the simulation model and performing backward substitution. The derivative calculation was reduced from O(N3) to nearly O(N2), where N is the number of non-Dirichlet state variables. A V-cycle multiscale derivatives approximation was implemented as the second stage, which used coarser mesh derivatives to interpolate finer mesh derivatives. Implementing the numerical derivatives method in a case study with over 1,600 state variables caused a reduction of more than two-thirds in computing time over the previous analytical derivatives method without loss of accuracy. Using the V-cycle multiscale derivatives approximation further reduced computing time by 29%, resulting in an overall 77% reduction compared to the previous analytical derivatives method. The reduction will be even greater for applications with more state variables, enabling the solution of much larger-scale problems than was previously possible.</p></li>
<li>Liu, Y., Minsker, B. & Saied, F. (2001). <a href="http://ascelibrary.org/doi/abs/10.1061/%28ASCE%290733-9496%282001%29127:2%28130%29">One-way spatial multiscale method for optimal bioremediation design</a>, <i>Journal of Water Resources Planning and Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid103');">Abstract</a><p class="more" id="hid103" style="display: none;">Ground-water management models can be computationally intensive and field-scale application of these techniques to complex processes such as in situ bioremediation is not currently possible. A one-way spatial multiscale methodology is investigated for reducing computational effort associated with solving a ground-water management model. The multiscale approach reduces the computational burden by solving the model on a coarse mesh and then using the coarse mesh solution as a starting point for successively finer meshes, proceeding "one-way" from coarse to fine meshes. The performance of the method is presented for two different cases with different dispersivities using a three-level multiscale approach. Over 50% reduction in computing time was achieved in both cases. For the case with low dispersivity, convergence difficulties were encountered on the coarsest mesh that were overcome by using a higher dispersivity on the coarsest mesh and then switching back to the desired low dispersivity on the finer meshes. The choice of penalty weights for constraint violations also proved to be critical to the performance of this approach. Guidelines for selecting appropriate penalty weights are given.</p></li>
<li>Reed, P., Minsker, B. & Goldberg, D. (2000). <a href="http://onlinelibrary.wiley.com/doi/10.1029/2000WR900231/abstract">Designing a competent simple genetic algorithm for search and optimization</a>, <i>Water Resources Research</i>
<br><a href="javascript:void(0)" onclick="toggle('hid104');">Abstract</a><p class="more" id="hid104" style="display: none;">Simple genetic algorithms have been used to solve many water resources problems, but specifying the parameters that control how adaptive search is performed can be a difficult and time-consuming trial-and-error process. However, theoretical relationships for population sizing and timescale analysis have been developed that can provide pragmatic tools for vastly limiting the number of parameter combinations that must be considered. The purpose of this technical note is to summarize these relationships for the water resources community and to illustrate their practical utility in a long-term groundwater monitoring design application. These relationships, which model the effects of the primary operators of a simple genetic algorithm (selection, recombination, and mutation), provide a highly efficient method for ensuring convergence to near-optimal or optimal solutions. Application of the method to a monitoring design test case identified robust parameter values using only three trial runs.</p></li>
<li>Reed, P., Minsker, B. & Valocchi, A. (2000). <a href="http://onlinelibrary.wiley.com/doi/10.1029/2000WR900232/abstract">Cost-effective long-term groundwater monitoring design using a genetic algorithm and global mass interpolation</a>, <i>Water Resources Research</i>
<br><a href="javascript:void(0)" onclick="toggle('hid105');">Abstract</a><p class="more" id="hid105" style="display: none;">A new methodology for sampling plan design has been developed to reduce the costs associated with long-term monitoring of sites with groundwater contamination. The method combines a fate-and-transport model, plume interpolation, and a genetic algorithm to identify cost-effective sampling plans that accurately quantify the total mass of dissolved contaminant. The plume interpolation methods considered were inverse-distance weighting, ordinary kriging, and a hybrid method that combines the two approaches. Application of the methodology to Hill Air Force Base indicated that sampling costs could be reduced by as much as 60% without significant loss in accuracy of the global mass estimates. Inverse-distance weighting was shown to be most effective as a screening tool for evaluating whether more comprehensive geostatistical modeling is warranted. The hybrid method was effective for implementing such a tiered approach, reducing computational time by more than 60% relative to kriging alone.A new methodology for sampling plan design has been developed to reduce the costs associated with long-term monitoring of sites with groundwater contamination. The method combines a fate-and-transport model, plume interpolation, and a genetic algorithm to identify cost-effective sampling plans that accurately quantify the total mass of dissolved contaminant. The plume interpolation methods considered were inverse-distance weighting, ordinary kriging, and a hybrid method that combines the two approaches. Application of the methodology to Hill Air Force Base indicated that sampling costs could be reduced by as much as 60% without significant loss in accuracy of the global mass estimates. Inverse-distance weighting was shown to be most effective as a screening tool for evaluating whether more comprehensive geostatistical modeling is warranted. The hybrid method was effective for implementing such a tiered approach, reducing computational time by more than 60% relative to kriging alone.</p></li>
<li>Smalley, J., Minsker, B. & Goldberg, D. (2000). <a href="http://onlinelibrary.wiley.com/doi/10.1029/2000WR900191/abstract">Risk-based in situ bioremediation design using a noisy genetic algorithm</a>, <i>Water Resources Research</i>
<br><a href="javascript:void(0)" onclick="toggle('hid106');">Abstract</a><p class="more" id="hid106" style="display: none;">Risk-based corrective action (RBCA) is rapidly becoming the method of choice for remediating contaminated groundwater. In this paper, a management model is presented that simultaneously predicts risk and proposes cost-effective options for reducing risk to acceptable levels under conditions of uncertainty. The model combines a noisy genetic algorithm with a numerical fate and transport model and an exposure and risk assessment model. The noisy genetic algorithm uses sampling from parameter distributions to assess the performance of candidate designs. Results from an application to a site from the literature show that the noisy genetic algorithm is capable of identifying highly reliable designs from a small number of samples, a significant advantage for computationally intensive groundwater management models. For the site considered, time-dependent costs associated with monitoring and the remedial system were significant, illustrating the potential importance of allowing variable cleanup lengths and a realistic cost function.Risk-based corrective action (RBCA) is rapidly becoming the method of choice for remediating contaminated groundwater. In this paper, a management model is presented that simultaneously predicts risk and proposes cost-effective options for reducing risk to acceptable levels under conditions of uncertainty. The model combines a noisy genetic algorithm with a numerical fate and transport model and an exposure and risk assessment model. The noisy genetic algorithm uses sampling from parameter distributions to assess the performance of candidate designs. Results from an application to a site from the literature show that the noisy genetic algorithm is capable of identifying highly reliable designs from a small number of samples, a significant advantage for computationally intensive groundwater management models. For the site considered, time-dependent costs associated with monitoring and the remedial system were significant, illustrating the potential importance of allowing variable cleanup lengths and a realistic cost function.</p></li>
<li>Minsker, B., Smalley, J. & Padera, B. (2000). <a href="">The role of risk and uncertainty in groundwater remediation design</a>, <i>ACS Division of Environmental Chemistry, Preprints</i>
<br><a href="javascript:void(0)" onclick="toggle('hid107');">Abstract</a><p class="more" id="hid107" style="display: none;">[No abstract available]</p></li>
<li>Kosegi, J., Minsker, B. & Dougherty, D. (2000). <a href="http://ascelibrary.org/doi/abs/10.1061/%28ASCE%290733-9372%282000%29126%3A7%28601%29">Feasibility study of thermal in situ bioremediation</a>, <i>Journal of Environmental Engineering</i>
<br><a href="javascript:void(0)" onclick="toggle('hid108');">Abstract</a><p class="more" id="hid108" style="display: none;">A feasibility study of a new technology for remediating sites contaminated with dense non-aqueous-phase liquids (DNAPLs) is presented. The technology combines two conventional remediation approaches, thermal treatment and in situ bioremediation, in an effort to improve bioavailability through increased dissolution and biodegradation rates at elevated temperatures. To evaluate this new approach, a simulation model has been developed that combines expressions for first-order dissolution of immobile DNAPL spheres, dual-Monod biodegradation kinetics, and diffusion-limited desorption from soil micropores. The model is used to simulate remediation of a possible future contained release at a test cell at the Groundwater Remediation Field Laboratory at Dover Air Force Base in Dover, Del. Model simulations were conducted for temperatures ranging from 15 to 40C using parameter values obtained from the literature. Simulation results show that, by increasing the temperature from 15 to 35C, the amount of mass removed in the effluent (i.e., the amount of mass not degraded in situ) is predicted to be reduced by 94%, and the time required to reach the cleanup objective is predicted to be reduced by 70%. Parameter value sensitivity was also examined. Only those parameters that substantially reduced the biodegradation rates were found to have a strong influence on the predicted benefits associated with elevated temperatures. Based on the results of these modeling experiments, coupling of these two remediation techniques appears to hold considerable promise for sites contaminated with DNAPLs.</p></li>
<li>Reed, P., Minsker, B., Valocchi, A. & Goldberg, D. (2000). <a href="">Efficient use of a genetic algorithm for long-term groundwater monitoring design</a>, <i>Computational methods in water resources - Volume 1 - Computational methods for subsurface flow and transport</i>
<br><a href="javascript:void(0)" onclick="toggle('hid109');">Abstract</a><p class="more" id="hid109" style="display: none;">This paper summarizes a methodology for designing long-term monitoring plans using groundwater fate-and-transport simulation, global mass estimation, and a genetic algorithm. Kriging and inverse distance weighting are the plume interpolation methods used to attain global mass estimates. Kriging provides the most accurate global mass estimates but has the drawback of having an increased computational complexity relative to inverse distance weighting. A hybrid method demonstrates how initial solutions found using inverse distance weighting can be refined using kriging to substantially reduce computational effort. Theoretical relationships available in the evolutionary literature were used to set the control parameters for the genetic algorithm, substantially reducing the number of trial runs required to ensure optimal or near optimal solutions. Results from the test case show that sampling costs could be reduced by as much as 60 percent without significant loss in accuracy of the global mass estimates.</p></li>
<li>Liu, Y., Minsker, B. & Saied, F. (2000). <a href="">Multiscale computational techniques for optimal groundwater in-situ bioremediation design</a>, <i>Computational methods in water resources - Volume 1 - Computational methods for subsurface flow and transport</i>
<br><a href="javascript:void(0)" onclick="toggle('hid110');">Abstract</a><p class="more" id="hid110" style="display: none;">The idea of multiscale computation has been extended to solve an optimal control model governed by time-dependent partial differential equations. The model, which was developed in previous work, uses successive approximation linear quadratic regulator to identify optimal well locations and pumping rates for groundwater in-situ bioremediation design. Several spatial and temporal multiscale methods are being explored to reduce the computational burden associated with solving this model. A one-way spatial multiscale method for solving the entire problem is presented together with a three-level case study. Over 50% reduction in computing time was achieved. A novel multiscale numerical derivative calculation is also proposed, which utilizes one-sided finite-difference approximations of the derivatives and a standard multigrid solver for solving the simulation model. The local effect of the derivative information is also examined which holds promise for further multiscale computation and to improve the computational efficiency of the derivative calculation.</p></li>
<li>Minsker, B., Padera, B. & Smalley, J. (2000). <a href="">Efficient methods for including uncertainty and multiple objectives in water resources management models using genetic algorithms</a>, <i>Computational methods in water resources - Volume 1 - Computational methods for subsurface flow and transport</i>
<br><a href="javascript:void(0)" onclick="toggle('hid111');">Abstract</a><p class="more" id="hid111" style="display: none;">Incorporating uncertainty and multiple objectives into water resources management models can be a significant challenge. Most existing methods require substantial computational effort that can be prohibitive for large-scale modeling. In this paper, two new genetic algorithm methods are presented that allow for more efficient solution of these types of problems than was previously possible. Noisy genetic algorithms use Monte Carlo-type sampling to sample from a noisy fitness function (objective function). Substantially fewer samples are required to obtain good solutions than with traditional Monte Carlo sampling, so the method is much more efficient than constraint stacking methods that have been used in water resources management models previously. Nondominated sorted genetic algorithms can efficiently identify the entire trade-off surface among multiple objectives in a single model run. Applications of both methods to risk-based corrective action design for contaminated groundwater are presented.</p></li>
<li>Minsker, B. & Shoemaker, C. (1998). <a href="">Quantifying the effects of uncertainty on optimal groundwater bioremediation policies</a>, <i>Water Resources Research</i>
<br><a href="javascript:void(0)" onclick="toggle('hid112');">Abstract</a><p class="more" id="hid112" style="display: none;">This paper describes a method for quantifying the economic and environmental effects of uncertainty in biological parameter values on optimal in situ bioremediation design. The range of uncertainty in model results associated with a range of input parameter values is quantified for both individual parameter errors and errors in combinations of parameters. Three measures of sensitivity are presented that quantify different aspects of the effects of model error on an implemented optimal policy. Numerical results are presented for an example site contaminated with phenol, with parameter ranges derived from values reported in the literature. For the example site, K(s) (the substrate half-velocity coefficient in the Monod kinetic equation for biodegradation) was found to be the most sensitive biological parameter and this sensitivity was asymmetric; i.e., reductions in the value of K(s) have a much greater effect than increases in the value of K(s). The methodology applied in this paper could also be applied to other water resource management problems, allowing the user to quantify the effects of wide ranges of possible parameter values on model results. The method is particularly useful for computationally intensive optimization models, as it requires a manageable number of model runs, and for the many situations where insufficient data are available to permit accurate estimation of probability distributions.</p></li>
<li>Minsker, B. & Shoemaker, C. (1998). <a href="">Computational issues for optimal in-situ bioremediation design</a>, <i>Journal of Water Resources Planning and Management</i>
<br><a href="javascript:void(0)" onclick="toggle('hid113');">Abstract</a><p class="more" id="hid113" style="display: none;">We have developed the first application of optimization techniques to the design of in-situ bioremediation, and have encountered a number of computational issues that have not arisen in previous efforts to optimize the design of pump-and-treat remediation. In this paper, these computational issues are discussed and a number of strategies for improving performance of the model are presented. The issues addressed include increased computational complexity, difficulties in convergence of the model, and convergence to local minima. The strategies developed will be useful for any application of optimization techniques to nonlinear processes in water resources.</p></li>
<li>Minsker, B. & Shoemaker, C. (1997). <a href="">Computational issues associated with optimal design of in situ bioremediation</a>, <i>Proceedings of the Annual Water Resources Planning and Management Conference</i>
<br><a href="javascript:void(0)" onclick="toggle('hid114');">Abstract</a><p class="more" id="hid114" style="display: none;">Computational issues associated with applying an optimal control algorithm to in situ bioremediation design are discussed. The issues include increased computational effort, convergence difficulties, and convergence to local minima. Strategies for reducing computational effort and overcoming convergence difficulties are presented.</p></li>
<li>Minsker,B. & Shoemaker, C. (1996). <a href="">Differentiating a finite element biodegradation simulation model for optimal control</a>, <i>Water Resources Research</i>
<br><a href="javascript:void(0)" onclick="toggle('hid115');">Abstract</a><p class="more" id="hid115" style="display: none;">An optimal control model for improving the design of in situ bioremediation of groundwater has been developed. The model uses a finite element biodegradation simulation model called Bio2D to find optimal pumping strategies. Analytical derivatives of the bioremediation finite element model are derived; these derivatives must be computed for the optimal control algorithm. The derivatives are complex and nonlinear; the bulk of the computational effort in solving the optimal control problem is required to calculate the derivatives. An overview of the optimal control and simulation model formulations is also given.</p></li>
<li>Minsker, B. & Shoemaker, C. (1995). <a href="">Optimizing in situ bioremediation of groundwater</a>, <i></i>
<br><a href="javascript:void(0)" onclick="toggle('hid116');">Abstract</a><p class="more" id="hid116" style="display: none;">A nonlinear optimization model for improving the design of in situ bioremediation of groundwater is presented. The model selects injection and extraction rates at specified pumping wells in each time period to minimize the cost of the cleanup. Numerical issues associated with the model are discussed and results are presented for three hypothetical cases.</p></li>
</div><hr class="large" /><div class="doc-section clearfix" id="proceedings"><h3>Conference Proceedings</h3>
<li>Chinta, I. and Minsker, B. (2011). <a href="http://ascelibrary.org/doi/abs/10.1061/41173%28414%29168">Forecasting Hypoxia in Corpus Christi Bay, Texas, by Model Fusion</a>, <i>World Environmental and Water Resources Congress, 167, 1611-1619</i>.
<br><a href="javascript:void(0)" onclick="toggle('cid1');">Abstract</a><p class="more" id="cid1" style="display: none;">This study aims to create more accurate and efficient near-real-time forecasts of hypoxia that will give researchers advance notice for manual sampling during hypoxic events. Hypoxic or dead zones, which occur when dissolved oxygen levels in water drop below 2 mg/L, are prevalent worldwide. An example of such an hypoxic zone forms intermittently in Corpus Christi Bay (CC Bay), Texas, a USEPA-recognized estuary of national significance. Hypoxia in CC Bay is caused by inflow of hypersaline waters that enter from adjacent bays and estuaries, natural fluctuations in oxygen levels due to the oxygen production-consumption cycle of the aquatic flora and fauna, seasonal fluctuations, and discharges from several wastewater treatment plants. The hypoxia forecasting method tested in this work involves a suite of data-driven model fusion techniques such as historical scenario modeling and boosting both a k-nearest neighbor (KNN) algorithm and the historical scenario model. Existing data-driven k-nearest neighbor and physics-based valve models are used as the basis for the model fusion. The historical scenario model combines the k-nearest neighbor algorithm with the valve model to predict the probability of hypoxia twenty-four hours ahead. Boosting involves training the model repeatedly on subsets of the training dataset. The results of the fused models are compared with those of the individual models to test the effectiveness of model fusion in predicting the estuarine conditions. The results showed that the valve model, which has been hitherto computing oxygen profiles, can be extended to forecast probabilities of hypoxia when combined with the k-nearest neighbor algorithm to form the historical scenario model. The findings also show that boosting significantly enhances the performance of the k-nearest neighbor algorithm and the historical scenario model, although further testing on more extensive continuous datasets is needed to verify the findings in other locations. The results show promise for model fusion to be effective for real-time forecasting in hypoxia-affected water bodies.</p>
<li>Zimmer, A. and Minsker, B. and Schmidt, A. and Ostfeld, A. (2011). <a href="http://ascelibrary.org/doi/abs/10.1061/41173%28414%29304">Benefits of Meta-Model Validation for Real-Time Sewer System Decision Support</a>, <i>World Environmental and Water Resources Congress, 303, 2911-2919</i>.
<br><a href="javascript:void(0)" onclick="toggle('cid2');">Abstract</a><p class="more" id="cid2" style="display: none;">Large-scale combined sewer systems are susceptible to overflows (CSOs) during heavy storm events. Management strategies that partition water flow into the sewers or nearby waterways may be based on conservative operational rules designed to prevent possible flow instabilities. However, these operations may not effectively utilize system storage capacity for all types of storm events. Real-time adaptation of system operating rules can reduce overflows while continuing to avoid hydraulic conditions that lead to transients and geysers. In this study, realtime genetic algorithm (GA) optimization is evaluated for its success in minimizing CSOs for a test case modeled after a portion of the Chicago Tunnel and Reservoir Plan (TARP).</p>
<li>Zimmer, A. and Minsker, B. and Schmidt, A. and Ostfeld, A. (2010). <a href="http://ascelibrary.org/doi/abs/10.1061/41114%28371%29232">Evolutionary Algorithm Memory Enhancement for Real-Time CSO Control</a>, <i>World Environmental and Water Resources Congress, 231, 2251-2259</i>.
<br><a href="javascript:void(0)" onclick="toggle('cid3');">Abstract</a><p class="more" id="cid3" style="display: none;">Control of combined sewer overflows (CSOs) may be enhanced through real-time decision support. An optimization algorithm adapted for changing rainfall is used to dynamically control complex sewer hydraulics to minimize CSO volume. Different methods of enhancing the optimization for real-time processing consist of: separating the hydraulic model for multi-objective optimization or to optimize only critical portions of the sewer system, using an efficient optimization technique, and incorporating memory into the optimization to speed convergence to a solution for each forecasted rainfall change. Potential optimal management solutions and the associated environmental characteristics can be stored and used to re-initialize the optimization at each environmental change. The memory may also be altered to indicate what precision of hydraulic model should be used for different rainfall conditions.</p>
<br><br><a href="#C2010"><h4>Publications previous to 2010</h4></a>
<ul class="a"><li class="am_dropdown">Hill, D., Minsker, B., Amir, E., and Choi, J (2009). <a href="http://eisa.ncsa.illinois.edu/Documents/Conference/Hill_etal_hic2009.pdf">Real-time anomaly detection in precipitation sensors</a>, <i>Proceedings of the 8th International Conference on Hydroinformatics  </i>.
<br><a href="javascript:void(0)" onclick="toggle('cid4');">Abstract</a><p class="more" id="cid4" style="display: none;">Recent advances in sensor technology are facilitating the deployment of sensors into the environment that can produce measurements at high spatial and/or temporal resolutions. Not only can these data be used to better characterize systems for improved modeling, but they can also be used to improve understanding of the mechanisms of environmental processes. With large volumes of data arriving in near real time, however, there is a need for automated anomaly detection to identify data that deviate from historical patterns. These anomalous data can be caused by sensor or data transmission errors or by infrequent system behaviors that may be of interest to the scientific or public safety communities. This study develops an automated anomaly detection method that employs a Dynamic Bayesian Network to assimilate data from multiple heterogeneous sensors into an uncertain model of the current state of the environment. Filtering (e.g., Kalman filtering) can then be used to infer the likelihood that a particular sensor measurement is anomalous. Measurements with a high likelihood of being anomalous are classified as such. The method developed in this study performs fast, incremental evaluation of data as they become available; scales to large quantities of data; and requires no a priori information regarding process variables or types of anomalies that may be encountered. The performance of the anomaly detector developed in this study is demonstrated using a precipitation sensor network composed of a NEXRAD weather radar and five telemetered rain gauges deployed by the USGS. The results indicate that the method performs well at identifying anomalous data caused by a real sensor failure.</p>
<li>Minsker, B.S. and Coopersmith, E. (2009). <a href="http://eisa.ncsa.illinois.edu/Documents/Conference/Minsker_Coopersmith_HIC2009.pdf">Harnessing the power of sensor and cyberinfrastructure towards environmental sustainability The WATERS Network vision and testbedding research</a>, <i>Proceedings of the 8th International Conference on Hydroinformatics  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid5');">Abstract</a><p class="more" id="cid5" style="display: none;">The WATERS Network is a U. S. National Science Foundation-supported environmental observatory initiative to transform science and engineering of the water environment through new infrastructure investments. WATERS Network seeks to answer the following overarching question: How do we establish a framework to more reliably predict and manage water quantity and quality as climate changes, populations grow, land use evolves, and individual and societal choices are made? Research enabled by the WATERS Network will provide the basic knowledge needed to understand, engineer, manage, and set policy for water resources systems and infrastructure that are critical for life and society. To assist with identifying needs and potential research outcomes from WATERS Network observatories, testbedding activities are underway in many locations around the U.S. This paper highlights ongoing research and infrastructure activities in the Upper Illinois River Basin and Corpus Christi Bay, Texas, and focuses on recent findings from the Corpus Christi Bay testbed. New approaches for enabling real-time forecasting of hypoxia have been developed using data integration and nearest-neighbor algorithms. Early forecasts were used to support adaptive sampling that identified far more widespread hypoxia than previously expected. These results show that the real-time, adaptive sampling envisioned under the WATERS Network is feasible and useful for understanding dynamic environmental systems.</p></li>
<li>Zimmer, A. and Hill, D. and Minsker, B. and Ostfeld, A. and Schmidt, A. (2009). <a href="http://ascelibrary.org/doi/abs/10.1061/41036%28342%29110">Evolutionary Optimization of Combined Sewer Overflow Control</a>, <i>World Environmental and Water Resources Congress, 109, 1-13</i>
<br><a href="javascript:void(0)" onclick="toggle('cid6');">Abstract</a><p class="more" id="cid6" style="display: none;">Model predictive control (MPC) is coupled with a real-coded Genetic Algorithm to predict a decision sequence that minimizes combined sewer overflow (CSO) volume for a 3-hour rainfall event over a hypothetical sewer system. Rainfall is transformed to overland runoff through the cell model which depicts each sewershed (draining to an overflow dropshaft) by two linear reservoirs in series, and water entering the interceptor is routed downstream to establish water levels at the dropshaft connections. A pumping rate at the most downstream end of the interceptor plus one sluice gate position for each dropshaft connection will be altered to produce the best control strategy. Resulting management scenarios disperse overflows differently throughout the sewer, but may yield similar overflow volumes. This paper describes the simulation approach taken and displays the overflow distribution for favorable control sequences.</p></li>
<li>Hill, D. and Minsker, B. and Schmidt, A. (2009). <a href="http://ascelibrary.org/doi/abs/10.1061/41036%28342%29220">Predicting CSOs for Real Time Decision Support</a>, <i>World Environmental and Water Resources Congress, 219, 1-10</i>
<br><a href="javascript:void(0)" onclick="toggle('cid7');">Abstract</a><p class="more" id="cid7" style="display: none;">This paper presents a novel data-driven method for modeling combined sewer overflows (CSOs) in real-time. This method treats CSO event generation as a threshold process that is triggered by increasingly intense rainfall events, and predicts the likelihood of a CSO given input conditions using a Bayesian network. The fusion of relevant data from multiple agencies into a unified data stream in real time is described, and a hierarchical modeling strategy is proposed that will facilitate the exploration of the causes of CSOs and direct research into the adaptive management of combined sewer systems using the Chicago wastewater system as a case study.</p></li>
<li>Dawsey, W. and Minsker, B. and Ostfeld, A. (2009). <a href="http://ascelibrary.org/doi/abs/10.1061/41036%28342%2964">Analysis of Model Sensitivity and Uncertainty for Chlorine Transport and Decay in a Water Distribution System</a>, <i>World Environmental and Water Resources Congress, 63, 1-11</i>
<br><a href="javascript:void(0)" onclick="toggle('cid8');">Abstract</a><p class="more" id="cid8" style="display: none;">There are a number of sources of uncertainty in drinking water distribution system modeling. Uncertain parameters include pipe diameters, consumer demands, hydraulic energy loss coefficients, reaction coefficients and others. Understanding the relative importance of these sources of uncertainty can improve the allocation of resources for model refinement and calibration, as well as, aid knowledge inference from monitoring data. This paper presents an analysis of uncertainty and model sensitivity for chlorine transport and decay in a water distribution system. A clustering and global variance-based sensitivity methodology is proposed to account for spatial inconsistencies found in the results of previous studies of this problem. Results are presented from small and large scalecase studies. This methodology is then used to explore the occurrence of intrusion events in a water distribution system, and the potential to detect such events through online monitoring of chlorine residual concentrations. Noise present in the chlorine monitoring signal has the potential to overwhelm the detection of an upstream intrusion and its associated chlorine demand. Results are presented from simulated intrusion events of varying magnitude and duration.</p></li>
<li>Hill, D.J. and Yong Liu and Myers, J. and Minsker, B. (2008). <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4736788&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F4736721%2F4736722%2F04736788.pdf%3Farnumber%3D4736788">End-to-End Cyberinfrastructure for Real-Time Enviornmental Decision Support</a>, <i>eScience, 2008. eScience 08. IEEE Fourth International Conference on</i>
<br><a href="javascript:void(0)" onclick="toggle('cid9');">Abstract</a><p class="more" id="cid9" style="display: none;">Recent advances in sensor technology are facilitating the deployment of sensor networks into the environment that can produce measurements at high spatial and/or temporal resolutions. Through telemetry, these measurements can be delivered in near real time as data streams for use in real-time applications. These data streams can be instrumental in furthering our understanding of the environment, in monitoring and modeling the quality of the environments in which the sensors are deployed, and in providing real-time decision support for managing environmental systems. However, in order to realize these benefits, several challenges must be addressed, such as accessing heterogeneous data streams from multiple agencies; validating the data and fusing or transforming them for a desired application; providing high-dimensional models, visualizations, and decision support systems to explore data and forecast future conditions; and sharing the resulting knowledge. This poster will present an end-to-end cyberinfrastructure that can make these tasks possible in near real time, using a case study on real-time decision support for the combined sewer system in the Chicago Metropolitan Area.</p></li>
<li>Liu, Y., Hill, D.J., Abdelzaher, T., Heo, J., Choi, J., Minsker, B., & Fazio, D (2008). <a href="http://eisa.ncsa.illinois.edu/Documents/Conference/Liu_etal_EIMC2008.pdf">Virtual sensor powered spatiotemporal aggregation and transformation A case study analyzing near-real-time NEXRAD and precipitation gauge data in a digital watershed</a>, <i>Proceedings of the Environmental Information Management Conference</i>
<br><a href="javascript:void(0)" onclick="toggle('cid10');">Abstract</a><p class="more" id="cid10" style="display: none;">In this paper, we describe the case for creating and managing "virtual sensors" for near-real-time sensor data aggregation and transformation with a case study in a watershed near Chicago. We explore various levels of abstractions of "virtual sensors", and how the virtual sensor concept and digital watershed tool can help facilitate community participation and build consensus on using and re-purposing the near-real-time data. We describe our proposed approach on aggregating NEXRAD data and on- ground in-situ precipitation gage data in near-real-time for anomaly detection purpose.</p></li>
<li>Liu, Yong and Hill, David J. and Rodriguez, Alejandro and Marini, Luigi and Kooper, Rob and Futrelle, Joe and Minsker, Barbara and Myers, James D. (2008). <a href="http://doi.acm.org/10.1145/1463434.1463528">Near-real-time precipitation virtual sensor using NEXRAD data</a>, <i>Proceedings of the 16th ACM SIGSPATIAL international conference on Advances in geographic information systems</i>
<br><a href="javascript:void(0)" onclick="toggle('cid11');">Abstract</a><p class="more" id="cid11" style="display: none;">In this demonstration paper, we describe the technologies and implementations that allow near real-time creation of new virtual precipitation sensors using NEXRAD Level II streaming data at user-specified point locations and time intervals in an integrated digital watershed with a Google Map-based web interface. The spatiotemporal and thematic transformation steps to produce such new time series data stream are implemented as a set of scientific workflows. A streaming data ontology is developed to handle temporal proximity concepts such as "previous" and "next" for irregular temporal data streams. Data and metadata management is provided by a semantic content management middleware. The new point-based virtual sensor can lower the barriers of using NEXRAD data for many hydrological applications.</p></li>
<li>Yong Liu and Marini, L. and Kooper, R. and Rodriguez, A. and Hill, D. and Myers, J. and Minsker, B. (2008). <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4736809&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4736809">Virtual Sensors in a Web 2.0 Virtual Watershed</a>, <i>eScience, 2008. eScience '08. IEEE Fourth International Conference on</i>
<br><a href="javascript:void(0)" onclick="toggle('cid12');">Abstract</a><p class="more" id="cid12" style="display: none;">This paper presents a Web 2.0 virtual observatory framework applied in an environmental watershed research context, where users not only can access existing sensor data such as the USGS (United States Geological Survey) rain gage data in near real-time, but also can create and share virtual sensors and trigger their associated workflows on-the-fly. Categories of virtual sensors are discussed and community participation and collaboration on creating virtual sensors can be promoted. An eScience use case which allows users to create virtual rain gages on a Google map front end using NEXRAD (next generation weather radar) data is presented.</p></li>
<li>DEMISSIE, Yonas and VALOCCHI, Albert and MINSKER, Barbara and BAILEY, Barbara (2008). <a href="http://cat.inist.fr/?aModele=afficheN\&cpsidt=20881990">Bias-corrected groundwater model prediction uncertainty analysis</a>, <i></i>
<br><a href="javascript:void(0)" onclick="toggle('cid13');">Abstract</a><p class="more" id="cid13" style="display: none;">The incomplete description of the subsurface processes by physically-based groundwater models often results in biased and correlated prediction errors, thus suggesting the need for systematic correction of errors before conducting prediction uncertainty analysis. In this work, error-mapping artificial neural networks (ANN) are used to correct the physically-based groundwater model (MODFLOW) prediction errors. The resulting prediction uncertainty of the coupled MODFLOW-ANN model is then assessed using three alternative methods. The first method establishes approximate confidence and prediction intervals using first-order least-squares regression approximation (also called first-order error analysis). The second method employs bootstrap approaches that involve resampling of the uncertain data with replacement and repeated model runs for constructing the confidence and prediction intervals. The third method relies on a Bayesian approach that uses analytical or Monte Carlo methods to derive the posterior distribution. The performance of these approaches is evaluated using a hypothetical case study developed based on a phytoremediation site at the Argonne National Laboratory, USA. The results indicate that the three approaches yield comparable confidence and prediction intervals, thus making the computationally efficient first-order error analysis approach attractive for estimating the coupled model uncertainty. The results also demonstrate that the error-mapping ANN not only captures some of the local biases in the MODFLOW prediction, but also systematically reduces the prediction variance.</p></li>
<li>Nelson, K., Montagna, P., Maidment, D., Hodges, B., To, E., Kulis, P., Minsker, B., & Coopersmith, E. (2007). <a href="">Web Services Facilitate Hypoxia Modeling of Corpus Christi Bay, TX</a>, <i>Proceedings of the Estuarine Research Federation Biennial Meeting  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid14');">Abstract</a><p class="more" id="cid14" style="display: none;"></p></li>
<li>Hill, D.J., Minsker, B.S., & Amir, E. (2007). <a href="http://eisa.ncsa.illinois.edu/Documents/Conference/Hill_etal_IAHR2007.pdf">Real-time Bayesian anomaly detection for environmental sensor data</a>, <i>Proceedings of the 32nd Congress of IAHR, International Association of Hydraulic Engineering and Research  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid15');">Abstract</a><p class="more" id="cid15" style="display: none;">Recent advances in sensor technology are facilitating the deployment of sensors into the environment that can produce measurements at high spatial and/or temporal resolutions. Not only can these data be used to better characterize systems for improved modeling, but they can also be used to produce better understandings of the mechanisms of environmental processes. One such use of these data is anomaly detection to identify data that deviate from historical patterns. These anomalous data can be caused by sensor or data transmission errors or by infrequent system behaviors that are often of interest to the scientific or public safety communities. Thus, anomaly detection has many practical applications, such as data quality assurance and control (QA/QC), where anomalous data are treated as data errors; focused data collection, where anomalous data indicate segments of data that are of interest to researchers; and event detection, where anomalous data signal system behaviors that could result in a natural disaster. This study develops two automated anomaly detection methods that employ Dynamic Bayesian Networks (DBNs). These machine learning methods can operate on a single sensor data stream, or they can consider several data streams at once, using all of the streams concurrently to perform coupled anomaly detection. This study investigates these methods' abilities, using both coupled and uncoupled detection, to perform QA/QC on two windspeed data streams from Corpus Christi, Texas; false positive and false negative rates serve as the basis for comparison of the methods. The results indicate that a coupled DBN anomaly detector, tracking the actual windspeeds, their measurements, and the status of these measurements, performs well at identifying erroneous data in these data streams.</p></li>
<li>Singh, A. and Minsker, B. and Valocchi, A. and Walker, D. (2007). <a href="http://ascelibrary.org/doi/abs/10.1061/40927%28243%29161">Interactive Multi-Objective Inverse Groundwater Modeling for the WIPP Site</a>, <i>World Environmental and Water Resources Congress, 160, 1-11</i>
<br><a href="javascript:void(0)" onclick="toggle('cid16');">Abstract</a><p class="more" id="cid16" style="display: none;">This paper presents ongoing research on building an interactive and multi-objective framework to solve the groundwater inverse problem. Our research has shown that the inherent instability and non-uniqueness of this problem can be improved by incorporating expert knowledge about the hydro-geology of the site. The interactive multi-objective genetic algorithm (IMOGA) considers user preference (for different transmissivity fields) as an additional objective along with quantitative calibration measures, converging to a set of Pareto optimal solutions representing the best trade-off among all (qualitative as well as quantitative) objectives. An important part of groundwater inversion is assessing parameter uncertainty and its effect upon model predictions. To assess the uncertainty in prediction we a multi-level sampling approach is used that incorporates uncertainty in both large-scale trends and the small-scale stochastic variability. The large-scale uncertainty is modeled using a Bayesian approach where both calibration error and a prior transmissivity field (as specified by the expert through the interactive rankings of the IMOGA) are considered. A geostatistical approach is adopted for the small-scale uncertainty, which is considered to be unconditional and auto-correlated with a specified covariance structure. The prediction model is run using all simulated fields to get the distribution of predictions. This methodology is being applied to a field-scale case study based on the Waste Isolation Pilot Plant (WIPP) situated in Carlsbad, New Mexico. This work is still in progress and results will be presented at the EWRI conference.</p></li>
<li>Coopersmith, E. and Minsker, B. and Maidment, D. and Hodges, B. and Bonner, J. and Ojo, T. and Montagna, P. (2007). <a href="http://ascelibrary.org/doi/abs/10.1061/40927%28243%29289">An Environmental Information System for Hypoxia in Corpus Christi Bay: A WATERS Network Testbed</a>, <i>World Environmental and Water Resources Congress, 288, 1-14</i>
<br><a href="javascript:void(0)" onclick="toggle('cid17');">Abstract</a><p class="more" id="cid17" style="display: none;">This project is creating and demonstrating a prototype Environmental Information System (EIS) that couples sensor measurements with end-to-end cyberinfrastructure to improve understanding of hypoxia in Corpus Christi Bay (CC Bay), Texas. Hypoxia is a common estuarine phenomenon that occurs when dissolved oxygen concentrations fall below 2 mg/L, and has resulted in about a ten-fold reduction in benthic standing stock and diversity in CC Bay. The hypoxia in CC Bay is correlated with salinity-induced stratification of the bay, but the stratification forcing and the spatial and temporal patterns of the hypoxia remain uncertain. In this project, an interdisciplinary team of hydrologists, environmental engineers, biologists, and computer scientists are collaborating to improve understanding of hypoxia by: (1) creating an Environmental Data Access System for CC Bay data archives, leveraging CUAHSI Hydrologic Information System (HIS) Web service developments to create data services that automatically ingest observed data in both national and local remote data archives; (2) developing an Environmental Modeling System for CC Bay hypoxia, leveraging NCSA Environmental Cyberinfrastructure Demonstrator (ECID) CyberIntegrator technology to combine numerical hydrodynamic, dissolved oxygen, and oxygen demand models with data mining using hierarchical machine learning algorithms; and (3) demonstrating the effectiveness of the EIS for supporting adaptive hypoxia sampling and collaborative research using ECID's CyberCollaboratory. This paper will give initial results and future plans for the project.</p></li>
<li>Dawsey, W. and Minsker, B. and Amir, E. (2007). <a href="http://ascelibrary.org/doi/abs/10.1061/40927%28243%29507">Real Time Assessment of Drinking Water Systems Using a Dynamic Bayesian Network</a>, <i>World Environmental and Water Resources Congress, 506, 1-6</i>
<br><a href="javascript:void(0)" onclick="toggle('cid18');">Abstract</a><p class="more" id="cid18" style="display: none;">This paper presents a methodology for real-time estimation of water distribution system state parameters using a dynamic Bayesian network to combine current observations with knowledge of past system behavior. The dynamic Bayesian network presented here allows the flexibility to model both discrete and continuous variables and represent causal relationships that exist within the distribution system. The posterior belief state can be inferred using a compact approximation algorithm that has been shown to contain inference errors. Simulations over stochastic variables are proposed to define the transition and observation models for the dynamic Bayesian network.</p></li>
<li>Hill, David J. and Minsker, Barbara S. and Amir, Eyal (2007). <a href="">Real-time Bayesian Anomaly Detection for Environmental Sensor Data</a>, <i>Proceedings of the 32nd conference of IAHR</i>
<br><a href="javascript:void(0)" onclick="toggle('cid19');">Abstract</a><p class="more" id="cid19" style="display: none;">Recent advances in sensor technology are facilitating the deployment of sensors into the environment that can produce measurements at high spatial and/or temporal resolutions. Not only can these data be used to better characterize systems for improved modeling, but they can also be used to produce better understandings of the mechanisms of environmental processes. One such use of these data is anomaly detection to identify data that deviate from historical patterns. These anomalous data can be caused by sensor or data transmission errors or by infrequent system behaviors that are often of interest to the scientific or public safety communities. Thus, anomaly detection has many practical applications, such as data quality assurance and control (QA/QC), where anomalous data are treated as data errors; focused data collection, where anomalous data indicate segments of data that are of interest to researchers; and event detection, where anomalous data signal system behaviors that could result in a natural disaster. This study develops two automated anomaly detection methods that employ Dynamic Bayesian Networks (DBNs). These machine learning methods can operate on a single sensor data stream, or they can consider several data streams at once, using all of the streams concurrently to perform coupled anomaly detection. This study investigates these methods' abilities, using both coupled and uncoupled detection, to perform QA/QC on two windspeed data streams from Corpus Christi, Texas; false positive and false negative rates serve as the basis for comparison of the methods. The results indicate that a coupled DBN anomaly detector, tracking the actual windspeeds, their measurements, and the status of these measurements, performs well at identifying erroneous data in these data streams.</p></li>
<li>Hill, D.J., & Minsker, B.S. (2006). <a href="http://eisa.ncsa.illinois.edu/Documents/Conference/Hill_Minsker_AWRA2006.pdf">Automated fault detection: Preparing real-time data for adaptive management</a>, <i>Proceedings of the American Water Resource Association (AWRA) Summer Specialty Conference on Adaptive Management  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid20');">Abstract</a><p class="more" id="cid20" style="display: none;">Real-time data provides an up-to-the minute characterization of environmental systems, which can be used to adapt management decisions to changing environmental conditions. The recent deployment of in-situ sensor arrays has increased the availability of real-time data to decision makers. When using sensor data for real-time decision-making, however, there is little time to verify its quality; therefore, a method for quickly and accurately identifying faults in the data collection process is needed. This study develops two fault detection strategies for in-situ sensors that are based on data- driven regression models of the sensor data. Sensor faults are determined by identifying anomalous measurements in the data stream, where anomalous measurements are operationally defined as measurements that fall outside of the bounds of an established prediction interval. Eight instantiations of each detection strategy are created, using different data-driven methods and either a 95% or a 99% prediction interval. The performance of these detectors for identifying data transmission faults is compared using windspeed data originating from Corpus Christi Bay, Texas. The basis for comparison is the number of false positives/negatives identified by each of the detectors. The results indicate that the strategies perform well for identifying data transmission faults.</p></li>
<li>Singh, A. and Minsker, B. (2006). <a href="http://ascelibrary.org/doi/abs/10.1061/40856%28200%29115">Interactive Multi-Objective Inverse Groundwater Modeling-Formulation and Addressing User Fatigue</a>, <i>World Environmental and Water Resource Congress, 114, 1-10</i>
<br><a href="javascript:void(0)" onclick="toggle('cid21');">Abstract</a><p class="more" id="cid21" style="display: none;">This paper builds on work done on using interactive multi-objective genetic algorithms (IMOGA) to solve the groundwater inverse problem (Singh & Minsker, 2005) by searching for optimal hydraulic conductivity fields conditioned on field measurements of hydraulic heads and conductivities. The biggest challenge faced when using such interactive systems is that of user fatigue because the user is expected to evaluate many solutions during the search process. This paper discusses a two-step approach to reduce user fatigue. First the user is shown only a fraction of the total population in every generation. To ensure minimum redundancy during evaluation, the solutions are clustered using unsupervised clustering and the expert is shown unique samples from distinct clusters. Next the unranked solutions are ranked using a surrogate model that 'learns' from the user preferences. This is implemented using a supervised classification algorithm to cluster the solutions based on the 2-D images of hydraulic conductivity. We test 'content-based' and 'spectral' algorithms for the clustering and classification as these have been shown to be similar to how humans process images. The work on applying and testing these algorithms is on-going and this paper discusses some preliminary results. Complete results will be shown at the EWRI conference.</p></li>
<li>Hayes, M. and Minsker, B. (2005). <a href="http://ascelibrary.org/doi/abs/10.1061/40792%28173%29339">Evaluation of Advanced Genetic Algorithms Applied to Groundwater Remediation Design</a>, <i>Impacts of Global Climate Change, 339, 1-9</i>
<br><a href="javascript:void(0)" onclick="toggle('cid22');">Abstract</a><p class="more" id="cid22" style="display: none;">Optimal design of a groundwater pump and treat system is a difficult task, especially given the computationally intensive nature of field-scale remediation design. Genetic algorithms (GAs) have been used extensively for remediation design because of their flexibility and global search capabilities, but computational intensity is a particularly difficult issue with GAs. This paper discusses a new competent GA, the hierarchical Bayesian Optimization Algorithm (hBOA), which is designed to reduce the computational effort. GAs operate by assembling highly fit segments of chromosomes (potential solutions), called building blocks. The hBOA enhances the efficiency of this process by using a Bayesian network to create models of the building blocks. The building blocks are nodes on the network, and the algorithm uses the network to generate new solutions, retaining the best building blocks of the parents. This work compares the performance of hBOA to a simple genetic algorithm (SGA) in solving a case study to determine if any benefit can be gained through the use of this approach. This work demonstrates that hBOA more reliably identifies the optimal solution to this groundwater remediation design problem.</p></li>
<li>Babbar, M. and Minsker, B. and Takagi, H. (2005). <a href="http://ascelibrary.org/doi/abs/10.1061/40792%28173%29340">Expert Knowledge in Long-Term Groundwater Monitoring Optimization Process: The Interactive Genetic Algorithm Perspective</a>, <i>Impacts of Global Climate Change, 340, 1-12</i>
<br><a href="javascript:void(0)" onclick="toggle('cid23');">Abstract</a><p class="more" id="cid23" style="display: none;">In most practical water resources optimization applications, a number of important subjective issues exist that cannot be represented in numerical optimization procedures. Considering these issues only in a post-optimization analysis of solutions by the expert (engineers, stakeholders, regulators, etc.) does not ensure that the final set of optimal designs address all qualitative issues important to the problem. The Interactive Genetic Algorithm (IGA) promises to overcome these hurdles by involving the expert directly in the online search process to steer the genetic algorithm to a solution or set of solutions that address both quantitative and qualitative criteria. This paper investigates the effect on the overall search process when a single user interacts with the IGA system. Some of the salient control parameters that affect performance of such a framework are algorithmic control parameters (i.e. the GA settings, visualization interfaces, etc.), human control parameters (i.e. the user's cognitive perception, user's degree of risk aversion, human fatigue, etc.), and external control parameters (i.e. environmental noise and uncertainty, etc.). This work begins a rigorous assessment of the effects of different control parameters on the IGA search process by simulating the human decision making process using fuzzy logic models of human preferences as `pseudo humans'. Comparison of such a system with a conventional optimization framework (that lacks progressive user feedback) is made for a long-term groundwater monitoring optimization problem, and related ramifications are highlighted.</p></li>
<li>Singh, A. and Minsker, B. and Takagi, H. (2005). <a href="http://ascelibrary.org/doi/abs/10.1061/40792%28173%29345">Interactive Genetic Algorithms for Inverse Groundwater Modeling: Issues with Human Fatigue and Prediction Models</a>, <i>Impacts of Global Climate Change, 345, 1-12</i>
<br><a href="javascript:void(0)" onclick="toggle('cid24');">Abstract</a><p class="more" id="cid24" style="display: none;">This paper presents an interactive multi-objective evolutionary optimization based approach to solve the inverse problem of estimating heterogeneous aquifer parameters (in this case - hydraulic conductivity) for a groundwater flow model. A hypothetical aquifer, for which the `true' parameter values are known, is used as a test case to demonstrate the usefulness of this method. It is shown that using automated calibration techniques without using expert interaction leads to parameter values that are not consistent with site knowledge. In such cases, it is desirable to incorporate expert knowledge in the estimation process to generate more reasonable estimates. An interactive approach is proposed within a multi-objective framework that allows the user to evaluate trade-offs between the expert knowledge and other measures of numerical errors. For the hypothetical aquifer, this type of expert interaction is shown to produce more plausible estimates. A major issue with interactive approaches is `human fatigue'. One way of dealing with human fatigue is to use machine learning to model user preferences. This work presents some initial results that show that machine learning models can be used to augment user interaction, allowing the IGA to find good solutions with much less user effort.</p></li>
<li>Yan, S. and Minsker, B. (2005). <a href="http://ascelibrary.org/doi/abs/10.1061/40792%28173%29348">Optimal Groundwater Remediation Design Using Trust Region Based Metamodels within a Genetic Algorithm</a>, <i>Impacts of Global Climate Change, 348, 1-12</i>
<br><a href="javascript:void(0)" onclick="toggle('cid25');">Abstract</a><p class="more" id="cid25" style="display: none;">Computational cost is a critical issue for large-scale water resource optimization problems that often involve time-consuming simulation models. Less accurate approximation ("meta") models can be used to improve computational efficiency. We propose a novel trust-region-based metamodel framework, in which hierarchically trained metamodels are embedded into a genetic algorithm (GA) optimization framework to replace time-consuming numerical models. Numerical solutions produced from early generations of the GA, along with solutions dynamically sampled from later generations, are used to retrain the metamodels and correct the GA's converging route. A bootstrap sampling technique is used to cluster the collected numerical solutions into hierarchical training regions and then multiple metamodels are trained based on these clustered regions. The hierarchically trained metamodels are then used to approximate the numerical models. A trust region testing strategy selects the most appropriate metamodels for prediction. This allows the local regions (particularly those near the optimal solution) to be approximated by smoother and smaller metamodels with higher accuracy. This can speed up GA's convergence when the population moves into local regions. The technique was tested with artificial neural networks (ANNs) and support vector machines (SVMs) on a field-scale groundwater remediation case in a distributed network computation environment. Our preliminary results show that the adaptive meta-model GA (AMGA) with the trust region based training technique converges with higher accuracy with the same computation effort.</p></li>
<li>Minsker, B. and Groves, P. and Beckmann, D. (2005). <a href="http://ascelibrary.org/doi/abs/10.1061/40792%28173%29377">Optimizing Long Term Monitoring at a BP Site Using Multi-Objective Optimization</a>, <i>Impacts of Global Climate Change, 377, 1-5</i>
<br><a href="javascript:void(0)" onclick="toggle('cid26');">Abstract</a><p class="more" id="cid26" style="display: none;">BP (formerly British Petroleum) incurs significant costs associated with monitoring subsurface remediation sites. The purpose of this project is to evaluate whether these costs could be reduced by identifying and eliminating both spatial and temporal redundancies in the monitoring data at a BP site without significantly increasing monitoring errors. The project also aims to demonstrate the potential for multi-objective optimization approaches to improve monitoring decision making at the many sites at BP and elsewhere with long-term monitoring records. The first step in the optimization process is to identify monitoring objectives and constraints, and express them in mathematical form. In this case, the initial objectives were to minimize the number of samples collected and to minimize relative BTEX interpolation error. The BTEX interpolation error for trial sets of sampling plans are calculated by comparing the concentrations interpolated using all sampling locations and times with those interpolated using only reduced sampling frequencies or locations. Historical data from the wells that are currently being sampled are used to develop a suite of interpolation models, which are then tested using a cross-validation approach. Adaptive Environmental Monitoring System (AEMS) software, developed at the University of Illinois and RiverGlass Inc., is then used to search through the billions of sampling plans to identify the optimal tradeoffs between the number of samples collected and the relative error.</p></li>
<li>Dawsey, W. and Minsker, B. and VanBlaricum, V. (2005). <a href="http://ascelibrary.org/doi/abs/10.1061/40792%28173%2943">Reducing Online Contaminant Monitoring Uncertainty Using a Bayesian Belief Network</a>, <i>Impacts of Global Climate Change, 42, 1-12</i>
<br><a href="javascript:void(0)" onclick="toggle('cid27');">Abstract</a><p class="more" id="cid27" style="display: none;">There is a great deal of uncertainty in real time characterization of water distribution system contamination events. Much of this uncertainty is due to the lack of targeted sensors which makes it necessary to use surrogate water quality parameters to indirectly measure the presence of a contaminant. A positive sensor detection can often be validated by pieces of evidence observed in a distribution system. This paper illustrates how Bayesian belief networks can be used to represent distribution system contamination scenarios. A framework was developed that integrated sensor data with other validating evidence of a contamination event. This framework was used to express causality between the events and observed evidence that comprise contamination scenarios.</p></li>
<li>Ren, X. and Minsker, B. (2005). <a href="http://ascelibrary.org/doi/abs/10.1061/%28ASCE%290733-9496%282005%29131%3A5%28351%29">Which Groundwater Remediation Objective is Better: A Realistic One or a Simple One?</a>, <i></i>
<br><a href="javascript:void(0)" onclick="toggle('cid28');">Abstract</a><p class="more" id="cid28" style="display: none;"></p></li>
<li>Babbar, M. and Minsker, B. and Takagi, H. (2004). <a href="http://ascelibrary.org/doi/abs/10.1061/40737%282004%29236">Interactive Genetic Algorithm Framework for Long Term Groundwater Monitoring Design</a>, <i>Critical Transitions in Water and Environmental Resources Management, 234, 1-10</i>
<br><a href="javascript:void(0)" onclick="toggle('cid29');">Abstract</a><p class="more" id="cid29" style="display: none;">In standard optimization approaches for water resources management problems, the designer is responsible for correctly formulating mathematical equations to describe the system objectives and constraints. The search for optimal or near-optimal solutions is made under the assumption that these formulated objectives and constraints completely describe the system. However, in real systems that is often not true. Many qualitative criteria can be integral parts of the design analysis that numerically based algorithms cannot capture. For such problems, designer interaction with the search algorithm can help the search be more creative and inclusive. Genetic algorithms are ideally suited for incorporating such interaction in their usual search process, and can successfully evolve solutions that are optimal with respect to both qualitative and quantitative objectives. Under an interactive approach, the genetic algorithm performs the usual operations of selection, crossover, and mutation, but the user evaluates the suitability (`fitness') of candidate solutions, enabling objectives that cannot be quantified to be included in the search process. In multi-objective problems, where quantitative objectives can be as important as qualitative fitness of designs, analysis of designs is done based on tradeoff fronts made from both quantitative and qualitative information. In this paper, we demonstrate the use of interactive genetic algorithms for long term groundwater monitoring problems, which have multiple numerical and subjective objectives. We also analyze the effects on the optimal monitoring designs of using an interactive optimization approach instead of more traditional numerical optimization approaches.</p></li>
<li>Sinha, E. and Minsker, B. and Babbar, M. (2004). <a href="http://ascelibrary.org/doi/abs/10.1061/40737%282004%29247">Multiscale Island Injection Genetic Algorithm for Ground Water Remediation</a>, <i>Critical Transitions in Water and Environmental Resources Management, 245, 1-8</i>
<br><a href="javascript:void(0)" onclick="toggle('cid30');">Abstract</a><p class="more" id="cid30" style="display: none;">Genetic algorithms have been shown to be powerful tools for solving a wide variety of water resources optimization problems. Applying these approaches to complex, large-scale applications, which is usually where these methods are most needed, can be difficult due to computational limitations. Large grid sizes are often needed for solving field-scale groundwater remediation design problems. Fine grids usually improve the accuracy of the solutions, but they are also computationally expensive. Multiscale parallel genetic algorithms have been shown to improve the performance of engineering design problems that use spatial grids. In this paper we present multiscale island injection genetic algorithms (IIGAs), in which the optimization algorithm has different multiscale populations working on different islands (group of processors). Each island has a fraction of its population on the coarse grid and a fraction on the fine grid. Different islands exchange the best individuals, at the same scale, after a fixed number of generations and thus drive the GA towards better and more accurate solutions faster. The performance of this approach is compared to a single population multiscale approach developed previously, using a field-scale pump-and-treat design problem at the Umatilla Army Depot.</p></li>
<li>Yan, S. and Minsker, B. (2004). <a href="http://ascelibrary.org/doi/abs/10.1061/40737%282004%29250">A Dynamic Meta-Model Approach to Genetic Algorithm Solution of a Risk-Based Groundwater Remediation Design Model</a>, <i>Critical Transitions in Water and Environmental Resources Management, 248, 1-10</i>
<br><a href="javascript:void(0)" onclick="toggle('cid31');">Abstract</a><p class="more" id="cid31" style="display: none;">Approximation ("meta") models have been used in coupled water resources optimization and simulation models to improve computational efficiency. In most instances, multiple simulation runs have been done before the optimization, which are then used to fit an approximate model that is used for the optimization. In this study, we propose a dynamic meta-modeling approach, in which artificial neural networks (ANN) is embedded into a genetic algorithm (GA) optimization framework to replace time-consuming flow and contaminant transport models. Data produced from early generations of the GA are sampled to train the ANN. We propose a dynamic learning approach that periodically re-samples new solutions both to update the ANN and correct the GA's converging route. This allows the meta model to adapt to the area in which the GA is searching and provide more accuracy. The results show that a proper sampling strategy can benefit both GA's searching and ANN'S retraining. In our test case, more than 90 percent of the numerical model calls were saved with no loss in accuracy of the optimal solution.</p></li>
<li>Singh, A. and Minsker, B. (2004). <a href="http://ascelibrary.org/doi/abs/10.1061/40737%282004%2996">Uncertainty Based Multi-Objective Optimization of Groundwater Remediation at the Umatilla Chemical Depot</a>, <i>Critical Transitions in Water and Environmental Resources Management, 95, 1-10</i>
<br><a href="javascript:void(0)" onclick="toggle('cid32');">Abstract</a><p class="more" id="cid32" style="display: none;">Management of groundwater contamination often involves conflicting objectives and substantial uncertainty. A critical source of this uncertainty in groundwater problems often stems from uncertainty in the hydraulic conductivity values for the aquifer. For a remediation solution to be reliable in practice it is important that it is robust over such potential errors. This paper presents the application of a robust multi-objective optimization method on a field-scale pump-and-treat design problem at the Umatilla Chemical Depot site at Hermiston, Oregon. A simple methodology is used to establish plausible realizations of hydraulic conductivity that are then efficiently sampled within the optimization framework using Latin Hypercube sampling. A noisy multi-objective genetic algorithm, developed and tested earlier on a hypothetical aquifer, is then applied to this field-scale case to come up with a set of robust and Pareto-dominant design solutions for the clean up of contaminants (RDX and TNT) in the groundwater. Interactions between the various trade-offs and the inherent uncertainty at the site are analyzed. Finally it is demonstrated that by using such robust multi-objective optimization schemes, it is possible to increase robustness of the optimal solutions without significant increases in costs.</p></li>
<li>Minsker, B.S., & the ASCE/EWRI Task Committee on the State of the Art in Long-Term Groundwater Monitoring Design (2003). <a href="http://eisa.ncsa.illinois.edu/Documents/Conference/Minsker_etal_AFCEE2003.pdf">Optimization of long-term groundwater monitoring</a>, <i>Proceedings of the 2003 AFCEE Technology Transfer Workshop  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid33');">Abstract</a><p class="more" id="cid33" style="display: none;"></p></li>
<li>Singh, A. and Minsker, B. (2003). <a href="http://ascelibrary.org/doi/abs/10.1061/40685%282003%29119">Modeling and Characterization of Uncertainty for Optimization of Groundwater Remediation at the Umatilla Chemical Depot</a>, <i>World Water &amp; Environmental Resources Congress, 117, 1-9</i>
<br><a href="javascript:void(0)" onclick="toggle('cid34');">Abstract</a><p class="more" id="cid34" style="display: none;">Management of groundwater contamination is a very cost-intensive proposition filled with conflicting objectives and substantial uncertainty. A critical source of this uncertainty in groundwater problems comes from the data for the conductivity values for the aquifer on which the flow and transport of the contaminant is dependent. For a remediation solution to be reliable in practice it is important that it is robust over the error in modeling data. This paper presents our efforts to model the uncertainty for the Umatilla Chemical Depot site at Oregon, a difficult task given that the scarcity of available data precludes use of stochastic hydraulic conductivity generation techniques. The installation's modeling team has divided the site into conductivity zones. We use the results from various pumping tests to establish plausible ranges for the hydraulic conductivity value in each zone. Realizations for each zone are then generated randomly from these ranges. The hydraulic head conditions resulting from each realization are then compared with measured head conditions. To incorporate spatial as well as quantitative differences in the comparison, the first moments of the hydraulic head scenarios are also compared. Unrealistic realizations are eliminated and the remaining realizations are ranked based on the moment values. The ranked realizations will be used for efficient sampling using Latin Hypercube Sampling within the framework of an advanced stochastic multi-objective genetic algorithm to obtain robust, reliable and optimal solutions.</p></li>
<li>Babbar, M. and Minsker, B. (2003). <a href="http://ascelibrary.org/doi/abs/10.1061/40685%282003%29122">Multiscale Strategies for Solving Water Resources Management Problems with Genetic Algorithms</a>, <i>World Water &amp; Environmental Resources Congress, 120, 1-10</i>
<br><a href="javascript:void(0)" onclick="toggle('cid35');">Abstract</a><p class="more" id="cid35" style="display: none;">Genetic Algorithms (GAs) face major computational bottlenecks when numerical models are used for estimating the fitness of the objective function. Especially in large-scale water resources design problems, where the scale of the spatial grids is important in determining the numerical accuracy of the design, a tradeoff exists in the precision of the fitness function and the computational expenses endured. This paper discusses multiscale strategies that can be utilized for improving the performance of GAs when working with spatial grid dependant fitness functions. The strategy uses fine grid and coarse grid fitness functions strategically to maintain the accuracy and computational speed of the problem and drive the GA towards better and more accurate solutions faster. The algorithm's efficacy is tested using a groundwater remediation design case study.</p></li>
<li>Espinoza, F. and Minsker, B. and Goldberg, D. (2003). <a href="http://ascelibrary.org/doi/abs/10.1061/40685%282003%2921">Local Search Issues for the Application of a Self-Adaptive Hybrid Genetic Algorithm in Groundwater Remediation Design</a>, <i>World Water &amp; Environmental Resources Congress, 20, 1-10</i>
<br><a href="javascript:void(0)" onclick="toggle('cid36');">Abstract</a><p class="more" id="cid36" style="display: none;">Water resources management problems can be computationally intensive and improved methods are needed to allow solution of more complex applications. In this paper, we study a numerical algorithm designed to efficiently solve water resources management applications such as groundwater management problems. The algorithm is a combination of a simple genetic algorithm and a local search method and is called a self-adaptive hybrid genetic algorithm (SAHGA). The paper presents new ways to improve performance of this algorithm together with an analysis of different alternative local search algorithms. The paper also includes an analysis of the reduction in population size that is possible when using SAHGA relative to a simple genetic algorithm (SGA). The results show that the improved algorithm is more reliable and effective in solving the proposed problem, with average savings of 68% with respect to the SGA.</p></li>
<li>Singh, A. and Minsker, B. and Goldberg, D. (2003). <a href="http://ascelibrary.org/doi/abs/10.1061/40685%282003%2993">Combining Reliability and Pareto Optimality-An Approach Using Stochastic Multi-Objective Genetic Algorithms</a>, <i>World Water & Environmental Resources Congress, 91, 1-10</i>
<br><a href="javascript:void(0)" onclick="toggle('cid37');">Abstract</a><p class="more" id="cid37" style="display: none;">Genetic Algorithms have been successfully applied to numerous water resources problems, including problems with multiple objectives or uncertainty (noise). GAs tackle multi-objective optimization by following three basic principles-advancing the non-dominated frontier; maintaining diversity in the population (through various techniques like sharing, niching, and crowding); and using an elitist. However finding Pareto-optimal solutions becomes complicated when we add uncertainty to the problem. It was found that the solutions obtained using existing multi-objective solvers, although Pareto optimal were not the most robust or reliable solutions. In single-objective problems noise has typically been dealt with using Monte-Carlo-type sampling and some form of aggregate statistics (e.g., the average of the sample fitness). With multiple objectives the noise can interfere in determining non-domination of individuals, diversity preservation, and elitism (the three basic steps in multi-objective optimization). This paper proposes and tests several approaches to tackling some of these problems. These approaches strike a balance between finding the most optimal and the most reliable solution to the problem, thus giving decision makers and designers a practical and robust optimization tool.</p></li>
<li>Arst. R., Minsker, B.S., & Goldberg, D. (2002). <a href="http://eisa.ncsa.illinois.edu/Documents/Conference/Arst_etal_EWRI2002.pdf">Comparing Advanced Genetic Algorithms and Simple Genetic Algorithms for Groundwater Management</a>, <i>Proceedings of the American Society of Civil Engineers (ASCE) Environmental & Water Resources Institute (EWRI) 2002 Water Resources Planning & Management Conference  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid38');">Abstract</a><p class="more" id="cid38" style="display: none;">The optimal design of groundwater remediation systems (well locations and pumping rates) is an importantprobleminenvironmentalengineering. Sincethisproblemisnon-linearandvery complex, it can be computationally expensive to solve. In this paper, the Extended Compact Genetic Algorithm (ECGA) and the Bayesian Optimization Algorithm (BOA), two advanced genetic algorithms, are tested to determine whether they will decrease computational time. The identification of the links between the building blocks (short, highly fit chromosome sequences), called linkage learning, is the main approach by which more effective genetic algorithms are created. Both BOA and ECGA use linkage learning, but in slightly different ways. The ECGA uses probabilistic modeling of the population to learn linkage. BOA improves on that by using Bayesian networks to estimate joint distributions that are used to generate new candidate solutions. The performances of these algorithms are compared with simple genetic algorithms to demonstrate their computational power for several different types of problems with different levels of complexities. This paper summarizes the methodology; results will be presented at the conference.</p></li>
<li>Babbar, M., Minsker, B.S., & Goldberg, D. (2002). <a href="http://eisa.ncsa.illinois.edu/Documents/Conference/Babbar_etal_EWRI2002.pdf">A Multiscale Island Injection Genetic Algorithm for Optimal Groundwater Remediation Design</a>, <i>Proceedings of the American Society of Civil Engineers (ASCE) Environmental & Water Resources Institute (EWRI) 2002 Water Resources Planning & Management Conference  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid39');">Abstract</a><p class="more" id="cid39" style="display: none;">Genetic algorithms have been shown to be powerful tools for solving a wide variety of water resources optimization problems. Applying these approaches to complex, large-scale applications, which is usually where these methods are most needed, can be difficult due to computational limitations. Large grid sizes are often needed for solving field-scale groundwater remediation design problems. Fine grids usually improve the accuracy of the solutions, but they also pose major bottlenecks in the computational efficiency of the algorithms. In this paper, we present multiscale parallel genetic algorithms that can be used to improve the performance of groundwater management problems that use multiscale grids. Application of the methods to a case study will be presented at the conference.</p></li>
<li>Michael, W.J., Tcheng, D.K., Minsker, B.S., Valocchi, A.J., Quinn, J.J., & Williams, G.P. (2002). <a href="http://eisa.ncsa.illinois.edu/Documents/Conference/Michael_etal_EWRI2002.pdf">Integrating Data Sources to Optimize Long-Term Monitoring, Operation, and Stewardship</a>, <i>Proceedings of the American Society of Civil Engineers (ASCE) Environmental & Water Resources Institute (EWRI) 2002 Water Resources Planning & Management Conference  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid40');">Abstract</a><p class="more" id="cid40" style="display: none;">Due to technical limitations and the high cost of hazardous waste site clean up, there has been a shift toward risk-based long-term management of sites, where contamination is left in place. This study demonstrates how integrating all available site data can improve LTMOS decision making and provide cost savings. A learning machine is used to integrate historic and current data from the 317/319 Area phytoremediation site at Argonne National Lab-East (ANL-E). The learning machine uses these data and daily weather data to build a model to forecast groundwater head levels. Development of the learning machine framework provides a method for integrating the diverse data sources available at the site and using that information to determine the importance of each data source in achieving monitoring objectives. Future work will determine how long the historical record will retain its accurate predictive capability and whether the value of the surrogate data (continuous samples and rainfall) increases over time. In this preliminary study, the entire historical quarterly dataset was shown to be the most important data source, which could be used to predict future water levels with far more accuracy than the most recent quarterly dataset alone.</p></li>
<li>Reed, P., Minsker, B.S., & Valocchi, A.J. (2002). <a href="http://eisa.ncsa.illinois.edu/Documents/Conference/Reed_etal_EWRI2002.pdf">Walking the Tightrope: Long Term Monitoring Design for Multiple Objectives</a>, <i>Proceedings of the American Society of Civil Engineers (ASCE) Environmental & Water Resources Institute (EWRI) 2002 Water Resources Planning & Management Conference  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid41');">Abstract</a><p class="more" id="cid41" style="display: none;">Tools such as multiobjective genetic algorithms that are capable of high order Pareto optimization (i.e., optimizing a system for more than 2 objectives) can serve as an interface between the physical system being designed and the human decision process. This paper demonstrates the use of high order Pareto optimization for long-term monitoring (LTM) design, combining quantile kriging and the Nondominated Sorted Genetic Algorithm-II (NSGA-II) to successfully balance four objectives. Optimizing the LTM application with respect to these objectives reduced the decision space of the problem from a total of 500 million designs to the set of 1156 designs identified on the 4 dimensional Pareto surface. Although the 4-dimensional Pareto surface cannot be visualized, this study demonstrates how the set of 1156 designs can inform decision making. First, we analyzed pairs of the objectives that were known to conflict. Visualizing the 7 designs from these tradeoffs provided a better understanding of how these objectives affect sampling designs and aided in discovering additional objective conflicts. Once these conflicts were discovered, they were then used to identify acceptable objective bounds and negotiate a single compromise design.</p></li>
<li>Meghna Babbar and Barbara S. Minsker (2002). <a href="">A Multiscale Master-Slave Parallel Genetic Algorithm with Application to Groundwater Remediation Design</a>, <i>Late Breaking papers at the Genetic and Evolutionary Computation Conference (GECCO-2002), New York, USA, 9-13 July 2002</i>
<br><a href="javascript:void(0)" onclick="toggle('cid42');">Abstract</a><p class="more" id="cid42" style="display: none;"></p></li>
<li>Reed, PM and Minsker, BS (2002). <a href="">Discovery \& Negotiation using Multiobjective Genetic Algorithms: A Case Study in Groundwater Monitoring Design</a>, <i></i>
<br><a href="javascript:void(0)" onclick="toggle('cid43');">Abstract</a><p class="more" id="cid43" style="display: none;"></p></li>
<li>Espinoza, F., Minsker, B.S., & Goldberg, D. (2001). <a href="http://eisa.ncsa.illinois.edu/Documents/Conference/Espinoza_etal_GECCO2001.pdf">A Self-Adaptive Hybrid Genetic Algorithm</a>, <i>Proceedings of the Genetic and Evolutionary Computation Conference  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid44');">Abstract</a><p class="more" id="cid44" style="display: none;">This paper presents a self-adaptive hybrid genetic algorithm (SAHGA) and compares its performance to a non-adaptive hybrid genetic algorithm (NAHGA) and the simple genetic algorithm (SGA) on two multi-modal test functions with complex geometry. The SAHGA is shown to be far more robust than the NAHGA, providing fast and reliable convergence across a broad range of parameter settings. For the most complex test function, the SAHGA required over %75 fewer function evaluations than the SGA to identify the optimal solution at a %99 reliability level.</p></li>
<li>Gopalakrishnan, G. and Minsker, B. and Padera, B. (2001). <a href="http://ascelibrary.org/doi/abs/10.1061/40569%282001%29101">Risk Based Corrective Action Design Using Genetic Algorithms</a>, <i>Bridging the Gap, 100, 1-9</i>
<br><a href="javascript:void(0)" onclick="toggle('cid45');">Abstract</a><p class="more" id="cid45" style="display: none;">Considerable resources have been expended in attempting to restore sites with contaminated groundwater. In the past, the cleanup goals were often established without regard to risk, mandating remediation of groundwater to background or non-detection or maximum contaminant limits. These are often difficult or impossible to achieve and have made site restoration prohibitively expensive. In response to these concerns, risk-based corrective action (RBCA) is becoming a method of choice for remediating contaminated groundwater sites. Under RBCA, the risks to human health and the environment due to contamination are evaluated and measures taken only to minimize the risk to acceptable levels. A major difficulty in RBCA is negotiating an appropriate risk-based limit and a reasonable corrective action approach, particularly given all of the sources of uncertainty in predicting risk. To aid in this process, a new framework for negotiation is being developed that combines an optimization model with simulation models in order to develop risk-based remedial designs that are both cost effective and reliable. The model combines contaminant fate and transport simulation models and health risk assessment procedures with genetic algorithms to simultaneously predict risk and propose cost effective strategies for reducing the risk. To use the model, stakeholders first negotiate the objectives of the remediation, which may include minimizing risk, minimizing cost, and minimizing cleanup time. Then any constraints such as hydraulic head limits or social or economic constraints are considered. In this paper, the steps are demonstrated using a case study.</p></li>
<li>Reed, P. and Minsker, B. and Valocchi, A. (2001). <a href="http://ascelibrary.org/doi/abs/10.1061/40569%282001%29502">Why Optimize Long Term Groundwater Monitoring Design?  A Multiobjective Case Study of Hill Air Force Base</a>, <i>Bridging the Gap, 501, 1-8</i>
<br><a href="javascript:void(0)" onclick="toggle('cid46');">Abstract</a><p class="more" id="cid46" style="display: none;">Mathematical tools from the field of optimization have significant potential for reducing long-term monitoring costs and aiding site managers in making informed decisions on sampling strategies for sites undergoing long-term monitoring. A case study is presented that demonstrates the use of a Nondominated Sorted Genetic Algorithm (NSGA) for monitoring design at Hill Air Force Base (AFB). The method combines fate-and-transport simulation (although it can also be used only with historical data), plume interpolation, and adaptive search to identify the tradeoff between monitoring costs and mass estimation error. The method efficiently provides decision makers a direct representation of the tradeoff between monitoring objectives such as cost and error. Additionally, the most and least significant monitoring wells in a preexisting monitoring network are identified.</p></li>
<li>Gopalakrishnan, G. and Minsker, B. and Goldberg, D. (2001). <a href="http://ascelibrary.org/doi/abs/10.1061/40569%282001%2994">Optimal Sampling in a Noisy Genetic Algorithm for Risk-Based Remediation Design</a>, <i>Bridging the Gap, 93, 1-8</i>
<br><a href="javascript:void(0)" onclick="toggle('cid47');">Abstract</a><p class="more" id="cid47" style="display: none;">A management model has been developed that predicts human health risk and uses a noisy genetic algorithm to identify promising risk-based corrective action designs. Noisy genetic algorithms are ordinary genetic algorithms that operate in noisy environments. The "noise" can be defined as any factor that hinders the accurate evaluation of the fitness of a given trial design. The noisy genetic algorithm uses a type of noisy fitness function called the sampling fitness function, which utilizes sampling in order to reduce the amount of noise from fitness evaluations in noisy environments. This Monte-Carlo-type sampling provides a more realistic estimate of the fitness as the design is exposed to a wide variety of conditions. Unlike Monte Carlo simulation modeling, however, the noisy genetic algorithm is highly efficient and can identify robust designs with only a few samples per design. For complex water resources and environmental engineering design problems with complex fitness functions, however, it is important that the sampling be as efficient as possible. In this paper, methods for reducing the computational effort through improved sampling techniques are investigated. A number of different sampling approaches will be presented and their performance compared using a case study of a risk-based corrective action design.</p></li>
<li>Espinoza, F. and Minsker, B. and Goldberg, D. (2001). <a href="http://ascelibrary.org/doi/abs/10.1061/40569%282001%2995">Optimal Settings for a Hybrid Genetic Algorithm Applied to a Groundwater Remediation Problem</a>, <i>Bridging the Gap, 94, 1-10</i>
<br><a href="javascript:void(0)" onclick="toggle('cid48');">Abstract</a><p class="more" id="cid48" style="display: none;">Water resources management problems can be computationally intensive and improved methods are needed to allow solution of more complex applications. In this paper, we present a numerical algorithm designed to efficiently solve optimization applications such as groundwater management problems. The algorithm is a combination of a simple genetic algorithm and a local search method and is called a hybrid genetic algorithm (HGA). As a first step in the development of an effective HGA, this paper presents a new self-adaptive HGA (SAHGA) that can be used to competently solve the problem without extensive trial-and-error experimentation. This paper presents the SAHGA approach and compares its performance with the SGA and a non-adaptive HGA (NAHGA) for several test functions. The results show considerable promise for the SAHGA, which required less than 25% of the number of function evaluations required for the SGA at a 99% reliability level. The SAHGA algorithm was also more robust than the NAHGA, performing optimally across a broad range of parameter values. The next step will be to apply the SAHGA to the groundwater remediation design problem.</p></li>
<li>Reed, P. and Minsker, B. and Goldberg, D. (2001). <a href="http://ascelibrary.org/doi/abs/10.1061/40569%282001%2997">The Practitioner's Role in Competent Search and Optimization Using Genetic Algorithms</a>, <i>Bridging the Gap, 96, 1-9</i>
<br><a href="javascript:void(0)" onclick="toggle('cid49');">Abstract</a><p class="more" id="cid49" style="display: none;">In the past decade genetic algorithms (GAs) have been used in a wide array of applications within the water resources field. Although usage of GAs has become widespread, the theoretical work from the genetic and evolutionary computation (GEC) field has been largely ignored. Most practitioners have instead treated the GA as a black box, specifying the parameters that control how the algorithms navigate the spaces of each application using trial-and-error analysis. Trial-and-error analysis is a time-consuming, difficult process resulting in an arbitrary selection of parameters without any regard to the fundamental properties of the GA. The concept of "competent search and optimization" as discussed in this work addresses this difficulty by using the available theoretical work from the GEC field to set the population size, the selection pressure, account for potential disruptions from crossover and mutation, and prevent drift stall. This paper provides an overview of a three-step method for utilizing GEC theory to ensure competent search and avoid common pitfalls in GA applications.</p></li>
<li>Minsker, B.S., Smalley, J.B., & Padera, B. (2000). <a href="">The Role of Risk and Uncertainty in Groundwater Remediation Design</a>, <i>  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid50');">Abstract</a><p class="more" id="cid50" style="display: none;"></p></li>
<li>Reed, P.M., Minsker, B.S., & Valocchi, A.J. (2000). <a href="">A multiobjective approach to long-term groundwater monitoring design.</a>, <i>Proceedings of the ASCE Water Resources Planning and Management and Water Resources Engineering Joint Conference  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid51');">Abstract</a><p class="more" id="cid51" style="display: none;"></p></li>
<li>Liu, Y., Minsker, B.S, & Saied, F. (2000). <a href="">Multiscale computational techniques for optimal groundwater in-situ bioremediation design.</a>, <i>Proceedings of the XIII International Conference on Computational Methods in Water Resources  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid52');">Abstract</a><p class="more" id="cid52" style="display: none;"></p></li>
<li>Minsker, B.S., Padera, B., & Smalley, J.B. (2000). <a href="">Efficient methods for including uncertainty and multiple objectives in water resources management models using genetic algorithms</a>, <i>  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid53');">Abstract</a><p class="more" id="cid53" style="display: none;"></p></li>
<li>Reed, P.M., Minsker, B.S., Valocchi, A.J., & Goldberg, D.E. (2000). <a href="">Efficient use of a genetic algorithm for long-term groundwater monitoring design.</a>, <i>  </i>
<br><a href="javascript:void(0)" onclick="toggle('cid54');">Abstract</a><p class="more" id="cid54" style="display: none;"></p></li>
<li>Liu, Y. and Minsker, B. and Saied, F. (2000). <a href="http://ascelibrary.org/doi/abs/10.1061/40517%282000%29380">Spatial Multiscale Techniques for Groundwater Management Modeling</a>, <i>Building Partnerships, 379, 1-5</i>
<br><a href="javascript:void(0)" onclick="toggle('cid55');">Abstract</a><p class="more" id="cid55" style="display: none;">An innovative spatial multiscale method has been proposed to solve a groundwater management model governed by partial differential equations. The management model, which was developed in previous work, uses an optimal control algorithm called successive approximation linear quadratic regulator (SALQR) to identify optimal well locations and pumping rates for in-situ bioremediation design. Previous work has shown that the computational time of this model is proportional to the cubic of the total number of nodes in the finite element mesh, which prevents it from being applied to large-scale, complex field sites. To reduce the computational burden of this model, a multiscale method for evaluating numerical derivatives is being developed, which exploits the "local-effect" of the derivative information by using multiple spatial scales within different subdomains of the model. This paper presents the methodology.</p></li>
<li>Reed, P. and Minsker, B. and Valocchi, A. (2000). <a href="http://ascelibrary.org/doi/abs/10.1061/40517%282000%29388">A Multiobjective Approach to Long-Term Groundwater Monitoring Design</a>, <i>Building Partnerships, 387, 1-5</i>
<br><a href="javascript:void(0)" onclick="toggle('cid56');">Abstract</a><p class="more" id="cid56" style="display: none;">The goal of this study is to develop a new methodology that enables decision makers to visualize and quantify the tradeoff between cost and uncertainty for sites undergoing long-term monitoring. Monte Carlo simulation is used to predict discrete cumulative probability distributions (cdf's) for the dissolved contaminant concentrations at every available monitoring location at a site. Indicator kriging is then used to evaluate the local uncertainty associated with sampling subsets of the available monitoring locations. A Non-dominated Sorted Genetic Algorithm (NSGA) searches for sampling plans that are non-dominated in terms of the two objectives: (1) minimizing sampling costs and (2) minimizing relative local uncertainty. The NSGA evolves the Pareto optimal frontier that represents the optimal tradeoff between sampling costs and relative local uncertainty. Each point on the Pareto front can be decomposed into a sampling design's cost and a spatial mapping of its local uncertainty. This methodology will be applied to the Williams Air Force Base in Arizona.</p></li>
<li>Minsker, B.S., & Smalley, J.B. (1999). <a href="">Cost-effective risk-based in situ bioremediation design.</a>, <i>Proceedings of the 5th International In Situ and On-site Bioremediation Symposium, 349-354</i>
<br><a href="javascript:void(0)" onclick="toggle('cid57');">Abstract</a><p class="more" id="cid57" style="display: none;"></p></li>
<li>Reed, P.M., Minsker, B.S., & Valocchi, A. (1999). <a href="">Cost-effective long-term monitoring design for intrinsic bioremediation</a>, <i>Proceedings of the 5th International In Situ and On-site Bioremediation Symposium, 301-306</i>
<br><a href="javascript:void(0)" onclick="toggle('cid58');">Abstract</a><p class="more" id="cid58" style="display: none;"></p></li>
<li>Reed, P. and Minsker, B. and Valocchi, A. (1999). <a href="http://ascelibrary.org/doi/abs/10.1061/40430%281999%29111">Cost-Effective Long-Term Monitoring Design for Intrinsic Bioremediation</a>, <i>WRPMD'99, 110, 1-10</i>
<br><a href="javascript:void(0)" onclick="toggle('cid59');">Abstract</a><p class="more" id="cid59" style="display: none;">The goal of this research is to develop a formal methodology for long-term sampling and monitoring at intrinsic bioremediation sites. Intrinsic bioremediation couples the ability of indigenous microbial activity to decay contaminants with long-term site sampling and monitoring to insure regulatory compliance. This methodology combines optimization and simulation to choose sampling locations that quantify the mass of contaminant while minimizing monitoring costs. It has three primary components: (1) groundwater fate-and-transport simulation, (2) geostatistical interpolation and global mass estimation, and (3) monitoring plan design using a genetic algorithm (GA). Contaminant concentrations at all potential monitoring locations are predicted using the Reactive Transport in 3-Dimensions (RT3D) simulation package. Kriging subroutines are then combined with a GA to search for sampling plans that accurately describe the contaminant mass in the plume at minimal cost. For each sampling plan, the RT3D output is used by the kriging subroutines to estimate contaminant concentrations at all unsampled locations within the domain and the total mass of contaminant. Results show that this methodology is effective at both reducing sampling costs and accurately quantifying the mass of contaminant in the plume. The effects of various GA parameters on model performance are also presented. Extensions of this work in the future will include exploring the efficacy of alternate plume interpolation schemes, incorporating uncertainty, and testing other types of GAs.</p></li>
<li>Liu, Y. and Minsker, B. and Saied, F. (1999). <a href="http://ascelibrary.org/doi/abs/10.1061/40430%281999%2989">Multiscale Optimal Control of <italic>In Situ</italic> Bioremediation</a>, <i>WRPMD'99, 88, 1-9</i>
<br><a href="javascript:void(0)" onclick="toggle('cid60');">Abstract</a><p class="more" id="cid60" style="display: none;">Multiscale methods have been demonstrated to be highly efficient techniques for solving partial differential equations. In this paper, the idea of applying multiscale computation to an optimal control model of groundwater in situ bioremediation is investigated. The optimal control model, which was developed in previous work, uses an optimal control method called successive approximation linear quadratic regulator to identify optimal well locations and pumping rates to minimize pumping costs. The model can be used as an aid in designing more cost-effective aerobic in situ bioremediation, where injection wells are used to supply oxygen and extraction wells are used to contain the contaminant plume. The goal of this research is to improve the computational efficiency of the model so that complex field sites can be addressed. A spatial multiscale approach is presented in this paper. The spatial multiscale concept comes from discretization of the model domain with different mesh sizes. By solving the optimization on different numerical meshes and using bilinear interpolation operator to switch from the coarser mesh to finest mesh, significant computational savings can be gained. Both the convergence behavior and CPU time are presented for a case study under homogeneous conditions. The impact and choice of penalty weight when applying the multiscale approach are also discussed.</p></li>
<li>Kosegi, J.M., Minsker, B.S., & Dougherty, D.E. (1997). <a href="">Thermally-enhanced in situ bioremediation of DNAPL</a>, <i>In Situ and On-Site Bioremediation: Volume 4, 135-140</i>
<br><a href="javascript:void(0)" onclick="toggle('cid61');">Abstract</a><p class="more" id="cid61" style="display: none;"></p></li>
<li>Smalley, J.B., & Minsker, B.S. (1997). <a href="">Risk-Based In Situ Bioremediation Design</a>, <i>In Situ and On-Site Bioremediation: Volume 4  353-358</i>
<br><a href="javascript:void(0)" onclick="toggle('cid62');">Abstract</a><p class="more" id="cid62" style="display: none;"></p></li>
</li></ul></div><hr class="large" /><div class="doc-section clearfix" id="thesis"><h3>Theses</h3>
<li>Evan, C. (2013). <a href="https://www.ideals.illinois.edu/handle/2142/45349">Data-driven modeling of hydroclimatic trends and soil moisture: multi-scale data integration and decision support</a>, <i>Ph.D. Thesis, University of Illinois at Urbana-Champaign.</i>
<li>Ankit, R. (2013). <a href="https://www.ideals.illinois.edu/handle/2142/45469">Designing green stormwater infrastructure for hydrologic and human benefits: an image based machine learning approach</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Rivera, S. (2013). <a href="https://www.ideals.illinois.edu/handle/2142/45309">Advancing sustainability indicators through text mining: a feasibility demonstration</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Wietsma, T. (2012). <a href="https://www.ideals.illinois.edu/handle/2142/30898">Adaptive sampling for multiscale environmental sensor networks</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Chinta, I. (2010). <a href="../Final/File/Theses/Chinta2010_MS_Thesis.pdf">Model Fusion for Improving Hypoxia Forecasts in Corpus Christi Bay, TX, USA: A Study of Boosting and Historical Scenario Modeling.</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Collier, A. (2008). <a href="../Final/File/Theses/Collier2008_MS_Thesis.pdf">Real-Time Environmental Visualization for Diverse User Communities.</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Coopersmith, E. (2008). <a href="../Final/File/Theses/Coopersmith2008_MS_Thesis.pdf">Understanding and Forecasting Hypoxia using Machine Learning Algorithmns.</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Hill, D. (2007). <a href="../Final/File/Theses/Hill2007_PhD_Thesis.pdf">Data Mining Approaches to Complex Environmental Problems.</a>, <i>Ph.D. Thesis, University of Illinois at Urbana-Champaign.</i>
<li>Singh, A. (2007). <a href="../Final/File/Theses/Singh2007_PhD_Thesis.pdf">An Interactive Multi-objective Framework for Groundwater Inverse Modeling.</a>, <i>Ph.D. Thesis, University of Illinois at Urbana-Champaign.</i>
<li>Babbar, M. (2006). <a href="../Final/File/Theses/Babbar2006_PhD_Thesis.pdf">Interactive Genetic Algorithms for Adaptive Decision Making in Groundwater Monitoring Design.</a>, <i>Ph.D. Thesis, University of Illinois at Urbana-Champaign.</i>
<li>Yan, S. (2006). <a href="../Final/File/Theses/Yan2006_PhD_Thesis.pdf">Optimizing Groundwater Remediation Designs using Dynamic Meta-models and Genetic Algorithms.</a>, <i>Ph.D. Thesis, University of Illinois at Urbana-Champaign.</i>
<li>Bhagwat, A. (2005). <a href="../Final/File/Theses/Bhagwat2005_MS_Thesis.pdf">Preliminary Cyberinfrastructure Needs Assessment and Technology Review for CLEANER (Collaborative Large-scale Engineering Analysis Network for Environmental Research)</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Hayes, M. (2005). <a href="../Final/File/Theses/Hayes2005_MS_Thesis.pdf">Evaluation of Advanced Genetic Algorithms Applied to Groundwater Remediation Design</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Farrell, D. (2004). <a href="../Final/File/Theses/Farrell2004_MS_Thesis.pdf">Data Mining to Improve Management and Reduce Costs Associated with Environmental Remediation</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Sinha, E. (2004). <a href="../Final/File/Theses/Sinha2004_MS_Thesis.pdf">Multiscale Island Injection Genetic Algorithms for Groundwater Remediation</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Zavislak, M. (2004). <a href="../Final/File/Theses/Zavislak2004_MS_Thesis.pdf">Constraint Handling in Groundwater Remediation Design with Genetic Algorithms</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Espinoza, F. (2004). <a href="../Final/File/Theses/Espinoza2003_PhD_Thesis.pdf">A Self-Adaptive Hybrid Genetic Algorithm For Optimal Groundwater Remediation Design</a>, <i>Ph.D. Thesis, University of Illinois at Urbana-Champaign.</i>
<li>Ren, X. (2003). <a href="../Final/File/Theses/Ren2003_MS_Thesis.pdf">Which Groundwater Remediation Objective Is Better, A Realistic One Or A Simple One?</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Singh, A. (2003). <a href="../Final/File/Theses/Singh2003_MS_Thesis.pdf">Uncertainty Based Multi-Objective Optimization Of Groundwater Remediation Design</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Arst, R. (2002). <a href="../Final/File/Theses/Arst2002_MS_Thesis.pdf">Which are Better, Probabalistic Model-Building Genetic Algorithms (PMBGAs) or Simple Genetic Algorithms (SGAs)? A Comparison for an Optimal Groundwater Remediation Design Problem </a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Babbar, M. (2002). <a href="../Final/File/Theses/Babbar2002_MS_Thesis.pdf">Multiscale Parallel Genetic Algorithms for Optimal Groundwater Remediation Design</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Hill, D. (2002). <a href="../Final/File/Theses/Hill2002_MS_Thesis.pdf">Modeling nitrogen transport and transformation in a heterogeneous, three-dimensional, tile-drained aquifer.</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Michael, W. (2002). <a href="../Final/File/Theses/Michael2002_MS_Thesis.pdf">Integrating Data Sources to Improve Long-Term Monitoring and Management: A Hierarchical Machine Learning Approach</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Reed, P. (2002). <a href="../Final/File/Theses/Reed2002_PhD_Thesis.pdf">Striking the Balance: Long-Term Groundwater Monitoring Design for Multiple Conflicting Objectives</a>, <i>Ph.D. Thesis, University of Illinois at Urbana-Champaign.</i>
<li>Gopalakrishnan, G. (2001). <a href="../Final/File/Theses/Gopalakrishnan2001_MS_Thesis.pdf">Optimal Sampling in a Noisy Genetic Algorithm for Risk-Based Remediation Design</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Liu, Y. (2001). <a href="../Final/File/Theses/Liu2001_PhD_Thesis.pdf">Multiscale Approach to Optimal Control of In-Situ Bioremediation of Groundwater</a>, <i>Ph.D. Thesis, University of Illinois at Urbana-Champaign.</i>
<li>Reed, P. (1999). <a href="../Final/File/Theses/Reed1999_MS_Thesis.pdf">Cost Effective Long-term Groundwater Monitoring Design using a Genetic Algorithm and Global Mass Interpolation</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Kosegi, J. (1998). <a href="../Final/File/Theses/">Thermally Enhancing In-Situ Bioremediation of DNAPLs: A Feasibility Study</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
<li>Smalley, J. (1998). <a href="../Final/File/Theses/Smalley1998_MS_Thesis.pdf">Risk-Based In Situ Bioremediation Design</a>, <i>M.S. Thesis, University of Illinois at Urbana-Champaign. </i>
</div><hr class="large" /><div class="doc-section clearfix" id="book"><h3>Books</h3>
<li>Minsker, B. (2010). <a href="http://www.joyful-professor.com/">The Joyful Professor: How to Shift From Surviving to Thriving in the Faculty Life</a>, <i>HenschelHaus Publishing Inc.</i>
<li>Minsker, B. (2005). <a href="http://books.google.com/books?id=dYDCYddq83cC&pg=PA538&lpg=PA538&dq=Hydroinformatics:+Data+Integrative+Approaches+in+Computation,+Analysis,+and+Modeling,&source=bl&ots=n0mHWXZ-dI&sig=g_f9gaBNTXHZvLNtXTCdZGDbSzw&hl=en&sa=X&ei=a4FyUtfBJ6fP2wXR4IC4BA&ved=0CFcQ6AEwBA#v=onepage&q=Hydroinformatics%3A%20Data%20Integrative%20Approaches%20in%20Computation%2C%20Analysis%2C%20and%20Modeling%2C&f=false">Genetic Algorithms</a>, <i>Hydroinformatics: Data Integrative Approaches in Computation, Analysis, and Modeling,ed. Praveen Kumar, CRC Press, ISBN 0849328942</i>
<li>Sastry, K., OReilly, U.M., Goldberg, D.E., & Hill, D.J. (2003). <a href="http://link.springer.com/chapter/10.1007%2F978-1-4419-8983-3_9">Building-block supply in genetic programming.</a>, <i>In R. Riolo, & B. Worzel (Eds.), Genetic programming in theory and practice (pp. 137152),Boston: Kluwer Academic Publishers</i>
</div></body>
